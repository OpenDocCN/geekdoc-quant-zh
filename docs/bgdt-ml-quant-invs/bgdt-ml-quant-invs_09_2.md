
# 第十二章：金融中的强化学习

戈登·里特

## 12.1 引言

我们生活在人工智能（AI）和机器学习迅速发展的时期，这些技术以惊人的方式改变了日常生活。AlphaGo Zero

（Silver 等人，2017 年）表明，纯粹的强化学习可以实现超人类的表现，只需要非常少的领域知识和（令人惊讶地！）不依赖于人类数据或指导。AlphaGo Zero 仅在被告知游戏规则后学会了玩，并且对抗了模拟器（在那种情况下是自己）。

围棋与交易有许多共同之处。优秀的交易员经常使用复杂的策略并计划几个时期的发展。他们有时会做出‘长期贪婪’的决策，并为了实施他们的长期计划而付出短期暂时损失的代价。在每一时刻，代理人可以采取的行动是相对较少的、离散的。在围棋和国际象棋等游戏中，可用的动作是由游戏规则决定的。

在交易中，也有游戏规则。目前在金融市场中最广泛使用的交易机制是‘具有时间优先权的连续双边竞价电子订单簿’。通过这种机制，报价到达和交易是连续进行的，执行优先级是根据报价的价格和它们的到达顺序来分配的。当提交买入（分别，卖出）订单 x 时，交易所的匹配引擎会检查是否可能将 x 与之前提交的某些其他卖出（分别，买入）订单匹配。如果是，则立即进行匹配。如果不是，则 x 变为活动状态，直到它与另一个新的卖出订单匹配为止

（分别，购买）订单或被取消。在给定价格水平上的所有活跃订单构成一个先进先出（FIFO）队列。关于微观结构理论还有很多要说的，我们参考 Hasbrouck（2007）的书，但简言之，这些是‘游戏规则’。

这些观察结果表明了量化金融的一个新的子领域的开始：

里特（2017 年）的交易强化学习。这个新兴领域中最基本的问题也许是以下问题。

## 基本问题 1

一个人工智能能否在不被告知要寻找什么样的策略的情况下发现一个*最优*的动态交易策略（带有交易成本）？

如果存在的话，这将是 AlphaGo Zero 的金融类比。在本说明中，我们将处理此问题的各个要素：

1. 什么是最优动态交易策略？我们如何计算其成本？2. 哪些学习方法甚至有可能攻击这样一个困难的问题？3. 我们如何设计奖励函数，以便人工智能有可能学习优化正确的事物？

这些子问题中的第一个也许是最简单的。在金融领域，*最优*意味着该策略优化了最终财富的预期效用（效用是一个微妙的概念，将在下文中解释）。最终财富是初始财富与较短时间段内的一系列财富增量之和：



最大化：$\mathbb{E}[u(w_{T})]=\mathbb{E}\left[u\left(w_{0}+\sum_{t=1}^{T}\delta w_{t}\right)\right]$（12.1）

其中 $w_t = w_t - w_{t−1}$。成本包括市场影响、交易价差、佣金、借入成本等。这些成本通常会对 $w_T$ 产生负面影响。

因为每个 $w_t$ 在该期间支付的成本而减少。

我们现在讨论第二个问题：哪些学习方法有可能奏效？当一个孩子第一次尝试骑自行车（没有平衡轮）时，孩子无法完美地知道确保自行车保持平衡并向前的确切行动顺序（踩踏板、转动把手、向左或向右倾斜等）。这是一个试错过程，正确的行动会受到奖励；错误的行动会受到惩罚。我们需要一个连贯的数学框架，承诺模拟或捕捉这个有关生物如何学习的方面。

此外，复杂的代理/行为者/存在能够进行复杂的战略规划。这通常涉及到向前思考几个时期，并可能为了在后续时期取得更大的预期收益而承受小损失。一个明显的例子是在象棋中失去一个兵作为捕获对手皇后的多部分策略的一部分。我们如何教会机器'战略性地思考'？

许多智能行为之所以被认为是'智能'，恰恰是因为它们是与环境的最优交互。如果算法能够优化分数，则它在计算机游戏中的表现是智能的。如果机器人能够找到没有碰撞的最短路径，则它的导航是智能的：最小化包含路径长度和碰撞的负惩罚的函数。

在这种情况下，学习是指学会如何明智地选择行动，以优化与环境的交互，并最大化随时间接收到的奖励。在人工智能领域，致力于研究这种学习方式的子领域称为*强化学习*。其中大部分关键发展内容总结在 Sutton 和 Barto (2018) 中。

一个经常引用的格言是，机器学习基本上分为三种类型：

监督，无监督和强化。故事如下，*监督学习*

从称为“训练集”的标记集中学习，而*无监督学习*是在未标记数据集中查找隐藏的结构，而强化学习则是完全不同的。现实情况是，这些形式的学习都是相互关联的。大多数生产质量的强化学习系统在价值函数的表示中使用了监督和无监督学习的元素。强化学习是关于随着时间的推移最大化累积奖励，而不是找到隐藏的结构，但通常情况下，最大化奖励信号的最佳方法是找到隐藏的结构。

## 12.2 马尔可夫决策过程：决策制定的通用框架

桑顿和巴托（1998）说：

强化学习的关键思想通常是使用价值函数来组织和结构化对好策略的搜索。

关于价值函数的基础论著是由贝尔曼（1957）撰写的，当时“机器学习”这个短语并不常见。然而，强化学习的存在部分归功于理查德·贝尔曼。

*价值函数*是在某种概率空间中的数学期望。潜在的概率度量是与经典训练有素的统计学家非常熟悉的系统相关联的。马尔可夫过程。当马尔可夫过程描述系统的状态时，有时称为*状态空间模型*。当在马尔可夫过程之上，你有可能从一系列可用选择中选择一个*决策*（或行动），并且有一些奖励度量告诉你你的选择有多好时，它被称为*马尔可夫决策过程*（MDP）。

在马尔可夫决策过程中，一旦我们观察到系统的当前状态，我们就拥有了做出决策所需的信息。换句话说（假设我们知道当前状态），那么也不会帮助我们（即我们无法做出更好的决策）了解导致当前状态的过去状态的完整历史。这种历史依赖性与贝尔曼原理密切相关。

贝尔曼（1957）写道：“在每个过程中，规定过程的泛函方程是通过以下直观应用得到的。”

最优策略具有这样的属性，即无论初始状态和初始决策是什么，其余的决策都必须构成对于由第一次决策产生的状态而言的最优策略。

贝尔曼（1957）

贝尔曼所谈论的“泛函方程”本质上是（12.7）和

（12.8），正如我们在下一节中解释的那样。考虑一个交互系统：代理与环境交互。 '环境' 是代理直接控制之外的系统部分。在每个时间步骤 t，代理观察环境的当前状态 St ∈ S 并选择行动 At ∈ A。这个选择影响到下一个状态的转换以及代理接收到的奖励（图 12.1）。

![228_image_0.png](img/228_image_0.png)

![228_image_1.png](img/228_image_1.png)

一切的基础是假设存在一个分布

$$(12.2)$$

p

（s 0,r ∣ s, a

）

对于过渡到状态 s0 ∈ S 并接收奖励 r 的联合概率，条件是前一状态是 s 并且代理采取行动 a。代理通常不知道这个分布，但其存在给出了诸如'期望奖励'之类的概念的数学意义。

代理的目标是最大化预期的累积奖励，表示为

$$G t=R_{t+1}+\gamma R_{t+2}+\gamma^{2}R_{t+3}\cdots$$

Gt = Rt+1 + Rt+2 + 2Rt+3 ...（12.2）

其中 0 << 1 是无限和被定义的必要条件。

*策略* 是大致上基于所处状态选择下一个动作的算法。更正式地，策略是从状态到动作空间的概率分布的映射。如果代理遵循策略 ，那么在状态 s 中，代理将以概率选择行动 a（a|s）。

强化学习是寻找最大化

$$\mathbb{E}[G_{t}]=\mathbb{E}\left[R_{t+1}+\gamma R_{t+2}+\gamma^{2}R_{t+3}+\dots\right]$$

通常，策略空间太大，不允许暴力搜索，因此寻找具有良好性质的策略必须通过值函数的使用来进行。

*状态值函数* 为策略  定义为

$$v_{\pi}(s)=\mathbb{E}_{\pi}[G_{t}\mid S_{t}=s]$$

其中 E 表示在假设策略  被遵循的情况下的期望。对于任何策略  和任何状态 s，以下一致性条件成立：

$v_{\pi}(s)=\mathbb{E}_{\pi}[G_{t}\mid S_{t}=s]$  $$=\mathbb{E}_{\pi}[\mathbb{R}_{t+1}+\gamma\,G_{t+1}\,\}S_{t}=s]$$ $$=\sum_{a}\pi(a\mid s)\sum_{s^{\prime},\pi}p(s^{\prime},r\mid s,a)[r+\gamma E_{\pi}[G_{t+1}\mid S_{t+1}-s^{\prime}]]$$ $$=\sum_{a}\pi(a\mid s)\sum_{s^{\prime},\pi}p(s^{\prime},r\mid s,a)[r+\gamma v_{\pi}(s^{\prime})]$$

$$(12.3)$$

$$(12.4)$$

$$(12.5)$$  $$(12.6)$$

′]]（12.5）

上述计算的最终结果，



$$v_{\pi}(s)=\sum_{a,s^{\prime},r}\pi(a\mid s)p(s^{\prime},r\mid s,a)[r+\gamma v_{\pi}(s^{\prime})]$$

被称为*贝尔曼方程*。值函数 v 是其贝尔曼方程的唯一解。类似地，*动作值函数* 表达了从状态 s 开始，采取行动 a，然后随后遵循策略  的价值：

$$q_{\pi}(s,a):=\mathrm{E}\pi[G_{t}\mid S_{t}=s,A_{t}=a]$$

策略  被定义为至少与 0 同样好，如果

$$\mathrm{{\dot{\boldsymbol{\theta}}}}\ \mathrm{{as}}\ \pi^{0}\ \mathrm{{if}}$$

$$v_{\pi}(s)\geq v_{\pi}0(s)$$

对于所有的状态 s。

*最优策略* 被定义为至少与任何其他策略一样好的策略。最优策略不一定是唯一的，但所有最优策略共享相同的最优状态值函数 v∗(s) = max v(s)

和最优动作值函数

$\star\cup\star$

$$d_{\pi}(s,a).$$

$$(*\backslash({\mathcal{S}},\mathbf{a})$$

q∗(s, a) = max q(s, a)。

注意到 v*(s) = maxa q*(s,a)，因此动作值函数比状态值函数更一般。

最优状态值函数和动作值函数满足贝尔曼方程

$$\nu_{\rm s}(s)=\max_{a}\sum_{s^{\prime},r}p(s^{\prime},r\mid s,a)[r+\gamma\nu_{\rm s}(s^{\prime})]\tag{12.7}$$ $$q_{\rm s}(s,a)=\sum_{s^{\prime},r}p(s^{\prime},r\mid s,a)[r+\gamma\max_{a^{\prime}}q_{\rm s}(s^{\prime},a)]\tag{12.8}$$

其中对 s0,r 的求和表示对所有状态 s0 和所有奖励 r 的求和。

如果我们拥有一个函数 q(s,a)，它是对 q*(s,a) 的估计，则 *贪婪策略*

(与函数 q 相关联)，在时间 t 选择动作 a∗t，它最大化了所有可能的 a 上的 q(*st,a*)，其中 st 是时间 t 的状态。给定函数 q*，

相关的贪婪策略是最优策略。因此我们可以将问题简化为寻找 q*，或者产生一系列收敛到 q* 的迭代。

值得注意的是，现代多期交易成本组合优化方法（Gârleanu 和 Pedersen 2013; Kolm 和 Ritter 2015; Benveniste 和 Ritter 2017）也被组织成了最优控制问题，原则上可以通过找到（12.7）的解来解决，尽管这些方程很难在约束和不可微成本下求解。

有一个简单的算法可以产生一系列收敛到 q* 的函数，称为 Q 学习（Watkins 1989）。许多后续的进展都建立在 Watkins 的开创性工作之上，最初形式的 Q 学习可能已经不再是最先进的。它的缺点包括可能需要大量的时间步骤才能收敛。

Watkins 算法包括以下步骤。首先初始化一个矩阵 Q

每个状态一行，每个动作一列的矩阵。这个矩阵可以最初是零矩阵，或者如果有的话，用一些先前的信息初始化。让 S 表示当前状态。

重复以下步骤，直到达到预先选择的收敛准则为止：

1. 使用从 Q 派生的既探索又利用的策略选择动作 A ∈ A。

2. 执行动作 A，之后环境的新状态是 S0，我们观察到奖励 R。

3. 更新 Q(S,A) 的值：设。

$$\operatorname*{max}_{a}Q(S^{\prime},a)$$

Target = R +  maxa Q(S′, a)



和

$$Q(S,A)+=\alpha[\mathrm{Target}-Q(S,A)]$$

$${\mathrm{TD-error}}$$

$$(12.9)$$

其中 ∈ (0,1) 被称为步长参数。步长参数不必是常数，事实上可以随着每个时间步长而变化。收敛证明通常需要这样做；据推测，生成奖励的 MDP 具有一些无法通过更好的学习来消除的不可避免的过程噪声，因此 TD 误差的方差永远不会趋于零。因此，t 必须对于大时间步长 t 趋于零。

假设所有状态-动作对都继续更新，并假设在一系列步长参数的变体上满足通常的随机逼近条件（见下文的（12.10）），已经证明 Q 学习算法以概率 1 收敛到 q*。

在许多感兴趣的问题中，状态空间、行动空间或两者都最自然地建模为连续空间（即适当维度 d 的 Rd 的子空间）。在这种情况下，无法直接使用上述算法。此外，上述收敛结果没有以任何明显的方式泛化。

许多最近的研究（例如，Mnih 等人（2015））提出用深度神经网络替换上述算法中的 Q 矩阵。与查找表不同，神经网络具有一组关联参数，有时称为*权重*。然后我们可以将 Q 函数写成 Q(s,a;)，强调其参数依赖性。

我们将不再在表中迭代更新值，而是迭代更新参数，，使得网络学会计算状态-动作值的更好估计。

虽然神经网络是一般的函数逼近器，但根据真正的最优 Q 函数 q*的结构，可能是神经网络学习速度非常慢，无论是在训练所需的 CPU 时间方面，还是在有效样本使用方面（定义为需要达到可接受结果所需的较低样本数）。网络拓扑结构和各种选择（如激活函数和优化器（Kingma 和 Ba 2014））在训练时间和有效样本使用方面都非常重要。

在某些问题中，使用更简单的函数逼近器（例如回归树集合）来表示未知函数 Q(s,a;)可能会导致更快的训练时间和更有效的样本使用。当 q*可以被简单的函数形式（如局部线性形式）很好地逼近时，尤其如此。

机敏的读者会注意到 Q 学习更新过程与随机梯度下降的相似性。这种有益的联系是由 Baird III 和 Moore（1999）注意到并利用的，他们将几种不同的学习过程重新阐述为随机梯度下降的特例。

随机梯度下降的收敛性在随机逼近文献中已经得到了广泛的研究（Bottou 2012）。收敛结果通常要求满足条件的学习率



$$\sum_{t}\alpha_{t}^{2}<\infty\,\mathrm{and}\,\,\sum_{t}\alpha_{t}=\infty$$

$$(12.10)$$

t = ∞ (12.10)

罗宾斯和西格蒙德（1985）的定理提供了一种方法来建立随机梯度下降的几乎确定收敛性，这在包括损失函数是非光滑的情况下都是出乎意料的温和条件。

## 12.3 理性与不确定条件下的决策制定

给定一些相互排斥的结果（每个结果可能都以某种方式影响财富或消费），一个*彩票*是这些事件上的概率分布，使得总概率为 1。通常，但不总是，这些结果涉及财富的增加或减少。例如，“支付 1000 以获得 10000 的 20%的机会”就是一个彩票。

尼古拉斯·伯努利在 1713 年 9 月 9 日给皮埃尔·雷蒙·德·蒙莫尔写信，描述了圣彼得堡彩票/悖论。一家赌场为单个玩家提供了一场只有一个公平硬币被投掷的机会游戏。奖池从$2 开始，并且每次出现正面时都会加倍。第一次出现反面时，游戏结束，玩家赢得奖池中的任何金额。数学期望值是

$${\frac{1}{2}}\cdot2+{\frac{1}{4}}\cdot4+\cdots=+\infty$$

这个悖论导致了大量新的发展，因为数学家和经济学家们努力理解解决它的所有方法。

丹尼尔·伯努利（尼古拉斯的堂兄）于 1738 年在圣彼得堡帝国科学院的评论中发表了一篇开创性的论文；事实上，这正是这个悖论的现代名称的来源。伯努利的作品有现代翻译（伯努利 1954 年），因此可以受到英语世界的欣赏。

对一个物品价值的确定不应该基于价格，而是基于它产生的效用……毫无疑问，对于乞丐而言，一千杜卡的收益比对于富人更重要，尽管两者都获得了相同的数量。

伯努利（1954）

除其他事项外，伯努利的论文中还提出了解决这个悖论的一个方案：如果投资者具有对数效用，则他们通过玩这个游戏的预期财富效用的变化是有限的。

当然，如果我们唯一的目标是研究圣彼得堡悖论，那么还有其他更实际的解决方案。在圣彼得堡彩票中，只有非常不太可能发生的事件才能产生导致无穷期望值的高奖金，因此，如果我们愿意，在实际上，忽略预计在宇宙整个寿命中不到一次发生的事件，那么期望值就变得有限。

此外，即使与最大资源实际上可想象的赌场进行游戏，彩票的期望值也相当适度。如果总资源

（或者赌场的总最大奖金）是 W 美元，那么 L = blog2(W)c 是赌场可以进行的最大次数，直到它不再完全覆盖下一笔赌注，即 2L ≤W 但 2L+1 > W。互斥事件是你翻转一次、两次、三次...直到 L 次，赢得 21,22,23...2L 或最后，你翻转 2L+1 次并赢得 W。然后彩票的预期值变为：



$$\sum_{k=1}^{L}{\frac{1}{2^{k}}}\cdot2^{k}+\left(1-\sum_{k=1}^{L}{\frac{1}{2^{k}}}\right)W=L+W2^{-L}$$

如果赌场有 W = 10 亿美元，那么'现实圣彼得堡彩票'的预期值只约为 30.86 美元。

如果彩票 M 优先于彩票 L，我们写 L≺ M。如果 M 优先于或相对于 L 视为无关，则写 L ≺ M。如果代理人对 L 和 M 持中立态度，则写 L∼ M。冯·诺伊曼和莫根斯滕（1945 年）提供了当偏好关系是合理的时的定义，并证明了任何合理的偏好关系都可以用效用函数的术语来表达。

定义 1（冯·诺伊曼和莫根斯滕 1945）。如果所有四个公理都成立，则偏好关系称为*合理*。

1. 对于任何彩票 L,M，以下情况确切成立：

$$L\prec M,M\prec L,o r\,L\sim M$$

2. 如果 L ≺ M，且 M ≺ N，则 L ≺ N 3. 如果 L ≺ M ≺ N，则存在 p ∈ [0,1]，使得

$$p L+(1-p)N\sim M$$

4. 如果 L≺ M，则对于任何 N 和 p ∈（0,1]，有

$$p L+(1-p)$$

pL + (1 − p)N ≺ pM + (1 − p)N。

最后一个公理称为'无关替代品的独立性'。满足 VNM 公理的偏好的代理人称为'VNM-理性'。我将进一步讨论'什么是合理'的问题留给哲学家，但是那些偏好不符合这些公理的人可能不应该被允许靠近赛车场

（或者股票市场）。定理 1（冯·诺伊曼和莫根斯滕 1945 年）。对于任何 VNM-理性的代理人（即

满足 1–4)，存在一个函数 u，将每个结果 A 分配给一个实数 u(A)

对于任何两个彩票，都有



$$L\prec M\operatorname{iff}\operatorname{E}(u(L))<\operatorname{E}(u(M)).$$

相反，任何试图最大化函数 u 的期望的代理人都将遵守公理 1–4。

由于

$\mbox{E}u(p_{1}A_{1}+\ldots+p_{r}A_{n})=p_{1}u(A_{1})+\ldots+p_{n}u(A_{n})$。

根据对*简单彩票*的偏好，u 是唯一确定的（除了加上一个常数和乘以正标量之外）。

（1−p）B 只有两个结果。

我们现在通过一个关于在面临风险的决策中实际情况下使用效用函数的直观理解和正确使用来说明。我们通过一个有趣的海上历险故事来做到这一点。现在是 1776 年，你拥有位于国外的商品，价值相当于一个标准尺寸的金条。这些商品在经过海上航行的船只将它们送回你之前，无法增加你的财富，但这是一段危险的旅程；船只在海上丢失的概率为 1/2。

你原计划将整个货物发送到一艘船上。库克船长建议你这样做是不明智的，并慷慨地提出将货物分成两半，并分别在两艘船上免费送货。你应该接受库克的提议吗？

一艘船： $\mathbb{E}[w_{T}]=\frac{1}{2}\times1=0.5$ 金条  两艘船： $\mathbb{E}[w_{T}]=\frac{1}{2}\times\frac{1}{2}+\frac{1}{2}\times\frac{1}{2}=0.5$ 金条

你正准备建议库克船长，由于极其巧妙地使用概率论，你已经证明这并不重要——他可以简单地使用一艘船。就在这时，丹尼尔·伯努利教授赶来建议你计算 E[u(wT)]

其中 u(w) = 1−e−w，导致：

$\nu$) = 1 - $e^{-\nu_{T}}$, 导致。 $$\mbox{一艘船}:\mathbb{E}[1-e^{-\nu_{T}}]=\frac{1}{2}\times(1-e^{-1})\approx0.32$$  两艘船： $\mathbb{E}[1-e^{-\nu_{T}}]=\frac{1}{4}(1-e^{-0})+\frac{1}{2}(1-e^{-1/2})+\frac{1}{4}(1-e^{-1})$  $$\approx0.35$$

使用伯努利方法，似乎更倾向于选择两艘船，尽管方法有效的原因可能仍然不明确。伯努利问你是否在比较两种情况时考虑了*风险*。你愤怒地回答说你更喜欢先行动，再考虑风险。但为了让伯努利满意，你计算了：



一艘船： $\mathbb{V}[w_{T}]=\frac{1}{2}(0-0.5)^{2}+\frac{1}{2}(1-0.5)^{2}$  $=0.25$

$$=0.25$$  两艘船： $\mathbb{V}[w_{T}]=\frac{1}{4}(0-0.5)^{2}+\frac{1}{2}(0.5-0.5)^{2}+\frac{1}{4}(1-0.5)^{2}$  $$=0.125$$

伯努利说如果 $\theta$

$$u(w)={\frac{1-\exp(-\kappa w)}{\kappa}}$$

$${\mathrm{is~normal}},$$

其中  > 0 是任意正标量，然后假设 wT 是正态分布的，

$$\mathbb{E}[u(w_{T})]=u\left(\mathbb{E}[w_{T}]-{\frac{\kappa}{2}}\mathbb{V}[w_{T}]\right)$$

)(12.11)

这意味着最大化 E[u(wT)] 相当于最大化

$$(12.11)$$

$$\mathbb{E}[w_{T}]-{\frac{\kappa}{2}}\mathbb{V}[w_{\mathrm{T}}]$$

$$(12.12)$$

[wT] (12.12)

因为 u 是单调的。事实证明，对于许多长尾分布，这也是成立的，正如我们在下一节中所展示的那样。

## 12.4 平均方差等价

在前一节中，我们回顾了一个众所周知的结果，即对于指数效用函数和正态分布的财富增量，可以放弃最大化 E[u(wT)]，并等价地解决数学上更简单的问题，即最大化 E[wT]−(/2)V[wT]。由于这实际上是我们的强化学习系统将要解决的问题，自然我们想知道它适用于哪类问题。结果发现，正态性或指数效用的条件都不是必要的；两者都可以大幅放宽。定义 2 如果一个效用函数 u：R → R 是增加的、凹的和连续可微的，则称其为*标准*。。

定义一个"标准"效用函数的特性具有经济意义。

即使是伟大的慈善家在其投资组合中也具有增加的财富效用——他们更愿意能够做更多来结束饥饿、疾病等。因此，一个不是线性的二次函数永远不能是一个标准效用函数。一个严格凹的二次函数必须上升然后下降，好像在某个点之后，更多的财富就变得更糟糕了。特别是（12.12）不是一个效用函数。

凹凸性对应于风险规避。最后，如果效用函数不是连续可微的，则意味着存在某个特定的财富水平，使得比该水平高一分与比该水平低一分相比大不相同。

定义 3 让'表示一场抽签，让 w'表示与抽签'相关联的（随机的）最终财富。对于两个标量 m ∈ R 和 s> 0，让 L(,)表示使得 E[w′] = 且 V[w′] = 2 的抽签'的空间。我们说*期望效用是均值*和方差的函数，如果对于所有'∈ L(,)，E[u(w′)]都是相同的。这意味着函数 Û

由 Û(, )定义：={E[u(w′)] ∶′ ∈ L(, )}



是单值的；右侧始终是一个单一的数字。

让 r ∈ Rn 表示[t,t+1]区间内的收益。因此，r ∈ Rn 是一个 n 维向量，其第 i 个分量是

$$1:^{\prime}\in L(\mu,\omega)\}$$

$$r_{i}=p_{i}(t+1)/p_{i}(t)-1$$

其中 pi(t)是第 i 个资产在时间 t 的价格（根据需要调整拆分或资本行动）。

让 h ∈ Rn 表示未来某个时间 t 的投资组合持有量，以美元或适当的计价货币计量。让 h0 表示当前的投资组合。

因此（一期）财富随机变量是

$${\widetilde{w}}=\mathrm{h}^{\prime}\mathrm{r}$$

$\left(12.13\right)^{2}$

$$(12.14)^{\frac{1}{2}}$$

期望效用最大化者选择由以下定义的最优组合 h*：

$$\mathbf{h}^{*}:={\mathrm{argmax}}[u(w^{\sim})]$$

h∗ := argmaxE[u(w∼)] (12.13)

定义 4 底层资产回报分布 p(r)被称为均值方差等价，如果分布的第一和第二矩存在，并且对于任何标准效用函数 u，存在某个常数> 0（其中取决于 u），使得

$$\mathbf{\dot{n}}^{*}=\operatorname{argn}$$

$$\mathbf{h}^{*}=\operatorname{argmax}\{\operatorname{E}[{\tilde{w}}]-(\kappa/2)\mathrm{V}[{\tilde{w}}]\}$$

h∗ = argmax{E[w*̃]−(*∕2)V[w̃]} (12.14)

其中 h* = argmaxE[u(w̃)] 如（12.13）所定义。

多变量柯西分布是椭圆形的，但其一阶及更高阶矩都是无限的/未定义的。因此，它不是均值-方差等价的，因为所需的均值和方差将是未定义的。

那么，哪些分布是均值-方差等价的？我们之前证明了正态分布是的；这很容易。许多分布，包括多元学生 t 分布等重尾分布，也是均值-方差等价的。

假设所有的抽签都对应于持有风险资产的投资组合，那么定义 3，就像定义 4 一样，是资产回报分布 p(r)的一个属性；有些分布具有这个属性，而有些分布则没有。

如果给定分布不满足定义 3，则希望均值-方差等价也不成立。直觉上，如果定义 3 不成立，那么 E[u(w′ )]必须依赖于除 E[w′ ]和 V[w′]之外的其他东西，因此很容易构造一个由于这个“额外项”而使右边项(12.14)不是最优的反例。

定义 5 一条*无差别曲线*是表面 Û的等高线，或者等价地，是形式为 Û−1(c)的集合。

定义 5 术语背后的直觉是，投资者对曲线上描述的各个点的结果都持无差别态度。

Tobin (1958) 假设期望效用是均值和方差的函数，并显示均值-方差等价是其结果。不幸的是，Tobin 的证明是有缺陷的 - 它包含了一个仅对椭圆形分布有效的推导。Feldstein (1969) 指出了 Tobin 证明中的缺陷和一个反例。在提出正确证明后，我们将讨论这个缺陷。

回顾一下，对于标量随机变量 X，*特征函数*由 X(t) = [*eitX*]定义，

如果变量有密度，那么特征函数是密度的傅里叶变换。实值随机变量的特征函数总是存在的，因为它是有界连续函数在有限测度空间上的积分。

一般来说，特征函数在分析随机变量的矩和随机变量的线性组合时特别有用。特征函数已经被用来提供概率论中一些关键结果的特别优雅的证明，如中心极限定理。

如果随机变量 X 的矩直到 k 阶，那么 X 的特征函数在 R 上连续可微 k 次。在这种情况下，



$$\mathbb{E}[X^{k}]=(-i)^{k}\phi_{X}^{(k)}(0).$$

如果 X 在零点具有 k 阶导数，则如果 k 为偶数，则 X 具有直到 k 的所有矩，但如果 k 为奇数，则只有直到 k-1 的所有矩，并且

$$\phi_{X}^{(k)}(0)=i^{k}\mathbb{E}[X^{k}]$$

如果 X1*，...，Xn*是独立随机变量，则

$$1+\ldots+X_{n}(t)=\phi$$

X1 + ... + Xn(t) = X1(t)··· Xn(t)。

定义 6 如果 Rn 值随机变量 x 的特征函数，由(t) = E[exp(it 0x)]定义，采用以下形式，则称其为*椭圆*的

$$(12.15)$$

$$\varphi(\mathbf{t})=\exp(i\mathbf{t}^{0}\mu)\psi(\mathbf{t}^{0}\mathbf{\Omega}\mathbf{t})$$

0Ωt) (12.15)

其中 ∈ Rn 是中位数向量，Ω是矩阵，假定为正定，称为*离散矩阵*。函数 不依赖于 n。

我们用特征函数(12.15)来表示分布 E(,Ω)。

名称“   名称“椭圆”源于等概率轮廓是椭球形的。如果存在方差，则协方差矩阵与Ω成比例，如果存在均值，则也是均值向量。

方程(12.15)并不意味着随机向量 x 具有密度，但如果有，则密度必须为形式



$$f_{n}(\mathbf{x})=| \Omega |^{-1/2}g_{n}[(\mathbf{x}-\mu)^{\prime}\Omega^{-1}(\mathbf{x}-\mu)]$$

′Ω−1(x − )] (12.16)

当假定存在密度时，方程(12.16)有时被用作椭圆分布的定义。特别地，(12.16)显示，如果 n = 1，则

$$(12.16)^{\frac{1}{2}}$$

$\mathsf{V}_{\mathrm{f}}$

$$(12.17)$$

√_

变换后的变量 z = (x−)/

√Ω满足 z∼E(0,1)。

多元正态分布是最知名的椭圆家族；对于正态分布，有 gn(s) = cn exp(−s/2)（其中 cn 是归一化常数），并且

(T) = exp(−T/2)。注意 gn 取决于 n，而 不取决于 n。椭圆类还包括许多非正态分布，包括显示重尾的例子，因此更适合建模资产回报。例如，具有 自由度的多元学生 t 分布具有形式的密度(12.16)

$$g n(s)\propto(v+s)-(n+v)/2$$

$$(12.18)$$

gn(s)∝(v + s)−(n + v)∕2 (12.17)

对于 = 1，可以恢复多元柯西分布。

人们可以选择对于足够大的 s，gn(s)为恒定的零，这将使得资产回报的分布在上限和下限上都有界。因此，对 CAPM 的一项批评 - 即它要求资产具有无限责任 - 不是一个有效的批评。

让 v = Tx 表示随机向量 x 的固定（非随机）线性变换。将 v 的特征函数与 x 的特征函数联系起来是很有意义的。

$\phi_{\rm v}(t)=\mathbb{E}[e^{it^{\prime}{\rm v}}]=\mathbb{E}[e^{it^{\prime}{\rm Tx}}]=\phi_{\rm x}(\Gamma^{\prime}t)=e^{it^{\prime}{\rm T}\mu}\psi(t^{\prime}{\rm T}\Omega{\rm T}^{\prime}t)$  $=e^{it^{\primei(t^{\prime}\Delta t)$

为了方便起见，我们定义 m = T，Δ = TΩT0。

即使是相同的函数，函数 *fn,gn* 出现在密度中(12.16)

对于不同的 n（=Rn 的维度），可能具有相当不同的形状，我们在中看到

(12.17)。然而，函数 与 n 无关。 因此，人们有时称椭圆的 'family' 与单个函数等同，但可能具有不同的值 ,Ω 和一组确定密度的函数 gn - 欧几里德空间的每个维度的一个这样的函数。 椭圆家族的边际化导致新的椭圆属于同一家族（即相同的 -函数）。

定理 2 如果 r 的分布是椭圆的，并且 u 是标准效用函数，则预期效用是均值和方差的函数，而且还有

$$\partial_{\mu}\widehat{U}(\mu,\omega)\geq0\;\mathrm{and}\;\partial_{\omega}\widehat{U}(\mu,\omega)\leq0$$

Û(, ) ≥ 0，而 U*̂(,* ) ≤ 0 (12.19)

证明。在此证明的持续时间内，固定一个持有向量 h ∈ Rn 的投资组合，令 x = h0r 表示财富增量。 让  = h0E[r] 和 2 = h0Ωh 表示 x 的时刻。 使用边际化性质 (12.18) 和 1 × n 矩阵 T = h0，我们有

$$(12.19)$$

x 的第 k 中心时刻将是



$$i^{-k}\frac{d^{k}}{d t^{k}}\psi(t^{2}\omega^{2})\Bigg|_{t=0}$$

从这里可以清楚地看出，所有奇数时刻都将为零，而第 2k 时刻将与 2k 成比例。因此，x 的完整分布完全由,,决定，因此预期效用是的函数，。

我们现在证明不等式 (12.19)。 写成

$$\widehat{U}(\mu,\omega)=\mathbb{E}[u(x)]=\int_{-\infty}^{\infty}u(x)f(x)dx.\tag{12.20}$$

请注意，积分是关于一个一维变量的。 使用 n = 1 的 Eq. (12.16) 的特例，我们有

$f_{1}(x)=\omega^{-1}g_{1}[(x-\mu)^{2}/\omega^{2}]$. (12.21)

使用 $(12.21)$ 来更新。

使用 (12.21) 更新 (12.20)，我们有

$${\hat{U}}(\mu,\omega)=\mathbb{E}[u(x)]=\int_{-\infty}^{\infty}(x)\omega^{-1}g_{1}[(x-\mu)^{2}/\omega^{2}]d x.$$

现在做变量变换 z = (x−)/ 和 dx =  dz，得到

$$\hat{U}(\mu,\omega)=\int_{-\infty}^{\infty}u(\mu+\omega z)g_{1}(z^{2})d z.$$

期望的性质 U*̂(,* ) ≥ 0 立即根据定义 2 中的条件得到，该条件是 u 递增的。

对于 Û的情况如下：

$$\partial_{\omega}\widehat{U}(\mu,\omega)=\int_{-\infty}^{\infty}\tau s^{\prime}(\mu+\omega z)zg_{1}(z^{2})dz$$ $$=\left[\int_{-\infty}^{0}+\int_{0}^{\infty}\right]u^{\prime}(\mu+\omega z)zg_{1}(z^{2})dz$$ $$=-\int_{0}^{\infty}u^{\prime}(\mu-\omega z)zg_{1}(z^{2})dz+\int_{0}^{\infty}u^{\prime}(\mu+\omega z)zg_{1}(z^{2})dz$$ $$=\int_{0}^{\infty}zg_{1}(z^{2})[u^{\prime}(\mu+\omega z)-u^{\prime}(\mu-\omega z)]dz$$

如果在一个区间上一个可微函数是凹的，当且仅当它的导数在该区间上单调递减，因此 u0( + z) − u0( − z) < 0 而 g1(z2) > 0，因为它是一个概率密度函数。因此在积分域内，∫ ∞0 zg1(z2)[u′( + z) − u′( − z)]dz 的被积函数是非正的，因此 U*̂(,* ) ≤ 0，完成定理 2 的证明。

回顾上面关于无差别曲线的定义 5。想象一下在 , 平面上写出的无差别曲线，其中在水平轴上。如果曲线有两个分支，那么只取上半部分。在定理 2 的条件下，关于无差别曲线可以做出两个陈述：

当 d/d > 0 时，或者投资者对于具有不同方差的两个投资组合不加区分，只有当具有更大的投资组合也具有更大的方差时，

d2/d2 > 0，或者一个人必须得到补偿才能接受更大的（这个速率是 d/d），这个速率随着的增加而增加。这两个属性表明无差别曲线是凸的。

如果想知道如何沿着无差别曲线计算 d/d，我们可以假设无差别曲线是由参数化的



$$\lambda\rightarrow(\mu(\lambda),\sigma(\lambda))$$

$$[z^{2}]d z.$$

并且将 E[u(x)] = u( + z)g1[z2]两边求导。

关于求导。根据假设，左侧在无差别曲线上是常数（导数为零）。因此

$$0=\int u^{\prime}(\mu+\sigma z)(\mu^{\prime}(\lambda)+z\sigma^{\prime}(\lambda))g_{1}(z^{2})d z$$  $$\frac{d\mu}{d\sigma}=\frac{\mu^{\prime}(\lambda)}{\sigma^{\prime}(\lambda)}=-\frac{\int_{\mathbb{R}}z u^{\prime}(\mu+\sigma z)g_{1}(z^{2})d z}{\int_{\mathbb{R}}u^{\prime}(\mu+\sigma z)g_{1}(z^{2})\,d z}$$

如果 u0 > 0 且 u00 < 0 对所有点都成立，则分子 RR z u0( +z)g1(z2)dz 为负，因此 d/d > 0。

证明 d2/d2 > 0 类似（练习）。

如果分布 p(r)不是椭圆形的话，具体是什么导致失败呢？这个证明的关键步骤假设可以通过变量变换 z = (x−)/将两参数分布 f(x;,)放入“标准形式”f(z;0,1)。这不是所有两参数概率分布的性质；例如，对数正态分布就不是。

可以通过直接计算来看到，对于对数效用函数，u(x) = logx，对于财富的对数正态分布，

$$f(x;m,s)={\frac{1}{s x{\sqrt{2\pi}}}}\exp(-(\log\,x-m)^{2}/2s^{2})$$

那么无差别曲线不是凸的。x 的矩是

$$\mu=e m+s^{2}/2,\quad\mathrm{and}$$

2∕2，且 2 = (em + s

$$\sigma2=(e m+s^{2}/2)2(e s^{2}-1)$$

通过一点代数运算得到



$$\mathbb{E}u=\log\,\mu-{\frac{1}{2}}\log(\sigma^{2}/\mu^{2}+1)$$

然后可以计算沿着 Eu = 常数的参数曲线的 d/d 和 d2/d2，并且看到在曲线的每个点处 d/d > 0，但是 d2/d2 的符号变化。因此，这个例子不能是均值-方差等价的。

定理 2 意味着对于给定水平的中位回报，正确类型的投资者总是不喜欢离散度。我们因此假设，除非另有说明，分布的前两个时刻存在。在这种情况下（对于椭圆分布），中位数是平均值，离散度是方差，因此根据定义 4 的意义上，基础资产回报分布是均值方差等价的。我们强调，这适用于任何光滑的、凹的效用。

## 12.5 奖励

在某些情况下，奖励函数的形状并不明显。这是问题和模型制定的艺术的一部分。我在制定奖励函数时的建议是，非常仔细地思考问题的“成功”是什么，并且以一种非常完整的方式思考。强化学习代理只能学会最大化它所知道的奖励。如果奖励函数中缺少了定义成功的某些部分，那么你正在训练的代理在成功的这一方面很可能会落后。

## 12.5.1 交易奖励函数的形式

在金融领域，与某些其他领域一样，奖励函数的问题也是微妙的，但令人高兴的是，这个微妙的问题已经被伯努利（1954）、冯·诺伊曼和莫根斯坦（1945）、阿罗（1971）和普拉特（1964）解决了。在不确定性下的决策理论足够一般，以包括非常多，如果不是所有的，组合选择和最优交易问题；如果你选择忽略它，你就会自食其果。

再次考虑最大化（12.12）：

$${\mathrm{maximize}}\colon\left\{\mathbb{E}[\omega_{T}]-{\frac{k}{2}}\mathbb{V}[\omega_{T}]\right\}$$

$$(12.22)$$

假设我们可以发明一些定义 'reward' Rt 的定义，以便

$$\mathbb{E}[\omega_{T}]-{\frac{k}{2}}\mathbb{V}[\omega_{T}]\approx\sum_{t=1}^{T}R_{t}$$

为了使得。

$$(12.23)$$

Rt (12.23)

这里（12.22）看起来像是一个随时间累积的奖励问题。

强化学习是寻找最大化期望 Gt = E[Rt + 1 + Rt + 2 + 2Rt + 3 + ...] 的策略。

根据（12.23），只要  ≈1，那么最大化预期效用。

考虑奖励函数



$R_{t}:=\delta\omega_{t}-\frac{k}{2}(\delta\omega_{t}-\widehat{\mu})^{2}$ (12.24)

其中 ̂ 是一个参数的估计，代表一个期间的平均财富增量， = E[wt]。

$${\frac{1}{T}}\sum_{t=1}^{T}R_{t}=\underbrace{{\frac{1}{T}}\sum_{t=1}^{T}\delta\omega_{t}}_{\rightarrow\mathbb{E}[\delta\omega_{t}]}-{\frac{k}{2}}\underbrace{{\frac{1}{T}}\sum_{t=1}^{T}(\delta\omega_{t}-{\widehat{\mu}})^{2}}_{\rightarrow\mathbb{V}[\delta\omega_{t}]}$$

那么，对于大的 T，右侧的两项逼近样本均值和样本方差，分别。

因此，通过这种特殊选择的奖励函数（12.24），如果代理学会最大化累积奖励，它也应该近似最大化均值方差形式的效用。

## 12.5.2  利润和损失的会计

假设在时刻 t=0,1,2*, . . . ,T* 的离散时间内发生 N 个资产的市场交易。

让 nt ∈ ZN 表示 t 时刻的 *股份数量* 向量，以便

$$\mathbf{\partial}\cdot\mathbf{\partial}p_{t}$$

ht := *ntpt* ∈ RN

表示了以美元计的持仓向量，其中 pt 表示 t 时刻的中点价格向量。

假设对于每个 t，数量 nt 的股票在 t 之前的瞬间交易，并且在 t+1 之前的瞬间没有进一步的交易发生。 让 vt = navt + casht，其中 navt = nt ⋅ pt 表示“投资组合价值”，我们将其定义为风险资产的净资产价值，加上现金。 在 [t,t+1) 区间内的*利润和损失*(PL) 在佣金和融资之前

是由组合价值 vt+1 的变化给出的。

例如，假设我们在 t 之前购买了 nt = 100 股股票，每股价格为 pt = 100 美元。 然后，尽管参考价格 pt 没有变化，但 navt 增加了 10 000，而 casht 减少了 10 000，导致 vt 保持不变。 假设在 t+1 之前没有进一步的交易发生，pt+1 = 105；那么 vt+1 = 500，尽管这个 PL 被称为*未实现*，直到我们再次交易并将利润转入现金项时，它才*实现*。

现在假设 pt = 100，但由于买卖价差、临时冲击或其他相关摩擦，我们的有效购买价格为 ̃pt = 101。 进一步假设我们继续使用中点价格 pt 进行“按市场标记”，或计算净资产价值。

然后，由于交易，navt 增加了 (nt)pt = 10 000，而 casht 减少了 10 100，这意味着即使参考价格 pt 没有变化，vt 也减少了 100。 这种差异称为*滑点*；它显示为 vt 的现金部分的成本项。

执行交易列表导致现金余额变化，给出



$$\delta(\mathrm{cash})_{t}=-\delta n_{t}\cdot\widetilde{p}_{t}$$

$$(12.25)$$ $$(12.26)$$ $$(12.27)$$

其中 ̃pt 是我们的有效交易价格，包括滑点。 如果 nt 的分量全部为正，则表示支付正金额的现金，而如果 nt 的分量为负，则我们收到现金收益。

因此，在融资和借款成本之前，一个人有 vt:=vt − vt−1 = (nav)t + (cash)t

$\mu_{0}\cdot\rho_{t-1}=\delta(\mu t)_{t}\cdot\delta(\mu t)_{t}$  $=n_{t}\cdot p_{t}-n_{t-1}\cdot p_{t-1}-\delta n_{t}\cdot\widetilde{p}_{t}$  $=nt\cdot pt-nt-1\cdot pt+nt-1\cdot pt-1\cdot pt-1-\delta nt\cdot\widetilde{pt}$  $=\delta nt\cdot(pt-\widetilde{pt})+nt-1\cdot(pt-pt-1)$  $=\delta n_{t}\cdot(p_{t}-\widetilde{p}_{t})+b_{t-1}\cdot r_{t}$

其中资产回报率为 rt = *pt/pt*−1 −1。 让我们将包括滑点和借款/融资成本在内的*总成本 ct* 定义如下：

$$(12.28)$$

$$(12.29)$$

$$c_{t}:=\mathrm{slip}_{t}+f\!n_{t},\quad\mathrm{where}$$

$$(12.30)$$

ct := slipt + *fint*, where (12.29)

$$\mathrm{slip}_{t}:=\delta n_{t}\cdot(\widetilde{p}_{t}-p_{t})$$

slipt := nt ⋅ (̃pt − pt) (12.30)

其中 fint 表示期间发生的佣金和融资成本，佣金与 nt 成正比，融资成本是 nt 组件的凸函数。slipt 组件被称为 *滑点成本*。我们的约定是 fint 总是大于 0，并且由于市场冲击和买卖价差，slipt 大概率大于 0。

## 12.6 投资组合价值与财富

结合 (12.29),(12.30) 与 (12.28)，我们最终得到

$$(12.31)$$

$$\delta v_{t}=h_{t-1}\cdot r_{t}-c_{t}$$

vt = ht−1 ⋅ rt − ct (12.31)

如果我们可以以中间价格矢量 pt 清算投资组合，那么 vt 将代表考虑中的交易策略在时间 t 关联的总财富。由于滑点，期望以 pt 价格清算投资组合是不合理的，这导致了形式为 (12.30) 的成本。

具体而言，vt = navt + casht 有现金部分和非现金部分。现金部分已经以财富单位存在，而非现金部分 navt = nt ⋅ pt 如果支付成本则可以转换为现金；该成本被称为 *清算滑点*：

liqslipt := − nt ⋅ (̃pt − pt)

因此，这是滑点的公式，但带有 nt = −nt。注意，清算在每个周期最多一次是相关的，这意味着清算滑点应该在最终时间 T 之后最多收费一次。

总之，只要我们愿意添加一个形式为 E[liqslipT] 的单个项，我们就可以将 vt 与财富过程 wt 互相对应（12.32）

对于多期目标。如果 T 很大且策略盈利，或者如果投资组合与典型的日交易量相比较小，则 liqslip T ≪ vT，并且 (12.32)

可以忽略而对最终策略的影响不大。在接下来的内容中，为简单起见，我们将 vt 与总财富 wt 都视为相同。

## 12.7 详细例子

将智能行为制定为强化学习问题始于对状态空间 S 和行动空间 A 的识别。状态变量 st 是一个数据结构，简单地说，必须包含代理需要做出交易决策所需的一切，而不包含其他内容。任何 alpha 预测或交易信号的值必须是状态的一部分，因为如果它们不是，代理将无法使用它们。

适合包含在状态中的变量：

1. 当前仓位或持有量。

2. 任何被认为具有预测性的信号的值。

3. 市场微观结构的当前状态（即限价订单簿），以便代理商可以决定最佳执行方式。

在交易问题中，行动的最明显选择是要交易的股票数量 nt，卖单对应于 nt < 0。在一些市场上，交易轮数有利可图，这限制了可能的行动到一个更粗的网格。如果代理与市场微观结构的互动很重要，通常会有更多选择要做，因此行动空间会更大。例如，代理可以决定使用哪种执行算法，是否跨越价差或被动，目标参与率等。

我们现在讨论如何在交易过程中观察奖励。在时刻 t 之前立即，代理观察到状态 pt 并决定一个行动，即以股票单位为单位的交易列表 nt。代理将此交易列表提交给执行系统，然后在 t+1 之前几乎什么都不能做。

代理等待一个周期并观察奖励



$$R_{t+1}\approx\delta v_{t+1}-\frac{k}{2}(\delta v_{t+1})^{2}.$$

2. (12.33)

$$(12.33)$$

在这种情况下，强化学习的目标是，代理将学会如何最大化累积奖励，即近似于均值方差形式 E[v]−(/2)V[v]的(12.33)的总和。

对于本例，假设存在一种可交易的证券，其严格正价格过程 pt > 0。（这个“证券”本身可以是其他证券的投资组合，如 ETF 或套期保值相对价值交易。）

进一步假设存在某个“均衡价格”pe，使得 xt = log(*pt/pe*)

具有动态 dxt = −xt +  t (12.34)



$$\mathbf{\omega}_{t}=-\lambda\mathbf{x}_{t}+\mathbf{\omega}$$

其中 t ∼ N(0,1)，并且当 t 6 = s 时 t,s 是独立的。这意味着 pt 倾向于以均值回归速率回归到其长期均衡水平 pe。这些假设暗示着类似套利的东西！在非常远离均衡的情况下采取的头寸几乎没有损失的可能性，并且具有极端不对称的损失-收益配置文件。

对于此练习，动态（12.34）的参数被取为

= log(2)/H，其中 H = 5 是半衰期， = 0.1，均衡价格为 pe = 50。

所有现实的交易系统都有限制来界定它们的行为。对于这个例子，我们使用一个减少了行动空间的空间，其中单个间隔内的交易量 nt 最多限制为 K 轮数，其中一个“轮数”通常是 100 股

大多数机构股票交易都是整数倍的轮数。此外，我们假设最大持仓量为 M 轮。因此，可能的交易空间，以及行动空间，是

$=\;\;\pi r$ .

$$\mathrm{A}=\mathrm{LotSize}\cdot\{-K,-K+1,\,\ldots,K\}$$

让 H 表示持有量 nt 的可能值，则类似地

$$H=\{-M,-M+1,\,\ldots,M\}.$$

对于以下示例，我们取 K = 5 和 M = 10。

真实市场的另一个特征是*价格跳动*，定义为小的价格增量

（例如美元 0.01 美元），以便所有报价价格（即所有买价和卖价）都是跳动尺寸的整数倍数。存在跳动尺寸，以平衡价格优先和时间优先。

这对我们来说很方便，因为我们无论如何都想构建一个离散模型。我们在这个例子中使用 TickSize = 0.1。

我们选择可能价格空间的（有限）边界，使得过程（12.34）的样本路径以极小的概率离开空间。使用上述参数，价格路径离开区间[0.1, 100]的概率足够小，以至于问题的任何方面都不依赖于这些边界。

具体来说，可能价格空间是：

P = TickSize - {1,2,..., 1000} C R+

最初，我们不允许代理知道任何关于动态的信息。因此，代理不知道，，甚至不知道（12.34）形式的某些动态是有效的。

代理也不知道交易成本。我们为任何交易收取一个 tick 大小的差价成本。如果买卖价差等于两个 ticks，那么这个固定成本就对应于穿过价差执行的积极成交所产生的滑点。如果价差仅为一个 tick，那么我们的选择过于保守。因此



$$\mathrm{SpreadCost}(\delta n)=\mathrm{TickSize}\cdot| \delta n |$$

$$(12.35)$$

$$(12.36)$$

SpreadCost(n) = TickSize ⋅ | n | (12.35)

我们还假设存在永久性价格影响，其具有线性的函数形式：假定每轮交易一手，价格就会移动一个 tick，因此每交易一股的美元成本为| nt | × TickSize/LotSize，因此所有股票的总美元成本为 ImpactCost(n)=(n)

2 × TickSize∕LotSize. (12.36)

总成本是 SpreadCost(n) + ImpactCost(n)的总和

= TickSize ⋅ | n | + (n)

2 × TickSize∕LotSize.

我们的主张不是这些是我们生活中的确切成本函数，尽管函数形式有些讲得通。

环境状态 st = (*pt,nt*−1)将包含安全价格 pt，以及代理进入期间的持仓（以股票计）：nt−1。因此，状态空间是笛卡尔积 S = H × P。然后代理选择一个动作

$$a_{t}=\delta n_{t}\in A$$

这改变了位置为 nt = nt−1 +nt，并观察到的利润/损失等于 vt = nt(pt + 1 − pt) − ct

并且奖励

$$R_{t+1}=\delta v_{t+1}-\frac{1}{2}k(\delta v_{t+1})^{2}$$

如式（12.33）中所示。

我们通过反复应用涉及的更新过程来训练 Q-学习者

（12.9）。该系统具有控制学习速率、折现率、风险厌恶等的各种参数。为了完整起见，以下示例中使用的参数值为：k = 10−4，= 0.999，= 0.001，= 0.1。我们使用 *ntrain* = 107 训练步骤（每个“训练步骤”包括根据（12.9）进行一次动作值更新），然后在 5000 个随机过程新样本上评估系统（见图 12.2）。

出乎意料的是，对样本外的优异表现或许是可以预料的；假设奥恩斯坦-乌伦贝克过程意味着系统中几乎存在套利机会。当价格偏离均衡太远时，打赌价格回归到均衡的交易几乎没有损失的可能性。即使考虑了成本，根据我们的参数设置，这仍然是。

![246_image_0.png](img/246_image_0.png)

正确。因此，在这个理想化的世界中存在类似套利的交易策略并不令人惊讶，而完美的均值回归过程，如（12.34），在真实市场中可能不存在。

相反，令人惊讶的是，Q-学习者最初并不知道资产价格存在均值回归，也不知道交易成本。它从不计算参数的估计值，。它学会在无模型的情境下最大化期望效用，即直接从奖励中而不是间接地（使用模型）。

我们还验证了期望效用最大化比期望利润最大化具有更高的样本外夏普比率。对这个原理的理解可以追溯到至少 1713 年，当时贝努利指出，基于马丁格尔赌博的财富最大化投资者在面对时表现得毫无道理（参见贝努利（1954）的最新翻译）。

## 12.7.1 基于模拟的方法

我们在这里提出的程序的一个主要缺点是它需要大量的训练步骤（在我们提出的问题上是几百万个）。当然，有数百万个时间步的金融数据集（例如，每秒采样一次的高频数据，持续几年），但在其他情况下，需要采用不同的方法。

即使在高频率的例子中，也可能不希望使用几年的数据来训练模型。

幸运的是，基于模拟的方法为这些问题提供了一种有吸引力的解决方案。我们提出了一个多步训练程序：



2. 从市场数据中估计模型的参数，确保参数估计的置信区间相对较小。

3. 使用模型模拟比实际世界呈现的数据集大得多的数据集。

4. 在模拟数据上训练强化学习系统。

对于模型 dxt = −xt + t，这相当于从市场数据中估计，符合简洁模型的标准。

“圣杯”将是市场微观结构如何对各种下单策略作出响应的完全真实的模拟器。 为了最大限度地有用，这样的模拟器应该能够准确地表示由过度激进交易引起的市场影响。

有了这两个组成部分 - 资产回报的随机过程模型和良好的微观结构模拟器 - 就可以生成任意大小的训练数据集。

然后学习过程仅部分是无模型的：它需要一个资产回报模型，但不需要明确的功能形式来模拟交易成本。 在这种情况下，“交易成本模型”将由市场微观结构模拟器提供，这可以说提供了比试图将交易成本简化为单一函数更详细的图像。

我们注意到，训练数据的自动生成是 AlphaGo Zero（Silver 等人 2017）的关键组成部分，该模型主要通过自我对弈进行训练 - 有效地使用其之前版本作为模拟器。 无论是否使用模拟器进行训练，都在寻找在较少时间步骤内收敛到所需性能水平的训练方法。 在所有训练数据都是真实市场数据的情况下，时间步骤的数量是固定的，毕竟。

## 12.8 结论与进一步工作

在本章中，我们看到强化学习关注寻找良好的价值函数（因此也是良好的策略）。 在金融的几乎每个子领域中，这种对最佳行为的模型都是基础性的。 例如，市场微观结构理论的大部分经典著作（Glosten 和 Milgrom 1985; Copeland 和 Galai 1983; Kyle 1985）都将经销商和知情交易者都建模为随着时间优化其累积货币奖励。 在许多情况下，被引用的作者假设经销商简单地交易以最大化预期利润（即风险中性）。 实际上，没有交易者是风险中性的，但是如果以某种其他方式控制了风险（例如，严格的库存控制），并且与经销商从事市场制造活动所赚取的溢价相比，风险非常小，则风险中性可能是一个很好的近似值。

最近的多期优化方法（Gârleanu 和 Pedersen 2013; Kolm 和 Ritter 2015; Benveniste 和 Ritter 2017; Boyd 等人 2017）都围绕贝尔曼最优性的价值函数方法展开。

期权定价是基于动态对冲，其目的是在期权的生命周期内最小化方差：在其中期权与复制组合进行对冲的组合的方差。 而带有交易成本时，实际上需要解决这种多期优化问题，而不仅仅是查看当前期权希腊字母。 有关一些相关工作，请参阅 Halperin（2017）。

因此，我们认为寻找和使用良好的价值函数可能是金融中最基本的问题之一，跨越从微观结构到衍生品定价和套期保值等各个领域。 强化学习，广义地定义，是研究如何在计算机上解决这些问题的学科； 因此，它也是基本的。

一个有趣的进一步研究领域受到古典物理学的启发。

牛顿动力学代表了相对于一个称为哈密尔顿'*的主要函数的动作价值函数的贪婪策略。 对于拥有大量（数千）资产的投资组合的最优执行问题，最好将其视为哈密尔顿动力学的特例，就像 Benveniste 和 Ritter 所展示的那样

（2017 年）。Benveniste 和 Ritter（2017）中的方法也可以看作是上述框架的特例； 那里的一个关键思想是使用与值函数相关的梯度下降的函数方法。 令人惊讶的是，即使它从连续路径开始，Benveniste 和 Ritter 的方法

（2017 年）将一般化为处理市场微观结构，其中行动空间总是有限的。

传闻中，一些大型投资银行的更为复杂的算法执行部门开始使用强化学习来优化他们在短期时间内的决策。 这看起来非常自然； 毕竟，强化学习提供了一种处理由结构丰富、微妙且非常离散的限价订单簿提出的离散行动空间的自然方法。 《最佳执行》（Almgren and Chriss, 1999）这本经典著作实际上并没有指定如何与订单簿交互。

如果将交易科学分为（1）涉及数千资产的大规模投资组合分配决策和（2）市场微观结构和最优执行理论，那么这两种问题都可以统一到最优控制理论（也许是随机的）的框架下。 主要区别在于，在问题（2）中，交易的离散性至关重要：在双向拍卖电子限价订单簿中，一般只有少数几个价格水平可以在任何时刻进行交易（例如，出价和要约，或者可能是附近的报价）

一次只有有限数量的股份会进行交易。 在大规模投资组合分配决策中，将投资组合持有建模为连续的（例如，Rn 中的向量）通常是足够的。

强化学习很好地处理离散性。相对较小的、有限的动作空间，如围棋、国际象棋和雅达利游戏中的空间，是强化学习实现超人类表现的领域。展望未来的 10 年，我们预测，在上述两个研究领域中，虽然像（12.24）这样的奖励函数同样适用，但在市场微观结构和最优执行领域，强化学习将最为有用。

## 参考文献

Almgren, R. 和 Chriss, N. (1999)。清算价值。*Risk* 12 (12): 61–63。

Arrow, K.J. (1971)。*风险承担理论论文集*。North-Holland，阿姆斯特丹。

Baird, L.C. III 和 Moore, A.W. (1999)。一般强化学习的梯度下降。

Advances in Neural Information Processing Systems 968–974。

Bellman, R. (1957)。*动态规划*。普林斯顿大学出版社。

Benveniste, E.J. 和 Ritter, G. (2017)。带有长期效用函数的最优微观结构交易。[链接](https://ssrn.com/abstract=3057570)。

Bernoulli, D. (1954)。对风险度量的新理论的阐述。*经济计量学*:

经济计量学会杂志 22 (1): 23–36。

Bottou, L. (2012)。随机梯度下降技巧。在：*神经网络：行业的技巧*，

421–436。Springer。

Boyd, S. 等人 (2017)。通过凸优化进行多期交易。arXiv 预印本 arXiv:1705.00109。

Copeland, T.E. 和 Galai, D. (1983)。信息对买卖差价的影响。*金融学杂志* 38 (5): 1457–1469。

Feldstein, M.S. (1969)。流动性偏好理论和投资组合选择中的均值-方差分析。*经济研究评论* 36 (1): 5–12。

Gârleanu, N. 和 Pedersen, L.H. (2013)。可预测收益和交易成本下的动态交易。*金融学杂志* 68 (6): 2309–2340。

Glosten, L.R. 和 Milgrom, P.R. (1985)。在具有异质信息交易者的专家市场中的报价、询价和交易价格。*金融经济学杂志* 14 (1): 71–100。

Halperin, I. (2017)。QLBS：黑-肖尔斯（-默顿）世界中的 Q 学习者。arXiv 预印本 arXiv:1712.04609。

Hasbrouck, J. (2007)。*经验市场微观结构*，第 250 卷。纽约：牛津大学出版社。

Kingma, D. 和 Ba, J. (2014)。Adam：一种随机优化方法。*arXiv 预印本* arXiv:1412.6980。

Kolm, P.N. 和 Ritter, G. (2015)。多期组合选择和贝叶斯动态模型。

风险 28 (3): 50–54。

Kyle, A.S. (1985)。连续拍卖和内幕交易。*经济计量学会的经济计量学* 53 (6): 1315–1335。

Mnih, V. 等人 (2015)。通过深度强化学习实现人类水平的控制。*自然* 518

(7540): 529。

Pratt, J.W. (1964)。小规模和大规模中的风险规避。*经济计量学会的经济计量学* 32 (1–2): 122–136。

Ritter, G. (2017)。用于交易的机器学习。*Risk* 30 (10): 84–89。[链接](https://ssrn.com/)

abstract=3015609。

Robbins, H. 和 Siegmund, D. (1985)。非负几乎超级马丁格尔的收敛定理及其一些应用。在： *赫伯特·罗宾斯精选论文*，111–135。

Springer.

Silver, D. 等人 (2017)。在没有人类知识的情况下掌握围棋。 *自然* 550 (7676)：

354–359。

Sutton, R.S. 和 Barto, A.G. (1998)。 *强化学习：导论*。剑桥：MIT

出版社。

Sutton, R.S. 和 Barto, A.G. (2018)。 *强化学习：导论*。第二版，正在进行中。剑桥：MIT 出版社 http://incompleteideas.net/book/bookdraft2018jan1.pdf。

Tobin, J. (1958)。流动性偏好作为对风险的行为。 *经济研究评论* 25 (2)：65–86。

Von Neumann, J. 和 Morgenstern, O. (1945)。 *博弈论与经济行为理论*。普林斯顿，NJ：普林斯顿大学出版社。

Watkins, C.J.C.H. (1989)。Q-learning. 博士论文。


