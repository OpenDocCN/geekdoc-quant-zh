# ã€å¹´åº¦ç³»åˆ—ã€‘ç›‘ç£å­¦ä¹ æ ‡ç­¾åœ¨è‚¡å¸‚ä¸­çš„åº”ç”¨ï¼ˆä»£ç +ä¹¦ç±ï¼‰

> åŸæ–‡ï¼š[`mp.weixin.qq.com/s?__biz=MzAxNTc0Mjg0Mg==&mid=2653289050&idx=1&sn=60043a5c95b877dd329a5fd150ddacc4&chksm=802e384fb759b1598e500087374772059aa21b31ae104b3dca04331cf4b63a233c5e04c1945a&scene=27#wechat_redirect`](http://mp.weixin.qq.com/s?__biz=MzAxNTc0Mjg0Mg==&mid=2653289050&idx=1&sn=60043a5c95b877dd329a5fd150ddacc4&chksm=802e384fb759b1598e500087374772059aa21b31ae104b3dca04331cf4b63a233c5e04c1945a&scene=27#wechat_redirect)

![](img/1a681c0b726a3a51b3508cf86dc7c2e8.png)

**å¾€æœŸç³»åˆ—**

**å¹´åº¦ç³»åˆ—ä¸€ï¼š**[ä½¿ç”¨ Tensorflow é¢„æµ‹è‚¡ç¥¨å¸‚åœºå˜åŠ¨](https://mp.weixin.qq.com/s?__biz=MzAxNTc0Mjg0Mg==&mid=2653289014&idx=1&sn=3762d405e332c599a21b48a7dc4df587&chksm=802e3823b759b135928d55044c2729aea9690f86752b680eb973d1a376dc53cfa18287d0060b&token=1304016003&lang=zh_CN&scene=21#wechat_redirect)

**å¹´åº¦ç³»åˆ—äºŒï¼š**[å…¨çƒæŠ•è¡Œé¡¶å°–æœºå™¨å­¦ä¹ å›¢é˜Ÿå…¨é¢åˆ†æ](https://mp.weixin.qq.com/s?__biz=MzAxNTc0Mjg0Mg==&mid=2653289018&idx=1&sn=8c411f676c2c0d92b0dd218f041bee4b&chksm=802e382fb759b139ffebf633ac14cdd0f21938e4613fe632d5d9231dab3d2aca95a11628378a&token=708329446&lang=zh_CN&scene=21#wechat_redirect)

**å¹´åº¦ç³»åˆ—ä¸‰ï¼š**[2018 å¹´å­¦ä¹  Python æœ€å¥½çš„ 5 é—¨è¯¾ç¨‹](https://mp.weixin.qq.com/s?__biz=MzAxNTc0Mjg0Mg==&mid=2653289028&idx=1&sn=631cbc728b0f857713fc65841e48e5d1&chksm=802e3851b759b147dc92afded432db568d9d77a1b97ef22a1e1a376fa0bc39b55781c18b5f4f&token=1802794913&lang=zh_CN&scene=21#wechat_redirect)

ç”±äºä½ä¿¡å™ªæ¯”å’Œéå¹³ç¨³çš„ä»·æ ¼åˆ†å¸ƒï¼Œé¢„æµ‹æœªæ¥è‚¡ç¥¨ä»·æ ¼èµ°åŠ¿æ˜¯ä¸€ä»¶ååˆ†å›°éš¾çš„äº‹ã€‚ç°åœ¨æµè¡Œçš„æœºå™¨å­¦ä¹ ç®—æ³•é€šå¸¸ä¼šç»™ä½ å¸¦æ¥ä¸æ€ä¹ˆæ»¡æ„çš„ç»“æœã€‚

æ­¤ç¯‡æ¨æ–‡ä¸­ï¼Œæˆ‘ä»¬ä¸»è¦è€ƒè™‘å¦‚ä½•æ ‡è®°æ•°æ®å’Œåº”ç”¨ç›‘ç£å­¦ä¹ ã€‚è¿™é‡Œçš„å¤§å¤šæ•°æ–¹æ³•éƒ½æ˜¯åŸºäºæœ€è¿‘å‡ºç‰ˆçš„ä¸€æœ¬ä¹¦ã€ŠAdvances in Financial Machine Learningã€‹ï¼Œå®‰åˆ©ä¸€ä¸‹å“¦ï¼

**ï¼ˆä¹¦ç±åœ¨æ–‡æœ«å…è´¹ä¸‹è½½ï¼‰**

![](img/0aa9138735a1cff54cafea60d4497e9f.png)

**åˆ†ç±»çš„æ–¹æ³•**

é¢„æµ‹è‚¡ç¥¨ä»·æ ¼çš„æ–¹å‘æ˜¯ç®—æ³•äº¤æ˜“çš„ä¿¡å·ã€‚åˆ†ç±»å™¨é¢„æµ‹æœªæ¥ä»·æ ¼çš„ä¸Šå‡æˆ–ä¸‹é™ã€‚ç„¶åï¼Œå°†åˆ†ç±»å™¨çš„ç»“æœè¾“å‡ºä½œä¸ºä¿¡å·è¾“å…¥åˆ°äº¤æ˜“ç®—æ³•ä¸­ï¼Œä¾‹å¦‚ï¼Œå½“é¢„æµ‹ä¸ºæ­£(è´Ÿ)æ—¶ä¹°å…¥(å–å‡º)è‚¡ç¥¨ã€‚è¦æŠŠè¿™ä¸ªæ“ä½œå½“åšåˆ†ç±»ï¼Œæˆ‘ä»¬éœ€è¦æ ¹æ®æœªæ¥ä»·æ ¼æ¥è·å¾—æ ‡ç­¾ã€‚ç»å¸¸çœ‹åˆ°çš„ä¸€ç§æ–¹æ³•å°±æ˜¯ç®€å•åœ°æ ‡æ³¨ï¼Œä¸‹ä¸€ä¸ªæ—¥æœŸçš„ä»·æ ¼æ–¹å‘ã€‚é€šè¿‡è¿™ç§æ–¹æ³•çš„æ ‡ç­¾å¯èƒ½ç”±äºä½ä¿¡å™ªæ¯”è€Œè¢«å™ªå£°æ±¡æŸ“ã€‚ 

ä¾‹å¦‚ï¼Œå³ä½¿ä¸‹ä¸€ä¸ªæ—¥æœŸè¿”å›çš„åˆ†å¸ƒæœ‰æ­£çš„å¹³å‡å€¼ï¼Œä»·æ ¼å¯èƒ½ä¼šå› ä¸ºå™ªå£°è€Œä¸‹é™ã€‚è®©æˆ‘ä»¬ä»æ•°å­¦çš„è§’åº¦æ¥çœ‹ï¼š

Pn å’Œ P0 ä»£è¡¨æœªæ¥å’Œå½“å‰çš„è‚¡ç¥¨ä»·æ ¼ï¼š

![](img/15eddd0e0aa71afea5ac57bf65ad803c.png)

å…¶ä¸­ğ‘Ÿğ‘–æ˜¯æ¯ä¸ªæ—¥æœŸçš„å›æŠ¥ã€‚

å¦‚ä½•ğ‘Ÿğ‘–<<1ï¼Œä¸Šå¼çš„å…³ç³»è¿‘ä¼¼ï¼š

![](img/0ded0bf980b4f44929eb4760a994bfe4.png)

å¦‚æœæ ¹æ®ç›¸åŒçš„æ­£æ€åˆ†å¸ƒå¯¹æ‰€æœ‰å›æŠ¥è¿›è¡Œé‡‡æ ·ï¼Œå³ğ‘Ÿğ‘– ~ N(Î¼,Ïƒ)ï¼š

![](img/db04cbd87458a6cf58af179612c0a01b.png)

å› æ­¤ï¼Œå¹³å‡å€¼å¢é•¿å¿«äºæ ‡å‡†åå·®ã€‚ è¿™ä¸€ç»“æœæ„å‘³ç€æ ‡ç­¾ä»·æ ¼ä¸æœªæ¥è¿›ä¸€æ­¥çš„èµ°åŠ¿ä¸ºä½ æä¾›æ›´å¯é çš„æ ‡ç­¾ã€‚ ä¾‹å¦‚ï¼Œè€ƒè™‘å¹³å‡å€¼ä¸ºæ­£çš„æƒ…å†µã€‚ æˆ‘ä»¬å¸Œæœ›å°†æ­£æ–¹å‘æ ‡è®°ä¸ºä¸€ä¸ªæ ‡å‡†åå·®ã€‚ ä¸ºäº†å®ç°è¿™ä¸€ç›®æ ‡ï¼Œæˆ‘ä»¬éœ€è¦é‡‡å–è¿™æ ·çš„æ–¹å¼ï¼š

![](img/425f966591bfc567678d71a26eb1bf30.png)

å¦‚æœæˆ‘ä»¬ä½¿ç”¨å¤§äºå™ªå£°æ¯”çš„å¹³æ–¹çš„ nï¼Œä»·æ ¼æ–¹å‘å°†è¢«æ­£ç¡®æ ‡è®°ä¸ºå¤§çº¦ 84%ã€‚

**åˆæˆæ•°æ®**

è®©æˆ‘ä»¬åœ¨åˆæˆæ•°æ®ä¸­æ¥éªŒè¯ä¸Šé¢çš„è®ºè¿°ï¼š

```py
import numpy as np

mu = 0.1
sig = 1.
N = 400
n_samples = 1000
x = np.arange(N)
samples = []
for i in range(n_samples):
 Â  Â rs = np.random.normal(mu, sig, N)
 Â  Â rs = np.cumsum(rs)
 Â  Â samples.append(rs)
samples = np.array(samples)
r_mu = np.mean(samples, axis=0)
r_sig = np.std(samples, axis=0)
```

æˆ‘ä»¬è€ƒè™‘è¿™ä¸ªä¾‹å­ï¼Œå‡å€¼æ˜¯ 0.1ï¼Œæ ‡å‡†å·®æ˜¯ 1.0ã€‚ç”±äºä¿¡å™ªæ¯”ä¸º 10ï¼Œæˆ‘ä»¬å¯ä»¥æ¨æ–­éœ€è¦ 100 æ­¥æ‰èƒ½å¾—åˆ°å¯é çš„æ ‡ç­¾ã€‚

![](img/2a3542dcce46b619281663610d9f3823.png)

æ­£å¦‚æˆ‘ä»¬æ‰€é¢„æœŸçš„ï¼Œåœ¨ 100 æ­¥ä¹‹åï¼Œä¸€ä¸ªæ ‡å‡†å·®å†…çš„ç‚¹ä½äºæ­£åŒºåŸŸã€‚åœ¨ 100 ä¸ªæ­¥éª¤ä¹‹å‰ï¼Œè¿™äº›ç‚¹å¾ˆå¯èƒ½æœ€ç»ˆä½äºè´ŸåŒºåŸŸã€‚å¦‚æœä½¿ç”¨çŸ­æœŸå‰å‘ç‚¹æ ‡è®°æ•°æ®ç‚¹ï¼Œåˆ™å¯èƒ½ä¼šå‡ºç°é”™è¯¯çš„æ ‡ç­¾ã€‚ 

**æ ‡ç­¾**

å¦‚æœä¸Šè¿°å£°æ˜é€‚ç”¨äºå®é™…è‚¡ç¥¨ä»·æ ¼æ•°æ®ï¼Œè®©æˆ‘ä»¬ç”¨æœªæ¥ 10 å¹´çš„ä»·æ ¼æ¥æ ‡æ³¨æ•°æ®ã€‚è¿™æœ‰æ„ä¹‰å—ï¼Ÿç­”æ¡ˆå¯èƒ½æ˜¯å¦å®šçš„ã€‚å®é™…è‚¡ç¥¨ä»·æ ¼ä¸ç»¼åˆæ•°æ®ä¹‹é—´çš„ä¸»è¦å·®å¼‚ä¹‹ä¸€æ˜¯åˆ†å¸ƒçš„å¹³ç¨³æ€§ã€‚ä»·æ ¼åˆ†å¸ƒæ€»æ˜¯éšç€å¸‚åœºæ¡ä»¶è€Œå˜åŒ–ï¼Œä½ ä½¿ç”¨çš„æœªæ¥ä»·æ ¼å°†åœ¨å¯é æ€§å’Œä¸€è‡´æ€§ä¹‹é—´è¿›è¡Œæƒè¡¡ã€‚å¿…é¡»å°†æ­¤é•¿åº¦è°ƒæ•´ä¸ºç®—æ³•çš„è¶…å‚æ•°ã€‚è™½ç„¶æˆ‘ä»¬åœ¨æ­¤ä¸è®¨è®ºå¦‚ä½•æ­£ç¡®éªŒè¯æ¨¡å‹æ€§èƒ½å¹¶è°ƒæ•´è¶…å‚æ•°ï¼Œä½†æ˜¯æˆ‘ä»¬å°†ä»‹ç»å¦‚ä½•å®ç°æ ‡è®°è‚¡ç¥¨ä»·æ ¼æ•°æ®çš„ç­–ç•¥ã€‚

ä¾‹å¦‚ï¼Œæˆ‘ä»¬ä½¿ç”¨ MicroSoft æ¯æ—¥è‚¡ç¥¨ä»·æ ¼æ•°æ®ã€‚

![](img/08accf450eada88ca767d206dc9dbbfa.png)

åœ¨ã€ŠAdvances in Financial Machine Learningã€‹ä¸­ï¼Œä½œè€…æå‡ºäº† triple barrier methodã€‚ ä»–ç»™æ•°æ®è´´ä¸Šäº†ä¸¤ä¸ªæ°´å¹³å’Œä¸€ä¸ªå‚ç›´çš„æ ‡ç­¾ã€‚ æ°´å¹³æ ‡ç­¾å®šä¹‰äº†ä»€ä¹ˆä»·æ ¼æ°´å¹³å°†è¢«åˆ†ä¸ºæ­£æˆ–è´Ÿï¼Œè€Œå‚ç›´æ ‡ç­¾åˆ™å†³å®šäº†ä½ åœ¨æ ‡ç­¾ä¸Šçœ‹å¾—æ›´è¿œçš„æ—¶é—´ã€‚ æ›´å‡†ç¡®åœ°è¯´ï¼Œeach data point is labeled by the first barrier hit by the future price. 

è¯¥æ–¹æ³•åŸºäºã€ŠAdvances in Financial Machine Learningã€‹çš„ç¬¬ä¸‰ç« ã€‚ä¸ºäº†ç®€å•èµ·è§ï¼Œçœç•¥äº†ä¸€äº›ç®—æ³•ã€‚å…·ä½“è¯·æŸ¥çœ‹å®Œæ•´ä»£ç **ï¼ˆä»£ç æ–‡æœ«ä¸‹è½½ï¼‰**

**![](img/eb487430ac180636c8e4654d8a5b137e.png)**

å‚ç›´çš„æ ‡ç­¾ï¼š

```py
def get_t1(close, timestamps, num_days):
 Â  Â t1 = close.index.searchsorted(timestamps + pd.Timedelta(days=num_days))
 Â  Â t1 = t1[t1 < close.shape[0]]
 Â  Â t1 = pd.Series(close.index[t1], index=timestamps[:t1.shape[0]])
 Â  Â return t1

# We use all data points in this article
timestamps = close.index
num_days = 10
t1 = get_t1(close, timestamps, num_days)
print(t1.head())
Output:
```

```py
Date
2000-01-03 Â  2000-01-13
2000-01-04 Â  2000-01-14
2000-01-05 Â  2000-01-18
2000-01-06 Â  2000-01-18
2000-01-07 Â  2000-01-18
Name: Date, dtype: datetime64[ns]
```

æ¯ä¸ªå…ƒç´ å®šä¹‰ä»€ä¹ˆæ—¶é—´æˆ³è¢«å®šä¹‰ä¸ºå‚ç›´ barrierã€‚

è¦å®šä¹‰æ°´å¹³ barrieï¼Œæˆ‘ä»¬éœ€è¦ä¸¤ä¸ªå‚æ•°ã€‚ å…¶ä¸­ä¸€ä¸ªæ˜¯ trgtï¼Œå®ƒå®šä¹‰äº† barrier çš„å®½åº¦æ¯”ä¾‹ã€‚ åŸºæœ¬æ€æƒ³æ˜¯æˆ‘ä»¬éœ€è¦æ ¹æ®å¸‚åœºæ¡ä»¶æ”¹å˜ barrier çš„å®½åº¦ã€‚ ä¾‹å¦‚ï¼Œå¦‚æœæˆ‘ä»¬é¢ä¸´æ³¢åŠ¨çš„å¸‚åœºï¼Œæˆ‘ä»¬éœ€è¦ä½¿ç”¨å®½å¹…ã€‚ ä½ å¯ä»¥ä½¿ç”¨æ¯æ—¥æ³¢åŠ¨ç‡æ¥è®¾ç½® trgtã€‚ æ¯æ—¥æ³¢åŠ¨ç‡é€šè¿‡æŒ‡æ•°ç§»åŠ¨å¹³å‡çº¿ä¼°ç®—ã€‚

å¦ä¸€ä¸ªå‚æ•°æ˜¯ sltpï¼šæ­¢æŸå’Œè·åˆ©ã€‚ è¿™äº›å‚æ•°ä½¿æ‚¨å¯ä»¥æ ¹æ®è‡ªå·±çš„å–œå¥½çµæ´»å®šä¹‰ barrier çš„å®½åº¦ã€‚ æ­£ï¼ˆè´Ÿï¼‰æ ‡ç­¾å±éšœç”± sltp [0]ï¼ˆsltp [1]ï¼‰ä¹˜ä»¥ trgtã€‚

```py
def get_touch_idx(close, events, sltp, molecule=None):
 Â  Â # Sample a subset with specific indices
 Â  Â if molecule is not None:
 Â  Â  Â  Â _events = events.loc[molecule]
 Â  Â else:
 Â  Â  Â  Â _events = events
 Â  Â touch_idx = pd.DataFrame(index=_events.index)
 Â  Â # Set Stop Loss and Take Profoit
 Â  Â if sltp[0] > 0:
 Â  Â  Â  Â sls = -sltp[0] * _events["trgt"]
 Â  Â else:
 Â  Â  Â  Â # Switch off stop loss
 Â  Â  Â  Â sls = pd.Series(index=_events.index)
 Â  Â if sltp[1] > 0:
 Â  Â  Â  Â tps = sltp[1] * _events["trgt"]
 Â  Â else:
 Â  Â  Â  Â # Switch off profit taking
 Â  Â  Â  Â tps = pd.Series(index=_events.index)
 Â  Â # Replace undefined value with the last time index
 Â  Â vertical_lines = _events["t1"].fillna(close.index[-1])
 Â  Â for loc, t1 in vertical_lines.iteritems():
 Â  Â  Â  Â df = close[loc:t1]
 Â  Â  Â  Â # Change the direction depending on the side
 Â  Â  Â  Â df = (df / close[loc] - 1) * _events.at[loc, 'side']
 Â  Â  Â  Â touch_idx.at[loc, 'sl'] = df[df < sls[loc]].index.min()
 Â  Â  Â  Â touch_idx.at[loc, 'tp'] = df[df > tps[loc]].index.min()
 Â  Â touch_idx['t1'] = _events['t1'].copy(deep=True)
 Â  Â return touch_idx
```

get_touch_idx è·å–æœªæ¥ä»·æ ¼ä½•æ—¶ä»¥åŠä½•ç§ barrierã€‚

```py
import pandas as pd
from finance_ml.multiprocessing import mp_pandas_obj

def get_events(close, timestamps, sltp, trgt, min_ret=0,
 Â  Â  Â  Â  Â  Â  Â  num_threads=1, t1=None, side=None):Â 
 Â  Â # Get sampled target values
 Â  Â trgt = trgt.loc[timestamps]
 Â  Â trgt = trgt[trgt > min_ret]
 Â  Â if len(trgt) == 0:
 Â  Â  Â  Â return pd.DataFrame(columns=['t1', 'trgt', 'side'])
 Â  Â # Get time boundary t1
 Â  Â if t1 is None:
 Â  Â  Â  Â t1 = pd.Series(pd.NaT, index=timestamps)
 Â  Â # slpt has to be either of integer, list or tuple
 Â  Â if isinstance(sltp, list) or isinstance(sltp, tuple):
 Â  Â  Â  Â _sltp = sltp[:2]
 Â  Â else:
 Â  Â  Â  Â _sltp = [sltp, sltp]
 Â  Â # Define the side
 Â  Â if side is None:
 Â  Â  Â  Â # Default is LONG
 Â  Â  Â  Â _side = pd.Series(1, index=trgt.index)
 Â  Â else:
 Â  Â  Â  Â _side = side.loc[trgt.index]
 Â  Â events = pd.concat({'t1': t1, 'trgt': trgt, 'side': _side}, axis=1)
 Â  Â events = events.dropna(subset=['trgt'])
 Â  Â time_idx = mp_pandas_obj(func=get_touch_idx,
 Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  pd_obj=('molecule', events.index),
 Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  num_threads=num_threads,
 Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  close=close, events=events, sltp=_sltp)
 Â  Â # Skip when all of barrier are not touched
 Â  Â time_idx = time_idx.dropna(how='all')
 Â  Â events['type'] = time_idx.idxmin(axis=1)
 Â  Â events['t1'] = time_idx.min(axis=1)
 Â  Â if side is None:
 Â  Â  Â  Â events = events.drop('side', axis=1)
 Â  Â return events
from finance_ml.stats import get_daily_vol

vol = get_daily_vol(close)
print('volatility')
print(vol.head())

events = get_events(close, timestamps, [2, 2], vol, min_ret=0,
 Â  Â  Â  Â  Â  Â  Â  Â  Â  Â num_threads=16, t1=t1, side=None)
print('events')
print(events.head())
```

```py
volatility
Date
2000-01-04 Â  Â  Â  Â  NaN
2000-01-05 Â  Â 0.031374
2000-01-06 Â  Â 0.025522
2000-01-10 Â  Â 0.024588
2000-01-11 Â  Â 0.022054
Name: Close, dtype: float64

events
 Â  Â  Â  Â  Â  Â  Â  Â  Â  t1 Â  Â  Â trgt type
Date Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â 
2000-01-05 2000-01-12 Â 0.031374 Â  sl
2000-01-06 2000-01-18 Â 0.025522 Â  t1
2000-01-10 2000-01-12 Â 0.024588 Â  sl
2000-01-11 2000-01-18 Â 0.022054 Â  tp
2000-01-12 2000-01-14 Â 0.020946 Â  tp
```

get_events åœ¨å†…éƒ¨ä½¿ç”¨ get_torch_idx å¹¶è·å–æ ‡ç­¾ã€‚

Output,Â events, contains the followings: -Â t1, when the barrier is touched -Â trgt, scale used to define horizontal barriers -Â type, which barrier is touchedã€‚

æ¥ä¸‹æ¥ï¼Œæˆ‘ä»¬å®šä¹‰ get_sizesï¼Œå®ƒä½¿ç”¨äº‹ä»¶ç”Ÿæˆæ•°å­—æ ‡ç­¾ã€‚ å½“æ ‡è®°ç‚¹å‡»ä¸­å‚ç›´ barrier æ—¶ï¼Œæœ‰ä¸¤ç§å¯èƒ½çš„é€‰æ‹©ã€‚One of them is assigning the sign of the return at the hitting point. The other way is using another label for hitting the vertical barrier.

åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬ä½¿ç”¨å‰ä¸€ç§æ–¹æ³•è·å¾—äºŒè¿›åˆ¶æ ‡ç­¾ã€‚

```py
def get_sizes(close, events, sign_label=True):Â Â 
 Â  Â # Prices algined with events
 Â  Â events = events.dropna(subset=['t1'])
 Â  Â # All used indices
 Â  Â time_idx = events.index.union(events['t1'].values).drop_duplicates()
 Â  Â close = close.reindex(time_idx, method='bfill')
 Â  Â # Create out object
 Â  Â out = pd.DataFrame(index=events.index)
 Â  Â out['ret'] = close.loc[events['t1'].values].values / close.loc[
 Â  Â  Â  Â events.index].values - 1.
 Â  Â if 'side' in events:
 Â  Â  Â  Â out['ret'] *= events['side']
 Â  Â  Â  Â out['side'] = events['side']
 Â  Â out['size'] = np.sign(out['ret'])
 Â  Â if sign_label:
 Â  Â  Â  Â out['size'] = np.sign(out['ret'])
 Â  Â  Â  Â out.loc[out['ret'] == 0, 'size'] = 1.
 Â  Â else:
 Â  Â  Â  Â # 0 when touching vertical line
 Â  Â  Â  Â out['size'].loc[events['type'] == 't1'] = 0
 Â  Â if 'side' in events:
 Â  Â  Â  Â out.loc[out['ret'] <= 0, 'size'] = 0
 Â  Â return out

labels = get_sizes(close, events, sign_label=True)
print(labels.head())
```

```py
ret Â size
Date Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â 
2000-01-05 -0.070293 Â -1.0
2000-01-06 Â 0.048273 Â  1.0
2000-01-10 -0.057372 Â -1.0
2000-01-11 Â 0.054311 Â  1.0
2000-01-12 Â 0.060864 Â  1.0
```

**é¢„æµ‹**

æœ€åï¼Œæˆ‘ä»¬æ¥åˆ°é¢„æµ‹é˜¶æ®µã€‚ åœ¨è¿™ä¸ªé˜¶æ®µï¼Œæˆ‘ä»¬åªæ˜¯é€šè¿‡åˆ†å‰²æ•°æ®é›†æµ‹è¯•ç»“æœï¼šè®­ç»ƒå’Œæµ‹è¯•ã€‚We simply use trailing histories of volume and close for input featuresã€‚

![](img/a88936bb9371eaf051e89bde7701082a.png)

æˆ‘ä»¬å°†æ•°æ®åˆ†ä¸º 2000-01-01 è‡³ 2017-08-31 è¿›è¡Œè®­ç»ƒï¼Œ2017-09-01 è‡³ 2018-03-31 è¿›è¡Œæµ‹è¯•ã€‚

```py
# Separate data time stamps
def get_partial_index(df, start=None, end=None):
 Â  Â if start is not None:
 Â  Â  Â  Â df = df.loc[df.index >= start]
 Â  Â if end is not None:
 Â  Â  Â  Â df = df.loc[df.index <= end]
 Â  Â return df.index

train_end = '2017-08-31'
test_start = '2017-09-01'
train_idx = get_partial_index(df, end=train_end)
test_idx = get_partial_index(df, start=test_start)

def generate_features(close, volume, label, timestamps, timelag):
 Â  Â index = close.index
 Â  Â data = []
 Â  Â for i in range(1, timelag):
 Â  Â  Â  Â # Normalize
 Â  Â  Â  Â data.append(close.shift(i).values / close.values)
 Â  Â  Â  Â data.append(volume.shift(i).values / volume.values)

 Â  Â features = pd.DataFrame(np.stack(data, axis=1), index=index)
 Â  Â features = features.loc[timestamps].dropna()
 Â  Â label = label.dropna()
 Â  Â time_idx = features.index & label.index
 Â  Â y = label.loc[time_idx].values
 Â  Â label_map = {-1: 0, 1: 1}
 Â  Â y = np.array([label_map[y_i] for y_i in y]).astype(int)
 Â  Â X = features.loc[time_idx].values
 Â  Â return X, y

timelag = 30
train_X, train_y = generate_features(close, volume, labels['size'], train_idx, timelag=timelag)
test_X, test_y = generate_features(close, volume, labels['size'], test_idx, timelag=timelag)
```

Note that close and volume features are normalized with the current value. Intuitively, the scales of close and volume themselves do not have any meanings. The value in comparison to the current close and value are rather essential information. This normalization allows you to build models irrelevant to the scales.

æˆ‘ä»¬ä½¿ç”¨ PyTorch å’Œå·¥å…·åº“ torch_utils æ„å»ºç¥ç»ç½‘ç»œåˆ†ç±»å™¨**ï¼ˆä»£ç æ–‡æœ«ä¸‹è½½ï¼‰**

```py
import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.utils.data as tdata
import torch.optim as optim
from sklearn.metrics import accuracy_score

from torch_utils.datasets import NumpyDataset
from torch_utils.training import train_step, test_step

input_dim = train_X.shape[1]
output_dim = 1

class Net(nn.Module):
 Â  Â def __init__(self):
 Â  Â  Â  Â super(Net, self).__init__()
 Â  Â  Â  Â self.fc1 = nn.Linear(input_dim, 16)
 Â  Â  Â  Â self.bn1 = nn.BatchNorm1d(16)
 Â  Â  Â  Â self.fc2 = nn.Linear(16, 8)
 Â  Â  Â  Â self.bn2 = nn.BatchNorm1d(8)
 Â  Â  Â  Â self.fc3 = nn.Linear(8, output_dim)

 Â  Â def forward(self, x):
 Â  Â  Â  Â x = F.relu(self.bn1(self.fc1(x)))
 Â  Â  Â  Â x = F.relu(self.bn2(self.fc2(x)))
 Â  Â  Â  Â x = self.fc3(x)
 Â  Â  Â  Â return x

 Â  Â def predict(self, x, threshold=.5):
 Â  Â  Â  Â x = self.forward(x)
 Â  Â  Â  Â x = F.sigmoid(x)
 Â  Â  Â  Â return x > threshold

batch_size = 32
train_loader = tdata.DataLoader(NumpyDataset(train_X, train_y[:, None].astype(float)),
 Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  batch_size=batch_size, shuffle=True)
test_loader = tdata.DataLoader(NumpyDataset(test_X, test_y[:, None].astype(float)),
 Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â batch_size=batch_size)

n_epochs = 1000
model = Net()
optimizer = optim.Adam(model.parameters(), lr=1e-3)
loss_func = F.binary_cross_entropy_with_logits
score_func = accuracy_score
for i in range(n_epochs):
 Â  Â train_step(model, train_loader, optimizer,
 Â  Â  Â  Â  Â  Â  Â  loss_func=loss_func, score_func=score_func,
 Â  Â  Â  Â  Â  Â  Â  epoch=i, log_interval=0, silent=True)
 Â  Â if i % 100 == 0:
 Â  Â  Â  Â test_step(model, test_loader, loss_func=loss_func, score_func=score_func)

model.eval()
output = model.predict(torch.tensor(test_X).float())
accuracy = accuracy_score(test_y, output)
print(f'Test Accuracy: {accuracy:.4g}')

Test Accuracy: 0.5229
```

ç”±æ­¤äº§ç”Ÿçš„æµ‹è¯•ç²¾åº¦ä¸º 0.5229ï¼Œè¿™å¹¶ä¸æ¯”å¶ç„¶ã€‚å…¶ä¸­ä¸€ä¸ªå¯èƒ½çš„åŸå› æ˜¯è¿‡åº¦æ‹Ÿåˆã€‚æˆ‘ä»¬éœ€è¦è°ƒæ•´æ¨¡å‹ä½“ç³»ç»“æ„å’Œè®­ç»ƒæ¨¡å‹çš„æ–¹æ³•ã€‚æˆ‘ä»¬ä¹Ÿå¯ä»¥è®¤ä¸ºï¼Œåˆ†ç±»å¯¹é‡‘èæ¥è¯´å¯èƒ½æ˜¯å›°éš¾çš„ã€‚å³ä½¿æ¨¡å‹èƒ½å¤Ÿå­¦ä¹ åˆ†å¸ƒï¼Œåœ¨å˜ˆæ‚çš„æƒ…å†µä¸‹ä¹Ÿå¾ˆéš¾é¢„æµ‹æ­£ç¡®çš„æ ‡ç­¾ã€‚

**å›å½’æ–¹æ³•**

æ­£å¦‚æˆ‘ä»¬å‰é¢çœ‹åˆ°çš„ï¼Œåˆ†ç±»æ˜¯å›°éš¾çš„ã€‚é¢„æµ‹ä»·æ ¼æˆ–å›æŠ¥æœ¬èº«å¯èƒ½æ›´æœ‰æ„ä¹‰ã€‚æˆ‘ä»¬å°è¯•åŸºäºä¸åˆ†ç±»ç›¸åŒçš„è¾“å…¥ç‰¹å¾æ¥é¢„æµ‹æœªæ¥çš„æ”¶ç›Šã€‚ 

ä¸ºäº†æ¯”è¾ƒä¸åŒ num_day å‚æ•°çš„æ€§èƒ½:1ã€2ã€3ã€4ã€5ã€10ã€20ï¼Œæˆ‘ä»¬ä½¿ç”¨è‡ªç”±å°ºåº¦åº¦é‡ np.meanï¼ˆnp.absï¼ˆy_pred-yï¼‰ï¼‰/ np.stdï¼ˆyï¼‰ï¼Œç»“æœå¦‚ä¸‹ï¼š

![](img/9a7e885b7f41e46c544c4e3a156991e7.png)

![](img/1775d55bb1b014cffc471270b946849f.png)

![](img/23df86ab377ae8d84064c3ab60bb6b52.png)

![](img/7329eefe4d73b4831ee60a87cda06c35.png)

![](img/bfb474b6aa9e22e9c63f18f7cb868324.png)

![](img/994b420102a4e63813add9357f03f3a4.png)

æˆ‘ä»¬æ²¡æœ‰çœ‹åˆ°å‘å‰å¤©æ•°å’Œè¡¨ç°ä¹‹é—´å­˜åœ¨ä»»ä½•å…·ä½“å…³ç³»ã€‚ä¸ºäº†æ‰¾åˆ°æ›´å¯é çš„ç»“æœï¼Œæˆ‘ä»¬éœ€è¦éªŒè¯æ¨¡å‹å¹¶è°ƒæ•´è¶…å‚æ•°ã€‚

**æ€»ç»“**

æˆ‘ä»¬åœ¨æœ¬æ–‡ä¸­çœ‹åˆ°çš„ç»“æœçœ‹èµ·æ¥å¾ˆä¸€èˆ¬ã€‚ä¸»è¦è€ƒè™‘ä»¥ä¸‹åŸå› ï¼š

1ã€æ•°æ®é‡å°‘ã€‚

2ã€éœ€è¦é€‰æ‹©åˆé€‚çš„æ¨¡å‹ã€‚ 

é’ˆå¯¹ç¬¬äºŒç‚¹ï¼Œæˆ‘ä»¬éœ€è¦å»ºç«‹æ­£ç¡®çš„æ–¹æ³•æ¥éªŒè¯æ¨¡å‹æ€§èƒ½ï¼Œæˆ‘ä»¬å¯ä»¥è€ƒè™‘è´å¶æ–¯æ–¹æ³•ã€‚ç”±äºé‡‘èæ•°æ®çš„å™ªå£°æœ¬è´¨ï¼Œè´å¶æ–¯æ–¹æ³•å¯å¸®åŠ©ä½ é¿å…è¿‡åº¦æ‹Ÿåˆï¼Œå¹¶ä¸ºä½ æä¾›æ›´åˆé€‚çš„é¢„æµ‹ç½®ä¿¡æ°´å¹³ã€‚åç»­æˆ‘ä»¬ä¼šç»§ç»­è®¨è®ºè¿™ä¸ªé—®é¢˜ã€‚

Tomoaki

**ä¸‹è½½ä¹¦ç±+ä»£ç **

åœ¨**åå°**è¾“å…¥

**å¹´åº¦ç³»åˆ—å››**

**åå°è·å–æ–¹å¼ä»‹ç»**

![](img/4842921f26900ec7d873bb68dc9b4fe9.png)

**çŸ¥è¯†åœ¨äºåˆ†äº«**

**åœ¨é‡åŒ–æŠ•èµ„çš„é“è·¯ä¸Š**

**ä½ ä¸æ˜¯ä¸€ä¸ªäººåœ¨æˆ˜æ–—**