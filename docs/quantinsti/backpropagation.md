# 理解反向传播

> 原文：<https://blog.quantinsti.com/backpropagation/>

由[瓦伦·迪瓦卡](https://www.linkedin.com/in/varun-divakar-b862a667/)

在之前的[博客](/forward-propagation-neural-networks)中，我们已经学习了如何执行[正向传播](/forward-propagation-neural-networks/)。在这篇博客中，我们将继续同样的例子，并使用反向传播技术纠正预测中的错误。

### **什么是反向传播？**

回想一下，我们创建了一个 3 层(2 个训练、2 个隐藏和 2 个输出)网络。但是一旦我们在网络中加入了偏向项，我们的网络就有了以下的形状。

![](img/5abf014ff19ec27d95229e34f60d9b0d.png)

在完成正向传播之后，我们发现我们的模型是不正确的，因为它给类 0 分配了比类 1 更大的概率。现在，我们将使用反向传播来纠正这个问题。

### 为什么要反向传播？

在前向传播期间，我们随机初始化权重。这就是我们模型的问题所在。假设我们随机初始化我们的权重，我们作为输出得到的概率也是随机的。因此，我们必须有一些方法使我们的重量更准确，这样我们的输出将更准确。我们使用反向传播来调整这些随机权重。

### **损失函数**

在进行反向传播时，我们需要计算我们的预测有多好。为此，我们使用损失/成本函数的概念。损失函数是我们的预测值和实际值之差。我们创建一个损失函数来寻找该函数的最小值，以优化我们的模型并提高我们预测的准确性。在本文中，我们将讨论一种称为梯度下降的技术，用于减少这种损失。根据问题，我们选择某种类型的损失函数。在本例中，我们将使用均方误差或 MSE 方法来计算损失。在 MSE 方法中，损失计算为实际值和预测值之差的平方和。

**损失=总和(预测-实际)**

让我们说，我们在预测中的损失或错误看起来像这样:

![Error in prediction](img/a7b45afc34d510bb484e85523a07757a.png)

我们的目标是通过改变权重来减少损失，使得损失收敛到尽可能低的值。我们试图以可控的方式减少损失，通过一小步一小步地减少损失。这个过程被称为梯度下降(GD)。在执行 GD 时，我们需要知道重量应该移动的方向。换句话说，我们需要决定是否增加或减少权重。为了知道这个方向，我们必须对损失函数求导。这给了我们功能变化的方向。下面是一个显示如何使用梯度下降更新权重的等式。

![Weights using gradient descent](img/eb04ac082d0f4f45d1c3e4dc248f2e5d.png)

这里，α项被称为学习率，并乘以我们的损失函数(J)的导数(请记住，我们已经在导数的链式法则文档中讨论了如何计算函数的导数)。我们从初始重量中减去这个乘积来更新它。还需要注意的是，这种形式的导数称为偏导数。在求偏导数时，其余项被视为常数。

如果您将上图中的曲线视为我们关于某个特征的损失函数，那么我们可以说，导数是我们损失函数的斜率，代表 y 相对于 x 的瞬时变化率。在执行反向传播时，我们要找到损失函数相对于权重的导数。换句话说，我们在问“当我们改变一个单位的权重时，我们的损失函数是如何变化的？”。然后我们将它乘以学习率α。学习率控制向最小值移动的步长。直觉上，如果我们有一个大的学习率，我们就会迈出大的步伐。相比之下，如果我们的学习率很低，我们就会小步前进。因此，学习率乘以导数可以被认为是在损失函数的域上采取的步骤。一旦我们完成了这一步，我们就更新我们的权重。并且对每个特征重复该过程。

在下面的例子中，我们将逐步演示反向传播的过程。

### **逐步反向传播**

让我们将反向传播的过程分解成可操作的步骤。

1.  计算损失函数；(即神经网络的总误差)
2.  计算总误差/损失函数相对于每个权重的偏导数
3.  执行梯度下降并更新我们的权重

我们需要做的第一件事是计算我们的误差。我们使用 MSE 公式定义误差如下:

**误差=(目标-输出)**

这是单个类的错误。如果我们想计算一个例子中两类预测概率的误差。然后我们将误差组合如下。

**Total Error = Error₁ + Error₂**

其中 Error₁和 Error₂代表两类预测的误差。回想一下，我们的输出是 a3，计算结果为:

![Error in prediction](img/1261e33ef71de8b8919488dc85e85ef2.png)

表示对类别 1 的预测小于类别 0。在我们的例子中，我们声明类 1 应该有更大的概率，因此是我们预测的类标签。为了进一步说明这一点，为了便于理解，我们为类 0 和类 1 创建一些假设的目标概率值。

让我们为输出层概率指定以下目标值(t ):

![Target values for output probabilities](img/111ffad4dfdd45bba97a2c2053c1ad20.png)

现在，让我们来计算误差。

![Error calculation](img/aadbdf210f5d07868f0a209fed9d20c8.png)

因此，预测的总误差为 **0.009895** 。

每个误差包含预测值，每个预测值是前一层的权重和输入的函数。延伸这一逻辑，可以说我们的总误差是不同权重的函数，或者换句话说是多元的。因为我们有多个重量，我们必须使用偏导数，或者找出一个特定重量的变化如何改变我们的总误差方程。这意味着我们必须使用链式法则来分解错误。

一旦我们计算出误差函数相对于权重的偏导数，我们就可以应用梯度下降方程来更新权重。我们对训练数据中的每个权重和所有示例重复这一过程。这个过程重复许多次，所有例子的每一次这样的经过被称为一个时期。我们执行这些过程，直到损失收敛，或者损失函数停止改善。

现在我们已经理解了反向传播的过程，让我们来实现它。为了进行反向传播，我们需要找到误差函数相对于每个权重的偏导数。回想一下，我们总共有八个权重(即在添加偏差项之前)。我们有两个从第一次输入到隐藏层的权重和两个从第二次输入到隐藏层的权重。从隐藏层到输出层，我们还有四个权重。让我们将这些重量标记如下。

![Labeling weights](img/0ad70f34f77459bfa75627270a3c27fd.png)

我们将第一个输入神经元的权重称为 w1 和 w3，将第二个输入神经元的权重称为 w2 和 w4。隐藏层的第一个神经元的权重是 w5 和 w7，隐藏层的第二个神经元的权重是 w6 和 w8。在这个例子中，我们将演示权重 w5 的反向传播。请注意，我们可以使用相同的过程来更新网络中的所有其他权重。

让我们看看如何使用链式法则来表示损失相对于重量 w5 的偏导数。

![Chain rule](img/0fb187ab07bd6de35ea013e09c9e5d4c.png)

其中下标中的“I”表示输出层中的第一个神经元。为了计算链的一阶导数，我们将总误差方程表示为:

![Total error equation](img/b06703726ee4dc374bf7f920b7c354e4.png)

这里下标中的 j 表示输出层中的第二个神经元。我们的误差方程对输出的偏导数为:

![Partial error equation](img/5f0edefd002cfcfa41d727e0f9cd6c05.png)

代入相应的值，我们将得到:

接下来，我们找到等式中的第二项。回想一下，在前向传播步骤中，我们使用 sigmoid 或逻辑函数作为激活函数。因此，为了计算链中的第二个元素，我们必须对其输入取 sigmoid 的偏导数。

现在，回想一下 sigmoid 函数如下:

![Sigmoid function](img/31eaff6c2df1fed9df9fe63277c6974c.png)

这个激活函数的导数也可以写成如下:

![Derivative of activation function](img/03b4b8650df27dfdfb5e3d55c6609584.png)

导数可应用于链式法则中的第二项，如下所示:

![Second term in chain rule](img/91aedd2107790f6d7e400eaa3440e7cd.png)

将输出值代入上式，我们得到:

**0.7333(1 - 0.733) = 0.1958**

接下来，我们计算链式方程中的最后一项。我们的第三项包含了我们用来传递给 sigmoid 激活函数的输入。回想一下，在前向传播期间，隐藏层的输出乘以权重。这些线性组合然后被传递到激活函数和最终输出层。

记住，这些权重由θ2 给出。

![](img/8d5bcd9e0c46b3ba22e8f340c1aac95d.png)

假设隐藏层的输出如下所示。

![Output of hidden layer](img/810b9aa7e9489bb6d1f6b4ced3fd83a1.png)

要直观显示下面的矩阵乘法，请参见下图:

这里，H1 和 H2 表示隐含层神经元。

我们的第三项方程涉及节点输入相对于第五个权重的偏导数。如上所示，我们的第五个权重与隐藏层中的第二个神经元相关。因此，当我们对 w5 进行偏微分时，所有其他重量都被视为常数，其导数为零。

因此，当输入是我们从θ2 和隐藏层的输出的组合中得到的值时，结果看起来像是:

![Hidden neuron output](img/23af40a400b0094fc87cc50d4adff09d.png)

![Visualize](img/629f58476ba4b07f2b5a7f85e03c7d29.png)

其中输出是隐藏神经元 H1 的输出。

现在，我们已经找到了方程中最后一项的值，我们可以计算所有三项的乘积，以导出误差函数 w5 的偏导数。

![Partial derivative of error function](img/caf932f939935bcb7729748975fdf00c.png)

我们现在可以在所示的梯度下降方程中使用该偏导数来调整权重 w5。

![w5 gradient descent equation](img/c4a26fbdeb2625d01b6d189b26d848f0.png)

因此，更新后的权重 w5 为 0.3995。如你所见，w5 的值变化很小，因为我们的学习率(0.1)很小。w5 值的这一微小变化可能不会对最终概率产生太大影响。但是，如果对两个示例多次执行相同的过程，并且针对每次运行(时期)调整权重，那么我们将得到具有预期预测的最终[神经网络](https://quantra.quantinsti.com/course/neural-networks-deep-learning-trading-ernest-chan)。

### **更新我们的模型**

在完成反向传播并多次更新所有层上的权重矩阵之后，我们得到对应于最小值的以下权重矩阵。

![weight matrices](img/99887cc4b6baf82497dcc23ac6b63d17.png)

我们现在可以使用这些权重并完成正向传播，以达到最佳可能输出。回想一下，该过程的第一步是将权重与输入相乘，如下所示。

![Theta](img/09a3b2dea032f200036de86f0a49b1ff.png)

回想一下，我们对 X 矩阵进行转置，以确保权重对齐。这里，我们对θ1 使用新的更新权重，我们的矩阵乘法将如下所示:

![Updated multiplication table](img/8a81a5c850ec80b2600460beddd9b29c.png)

这是我们新的 z 矩阵，或者说第一层的输出。

回想一下，我们向前传播的下一步是将 sigmoid 函数元素应用于我们的矩阵。这将产生以下结果:

![Application of sigmoid function](img/8600bf456c1cceb837da82c37d8cc45c.png)

这里， a 是隐藏层的输出。同样，这是我们的激活层，并将作为最终层的新输入。我们再次添加回我们的偏差项，因此我们的新 a 如下所示:

![Final layer](img/25c165b14c5bc62cb3fbcce2a49d45df.png)

现在，我们将使用θ2 权重矩阵的新值来创建输出层的输入。我们现在执行下面的计算来得到我们的 z 3 的新值，或者输出矩阵。

![Output matrix](img/6efce65561626c242e37bc9095ddebc8.png)

![Output matrix 1](img/4a3c9fd84bef246537c295db6181f68b.png)

在该矩阵乘法之后，我们逐元素地应用 sigmoid 函数，并得出最终输出矩阵如下。

![Final back propagation](img/a1f2ce46c0372e1df22ca79f413a88ff.png)

我们在这里可以看到，在执行反向传播并使用梯度下降来更新我们在每一层的权重之后，我们得到了与我们的初始假设一致的类别 1 的预测。

如果你想学习如何在交易中应用神经网络，那么请查看我们关于交易中的[神经网络的新课程。](https://quantra.quantinsti.com/course/neural-networks-deep-learning-trading-ernest-chan)