# 时间序列预测与递归神经网络在 Keras 的应用基于 Python

> 原文：[`mp.weixin.qq.com/s?__biz=MzAxNTc0Mjg0Mg==&mid=2653284849&idx=1&sn=c5b70fa679397e654c1468ea2fb8f3f1&chksm=802e2be4b759a2f2f6acac4efb6f780ab0eca54cf32dbc1fc4fade7ba43e284ff8592fb35c9a&scene=27#wechat_redirect`](http://mp.weixin.qq.com/s?__biz=MzAxNTc0Mjg0Mg==&mid=2653284849&idx=1&sn=c5b70fa679397e654c1468ea2fb8f3f1&chksm=802e2be4b759a2f2f6acac4efb6f780ab0eca54cf32dbc1fc4fade7ba43e284ff8592fb35c9a&scene=27#wechat_redirect)

![](img/cb3bd660442e6bc134fbecf2477c43d1.png)

**编辑部**

微信公众号

**关键字**全网搜索

**『量化投资』：排名第一**

**『量       化』：排名第二**

**『机器学习』：排名第三**

我们会再接再厉

成为全网**优质的**金融、技术类公众号

**编辑整理**

编辑部：西西

**原文作者**

Jason Brownlee

**问题描述**

问题为：国际客运量预测。该数据范围从 1949 年 1 月至 1960 年 12 月。

下面是一个样本的文件

![](img/7cc780de03f8ede002cb76254f8960f2.png)

但是我们感兴趣的不是日期 ， 因为每个被观察的相同间距隔开的一个月。因此，我们可以排除加载数据集的第一列。

![](img/d9b27e6f2fc33ad497c66fba11d582b1.png)

你可以看到数据集有一个上升趋势的。你还可以看到一些周期性等。

![](img/1e2b9bf566dcf31150a823843bef65d9.png)

**长短期记忆网络**

The Long Short-Term Memory network, or LSTM network, is a recurrent neural network that is trained using Backpropagation Through Time and overcomes the vanishing gradient problem.

As such, it can be used to create large recurrent networks that in turn can be used to address difficult sequence problems in machine learning and achieve state-of-the-art results.

Instead of neurons, LSTM networks have memory blocks that are connected through layers.

A block has components that make it smarter than a classical neuron and a memory for recent sequences. A block contains gates that manage the block’s state and output. A block operates upon an input sequence and each gate within a block uses the sigmoid activation units to control whether they are triggered or not, making the change of state and addition of information flowing through the block conditional.

There are three types of gates within a unit:

*   Forget Gate: conditionally decides what information to throw away from the block.

*   Input Gate: conditionally decides which values from the input to update the memory state.

*   Output Gate: conditionally decides what to output based on input and the memory of the block.

Each unit is like a mini-state machine where the gates of the units have weights that are learned during the training procedure.

You can see how you may achieve sophisticated learning and memory from a layer of LSTMs, and it is not hard to imagine how higher-order abstractions may be layered with multiple such layers.

**LSTM 网络回归**

We can phrase the problem as a regression problem.

That is, given the number of passengers (in units of thousands) this month, what is the number of passengers next month?

We can write a simple function to convert our single column of data into a two-column dataset: the first column containing this month’s (t) passenger count and the second column containing next month’s (t+1) passenger count, to be predicted.

Before we get started, let’s first import all of the functions and classes we intend to use. This assumes a working SciPy environment with the Keras deep learning library installed.

![](img/ffaf31a5faac2cd926647e3f3045c7c3.png)

Before we do anything, it is a good idea to fix the random number seed to ensure our results are reproducible.

![](img/b8af077ac81a5ed881e450ee08c2d547.png)

We can also use the code from the previous section to load the dataset as a Pandas dataframe. We can then extract the NumPy array from the dataframe and convert the integer values to floating point values, which are more suitable for modeling with a neural network.

![](img/c08c081d3da46364959f5cf35353ee32.png)

LSTMs are sensitive to the scale of the input data, specifically when the sigmoid (default) or tanh activation functions are used. It can be a good practice to rescale the data to the range of 0-to-1, also called normalizing. We can easily normalize the dataset using the MinMaxScalerpreprocessing class from the scikit-learn library.

![](img/7d758f50e5d196b8fcd7a5726b2930d4.png)

After we model our data and estimate the skill of our model on the training dataset, we need to get an idea of the skill of the model on new unseen data. For a normal classification or regression problem, we would do this using cross validation. 

With time series data, the sequence of values is important. A simple method that we can use is to split the ordered dataset into train and test datasets. The code below calculates the index of the split point and separates the data into the training datasets with 67% of the observations that we can use to train our model, leaving the remaining 33% for testing the model.

![](img/dee4620bf65fcde231d282d804a8da9d.png)

Now we can define a function to create a new dataset, as described above.

The function takes two arguments: the dataset, which is a NumPy array that we want to convert into a dataset, and the look_back, which is the number of previous time steps to use as input variables to predict the next time period — in this case defaulted to 1.

This default will create a dataset where X is the number of passengers at a given time (t) and Y is the number of passengers at the next time (t + 1).

It can be configured, and we will by constructing a differently shaped dataset in the next section.

![](img/e82bf14ed56f83eddd9600fa72a6b6fb.png)

Let’s take a look at the effect of this function on the first rows of the dataset (shown in the unnormalized form for clarity).

![](img/0318d9fb315e7dae3d16e7418d0299d9.png)

If you compare these first 5 rows to the original dataset sample listed in the previous section, you can see the X=t and Y=t+1 pattern in the numbers. 

Let’s use this function to prepare the train and test datasets for modeling.

![](img/3d314f4c331562326dbb04f98db3ae60.png)

The LSTM network expects the input data (X) to be provided with a specific array structure in the form of: [samples, time steps, features].

Currently, our data is in the form: [samples, features] and we are framing the problem as one time step for each sample. We can transform the prepared train and test input data into the expected structure using numpy.reshape() as follows:

![](img/4155158002784f82eea76a3bbab7600c.png)

We are now ready to design and fit our LSTM network for this problem.

The network has a visible layer with 1 input, a hidden layer with 4 LSTM blocks or neurons, and an output layer that makes a single value prediction. The default sigmoid activation function is used for the LSTM blocks. The network is trained for 100 epochs and a batch size of 1 is used.

![](img/7b70985d2a0abd418a1217e9a3ee5f14.png)

Once the model is fit, we can estimate the performance of the model on the train and test datasets. This will give us a point of comparison for new models.

Note that we invert the predictions before calculating error scores to ensure that performance is reported in the same units as the original data (thousands of passengers per month).

![](img/34c4e397d9d73eb9066b02dac75c8f97.png)

Finally, we can generate predictions using the model for both the train and test dataset to get a visual indication of the skill of the model.

Because of how the dataset was prepared, we must shift the predictions so that they align on the x-axis with the original dataset. Once prepared, the data is plotted, showing the original dataset in blue, the predictions for the training dataset in green, and the predictions on the unseen test dataset in red.

![](img/22667341f31b3690369af1d5a5d19390.png)

We can see that the model did an excellent job of fitting both the training and the test datasets.

![](img/280bb6508a62f18278d690dda40045f8.png)

For completeness, below is the entire code example.

![](img/9f01b05601fa0964471731c0dd7d9a34.png)

![](img/5d7cdd8a8fd9d79d322622b5f9bc9732.png)

Running the example produces the following output.

![](img/70e7288ab1d883a595fdafcf6a2682ba.png)We can see that the model has an average error of about 23 passengers (in thousands) on the training dataset, and about 52 passengers (in thousands) on the test dataset. Not that bad.

**LSTM 窗口的使用方法**

We can also phrase the problem so that multiple, recent time steps can be used to make the prediction for the next time step.

This is called a window, and the size of the window is a parameter that can be tuned for each problem.

For example, given the current time (t) we want to predict the value at the next time in the sequence (t+1), we can use the current time (t), as well as the two prior times (t-1 and t-2) as input variables.

When phrased as a regression problem, the input variables are t-2, t-1, t and the output variable is t+1.

The create_dataset() function we created in the previous section allows us to create this formulation of the time series problem by increasing the look_back argument from 1 to 3.

A sample of the dataset with this formulation looks as follows:

![](img/d14ed00db8226d8799de09f8582c7e34.png)

We can re-run the example in the previous section with the larger window size. The whole code listing with just the window size change is listed below for completeness.

![](img/7560fb15914226a16e184156a2e53273.png)

![](img/6fed986fb88d59405c69288159217946.png)

![](img/43be08f1d7eede5ac73f081b27140840.png)

We can see that the error was increased slightly compared to that of the previous section. The window size and the network architecture were not tuned: this is just a demonstration of how to frame a prediction problem.  

![](img/a7732651fcc1d135688a2d46f6f5749f.png)

**LSTM with Memory Between Batches**

The LSTM network has memory, which is capable of remembering across long sequences.

Normally, the state within the network is reset after each training batch when fitting the model, as well as each call to model.predict() or model.evaluate().

We can gain finer control over when the internal state of the LSTM network is cleared in Keras by making the LSTM layer “stateful”. This means that it can build state over the entire training sequence and even maintain that state if needed to make predictions.

It requires that the training data not be shuffled when fitting the network. It also requires explicit resetting of the network state after each exposure to the training data (epoch) by calls tomodel.reset_states(). This means that we must create our own outer loop of epochs and within each epoch call model.fit() and model.reset_states(). For example:

![](img/3b6cde928b92f4aebe1b4a0c043d26ce.png)

Finally, when the LSTM layer is constructed, the stateful parameter must be set True and instead of specifying the input dimensions, we must hard code the number of samples in a batch, number of time steps in a sample and number of features in a time step by setting thebatch_input_shape parameter. For example:

![](img/6245cb2277814398b6c09811827a2210.png)

Finally, when the LSTM layer is constructed, the stateful parameter must be set True and instead of specifying the input dimensions, we must hard code the number of samples in a batch, number of time steps in a sample and number of features in a time step by setting thebatch_input_shape parameter. For example:

![](img/3c3202485acc70f38fd4e3e6ee6b119d.png)

This same batch size must then be used later when evaluating the model and making predictions. For example:

![](img/703163b5e6174e378a835ea689c3493f.png)

![](img/e994e326e8c1364b40ea1ae7df3f9ce8.png)

We can adapt the previous time step example to use a stateful LSTM. The full code listing is provided below.

![](img/4ae07af2e5db197e27abded457386651.png)

![](img/d82ce3634e3fef1ab573bc39ac1cd61a.png)

**Stacked LSTMs with Memory Between Batches**

Finally, we will take a look at one of the big benefits of LSTMs: the fact that they can be successfully trained when stacked into deep network architectures.

LSTM networks can be stacked in Keras in the same way that other layer types can be stacked. One addition to the configuration that is required is that an LSTM layer prior to each subsequent LSTM layer must return the sequence. This can be done by setting the return_sequencesparameter on the layer to True.

![](img/35dc0d97e28e9f5988885125fa8c5ea4.png)

![](img/dd5f37295eb4e01060c644a5bcbf8510.png)

![](img/9b94b8c0dd3af790f6537bab309dbeb8.png)

![](img/9fc99bb34d0909cade710ce3a96f034e.png)

**总结**

In this post, you discovered how to develop LSTM recurrent neural networks for time series prediction in Python with the Keras deep learning network.

Specifically, you learned:

*   About the international airline passenger time series prediction problem.

*   How to create an LSTM for a regression and a window formulation of the time series problem.

*   How to create an LSTM with a time step formulation of the time series problem.

*   How to create an LSTM with state and stacked LSTMs with state to learn long sequences.

**投稿、商业合作**

**请发邮件到：lhtzjqxx@163.com**

**关注者**

**从****1 到 10000+**

**我们每天都在进步**

听说，置顶关注我们的人都不一般

![](img/74c285b465d1c5684165b6d5f0ebcd06.png)

**![](img/40429cd849aaf6f87544f9c00f4f92ad.png)**