# è‚¡ç¥¨æˆäº¤é‡çš„é¢„æµ‹åŸºäº Keras&sklearn

> åŸæ–‡ï¼š[`mp.weixin.qq.com/s?__biz=MzAxNTc0Mjg0Mg==&mid=2653285152&idx=1&sn=dffda5618726811aa7ea22d10fe7dda8&chksm=802e2935b759a023469de332c9ea0c1b5a9da48082000544eb9ca49023ee57ccae5a685300e3&scene=27#wechat_redirect`](http://mp.weixin.qq.com/s?__biz=MzAxNTc0Mjg0Mg==&mid=2653285152&idx=1&sn=dffda5618726811aa7ea22d10fe7dda8&chksm=802e2935b759a023469de332c9ea0c1b5a9da48082000544eb9ca49023ee57ccae5a685300e3&scene=27#wechat_redirect)

![](img/cb3bd660442e6bc134fbecf2477c43d1.png)

**ç¼–è¾‘éƒ¨**

å¾®ä¿¡å…¬ä¼—å·

**å…³é”®å­—**å…¨ç½‘æœç´¢æœ€æ–°æ’å

**ã€é‡åŒ–æŠ•èµ„ã€ï¼šæ’åç¬¬ä¸€**

**ã€é‡ Â  Â  Â  åŒ–ã€ï¼šæ’åç¬¬ä¸€**

**ã€æœºå™¨å­¦ä¹ ã€ï¼šæ’åç¬¬å››**

æˆ‘ä»¬ä¼šå†æ¥å†å‰

æˆä¸ºå…¨ç½‘**ä¼˜è´¨çš„**é‡‘èã€æŠ€æœ¯ç±»å…¬ä¼—å·

ç¾å›½è‚¡å¸‚ä¸Šåˆ 9:30 å¼€ç›˜ï¼Œä¸‹åˆ 4:00 æ”¶ç›˜é—­ï¼Œæä¾›ä¸Šåˆ 9:30 å’Œä¸‹åˆ 2:00 ä¹‹é—´çš„äº¤æ˜“æ•°æ®ï¼Œä»ä¸‹åˆ 2:00 åˆ°ä¸‹åˆ 4:00 é¢„æµ‹æˆäº¤é‡çš„å˜åŠ¨ã€‚

```py
import pandas as pd
import time
import numpy as np
import matplotlib.pyplot as plt
pd.options.mode.chained_assignment = None Â 
plt.style.use('ggplot')
```

åº¦é‡é”™è¯¯ç‡ï¼šå¹³å‡ç»å¯¹ç™¾åˆ†æ¯”è¯¯å·®ï¼ˆMAPEï¼‰ã€‚

```py
def mean_absolute_percentage_error(y_true, y_pred): 
 Â  Â return np.mean(np.abs((y_true - y_pred) / y_true)) * 10
```

**æ•°æ®è¯´æ˜**

è®­ç»ƒæ•°æ®åŒ…å« 1882 ä¸ªä¸åŒæ—¥æœŸçš„ 613220 ä¸ªä¾‹å­å’Œ 352 ä¸ªä¾‹å­ã€‚

æµ‹è¯•æ•°æ®åŒ…å« 614098 ä¸ªä¾‹å­ï¼Œæˆ‘ä»¬çš„ç›®çš„æ˜¯é¢„æµ‹ç›®æ ‡å€¼ã€‚

```py
Training = pd.read_csv('training_input.csv', delimiter=',')
Output = pd.read_csv('training_output.csv', delimiter=';')
Testing = pd.read_csv('testing_input.csv', delimiter=',')
print Training.shape
Training.head()
```

```py
(613220, 57)
```

![](img/6416ef1ba92e26c86549c6ba816fb0f5.png)

```py
print Output.shape
Output.head()
```

```py
(613220, 2)
```

![](img/3c66c4e49a7605701d91465d34d19669.png)

```py
print 'Number of product_id :', len(Training['product_id'].unique())
print 'Number of date :', len(Training['date'].unique())
```

```py
Number of product_id : 352
Number of date : 1882
```

è¿™ä¸ªæ•°æ®é›†çš„ä¸»è¦å›°éš¾æ˜¯åœ¨è®­ç»ƒå’Œæµ‹è¯•é›†ä¸­éƒ½æœ‰å¤§é‡çš„ç¼ºå¤±å€¼ã€‚å› æ­¤ï¼Œæœ‰ä¸¤ä¸ªå…³é”®é—®é¢˜ï¼šé¢„æµ‹ç›®æ ‡å€¼ï¼Œå¹¶å¤„ç†ç¼ºå¤±å€¼ã€‚

```py
print 'Number of rows with missing values in the test set :',Traini
```

```py
Number of rows with missing values in the test set : 99273
```

æˆ‘ä»¬é€è¡Œç ”ç©¶ç¼ºå¤±å€¼çš„æ•°é‡ï¼ˆå¯¹äºå…·æœ‰ç¼ºå°‘æ¡ç›®çš„è¡Œï¼‰ï¼šæˆ‘ä»¬ç»˜åˆ¶å•ä¸ªè¡Œçš„ç¼ºå¤±å€¼çš„ç›´æ–¹å›¾ï¼›æ­¤æ•°æ®é›†ä¸­çš„å¤§å¤šæ•°ç¤ºä¾‹åªæœ‰ 1 æˆ– 2 ä¸ªç¼ºå¤±å€¼ã€‚

```py
print 'Distribution of the rows with missing values in the train set' (Training.shape[1] - Training.count(axis=1)).value_counts()[1:].plot(kind='bar',
figsize=(14, 6), 
```

![](img/6a366743a7d37651bfc8655a73cec6fd.png)

```py
print 'Distribution of the rows with missing values in the test set' (Testing.shape[1] - Testing.count(axis=1)).value_counts()[1:].plot(kind='bar',
figsize=(14, 6), 
```

![](img/bbbfec91d0c0e13522271fbe254a52b9.png)

```py
print 'Distribution of the missing values in the train set by features' (Training.shape[0] - Training.count(axis=0)).sort_values(ascending=False)[:-3].plot(kind='bar',
figsize=(14, 6), Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â 
```

![](img/1a13cf82fed48748e236b01014b411da.png)

![](img/3765f5d75b4b6d6bcc065a30564af91c.png)

æˆ‘ä»¬æ¸…æ¥šåœ°çœ‹åˆ°ï¼Œè¿™ä¸¤ä¸ªå›¾å½¢éå¸¸ç›¸ä¼¼ï¼Œç¬¬ä¸€ä¸ªæ—¶é—´å˜é‡'09ï¼š30ï¼š00'çš„ç¼ºå¤±å€¼è¡¨ç¤ºä¸º sur-representationã€‚

**å¼€å§‹**

è¦å¯¹æˆ‘ä»¬çš„é¢„æµ‹å˜é‡çš„è´¨é‡æœ‰ä¸€ä¸ªå¾ˆå¥½çš„åˆæ­¥äº†è§£ï¼Œåœ¨è¿™é‡Œåˆ é™¤æ‰€æœ‰ç¼ºå°‘æ¡ç›®çš„ç¤ºä¾‹ï¼Œå¹¶åœ¨å®Œå…¨å¡«å……çš„æ•°æ®ä¸Šæ„å»ºæˆ‘ä»¬çš„é¢„æµ‹æ¨¡å‹ã€‚

```py
train = pd.merge(Training, Output, on='ID', how='inner')
train_full = train.drop(pd.isnull(train).any(1).nonzero()[0]).reset
```

![](img/e0ee307fac931eda7caa206c02659e1c.png)

![](img/d9b9ad2ccd4b2d09036a263b6ba68e8c.png)

![](img/56caa2d9c4cc61d146e35650677851dd.png)

![](img/c103cf1492045307910504cab7bbc5c7.png)

```py
import pywtclass DictT(object):def __init__(self, name, level):
 Â  Â  Â  Â  Â  Â self.name = name
 Â  Â  Â  Â  Â  Â self.level = level
 Â  Â  Â  Â  Â  Â self.sizes = [] Â  Â 
def dot(self, mat):
 Â  Â  m = [] Â  Â  Â  Â 
 Â  Â if mat.shape[0] != mat.size: Â  Â  Â  Â  Â  Â 
 Â  Â  Â  Â for i in xrange(mat.shape[1]):
 Â  Â  Â  Â  Â  Â c = pywt.wavedec(mat[:, i], self.name, level=self.level)
 Â  Â  Â  Â  Â  Â self.sizes.append(map(len, c))
 Â  Â  Â  Â  Â  Â c = np.concatenate(c)
 Â  Â  Â  Â  Â  Â m.append(c) Â  Â  Â  Â  Â  
 Â  Â  Â  Â return np.asarray(m).T Â  Â  Â  Â 
 Â  Â  else:
 Â  Â  Â  Â  c = pywt.wavedec(mat, self.name, level=self.level)
 Â  Â  Â  Â  self.sizes.append(map(len, c)) Â  Â  Â  Â  Â  Â 
 Â  Â  Â  Â  return np.concatenate(c)
```

ç°åœ¨ï¼Œä¸ºäº†è®¡ç®—çŸ©é˜µ X çš„å˜æ¢ï¼Œæˆ‘ä»¬ç®€å•åœ°å°†å…¶æ›¿æ¢ä¸ºä»¥ä¸‹ï¼š

```py
wave_name = 'db20' wave_level = None wavelet_operator_t = DictT(level=wave_level, name=wave_name)

basis_t = wavelet_operator_t.dot(np.identity(X.shape[1]))
basis_t /= np.sqrt(np.sum(basis_t ** 2, axis=0))
basis = basis_t.T
```

ç°åœ¨ï¼ŒX å˜ä¸º X.dotï¼ˆåŸºå‡†ï¼‰ä½œä¸ºå›å½’æ¨¡å‹çš„è¾“å…¥ã€‚ ä¸å¹¸çš„æ˜¯ï¼Œç¦»æ•£å°æ³¢å˜æ¢ï¼ˆæˆ–å‚…é‡Œå¶å˜æ¢ï¼‰ç»™å‡ºäº†æ¯”æœ€åä¸€ä¸ªæ–¹æ³•æ›´ç®€å•çš„ç»“æœï¼Œå¦‚ä¸‹æ‰€è¿°ã€‚ æ­¤å¤–ï¼Œå½“å¤„ç†ç¼ºå¤±å€¼æ—¶ï¼Œå†…æ’æ•°æ®çš„ DWT / DFTï¼ˆå‚è§ä¸‹é¢çš„éƒ¨åˆ†ï¼‰ä»‹ç»äº†æ‰€æœ‰çº§åˆ«çš„å†…æ’å™ªå£°ï¼Œå¹¶ä½¿æ­¤è¡¨ç¤ºåœ¨æ­¤æ–¹æ³•ä¸­æˆä¸ºä¸æ–¹ä¾¿çš„é€‰æ‹©ã€‚

**CNN** 

æˆ‘ä»¬è¿˜è§‚å¯Ÿäº†æ·±åº¦å­¦ä¹ æ¨¡å‹çš„å›å½’é˜¶æ®µã€‚ å…¶ä¸­ï¼Œæˆ‘ä»¬åœ¨æ•°æ®é›†ä¸Šå°è¯•äº† LSTM å’Œå·ç§¯ç¥ç»ç½‘ç»œã€‚æˆ‘ä»¬ä½¿ç”¨ Keras è½»æ¾ç»„å»ºç½‘ç»œï¼Œå¹¶ä¸”å¤§é‡åœ°ä½¿ç”¨å‚æ•°ï¼ŒåŒ…æ‹¬ Adam Optimizer çš„å­¦ä¹ æ­¥éª¤ï¼Œæ¯ä¸ªå·ç§¯å±‚çš„è¿‡æ»¤å™¨æ•°é‡æˆ–è¿‡æ»¤å™¨é•¿åº¦ã€‚

ä¸å¹¸çš„æ˜¯ï¼Œæ²¡æœ‰å‚æ•°é…ç½®ç»™æˆ‘ä»¬ä¸€ä¸ªä½äº 32ï¼…çš„è®­ç»ƒåˆ†æ•°ã€‚ æˆ‘ä»¬ä»ç„¶ç›¸ä¿¡ï¼Œè¿™ç§æ–¹æ³•ä»ç„¶å…·æœ‰å¾ˆå¤§çš„æ½œåŠ›ï¼Œæˆ‘ä»¬å¯ä»¥é€šè¿‡æ›´å¤šçš„å®éªŒè·å¾—æ›´å¤šçš„ç»“æœã€‚

```py
from keras.models import Sequential
from keras.layers.core import Dense, Dropout, Activation
from keras.layers.convolutional 
import Convolution1D, MaxPooling1D
from keras.layers.core import Flatten
from keras.optimizers import Adam
# Reshape data. train_shape = (x_train.shape[0], x_train.shape[1], 1)
val_shape = (x_val.shape[0], x_val.shape[1], 1)

x_train_2 = np.reshape(x_train, train_shape).astype(theano.config.floatX)
x_val_2 = np.reshape(x_val, val_shape).astype(theano.config.floatX)
 # CNN Model. model = Sequential()
model.add(Convolution1D(nb_filter=16,
 Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â filter_length=2,
 Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â init='glorot_uniform',
 Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â input_shape=(x_train.shape[1], 1)))
model.add(Activation('relu'))
model.add(MaxPooling1D(3))
model.add(Dropout(0.5))
model.add(Convolution1D(nb_filter=32,
 Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â filter_length=4,
 Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â init='glorot_uniform'))
model.add(Activation('relu'))
model.add(MaxPooling1D(2))
model.add(Dropout(0.5))
model.add(Flatten())
model.add(Dense(128, init='glorot_uniform'))
model.add(Activation('relu'))
model.add(Dropout(0.5))
model.add(Dense(1))

learning_rate = 10 batch_size = 100 nb_epoch = 10 adam = Adam(lr=learning_rate)
model.compile(loss='mean_absolute_percentage_error', optimizer=ad

print  'Training | Batch size :', batch_size, ", Number of epochs :", nb_epoch
model.fit(x_train_2, y_train, batch_size=batch_size, nb_epoch=nb_epoch,
 Â  Â  Â  Â  Â validation_data=(x_val_2, y_val), show_accuracy=True)
score, acc = model.evaluate(x_val_2, y_val, batch_size=batch_size,
 Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â show_accuracy=True) print  'Test score :', score print  'Test accuracy:', acc
```

**æœ€ç»ˆå›å½’**

åœ¨æµ‹è¯•å„ç§é¢„æµ‹å˜é‡åï¼Œæˆ‘ä»¬çš„ä¸»è¦é€‰æ‹©æ˜¯ğ–±ğ–ºğ—‡ğ–½ğ—ˆğ—†ğ–¥ğ—ˆğ—‹ğ–¾ğ—Œğ—ğ–±ğ–¾ğ—€ğ—‹ğ–¾ğ—Œğ—Œğ—ˆğ—‹ï¼Œåœ¨ç¼©çŸ­çš„æ•°æ®é›†ä¸Šä½¿ç”¨ğ–¦ğ—‹ğ—‚ğ–½ğ–²ğ–¾ğ–ºğ—‹ğ–¼ğ—ğ–¢ğ–µè¿›è¡Œå‚æ•°é€‰æ‹©ã€‚ è€ƒè™‘åˆ°åœ¨è¿™æ ·å¤§çš„æ•°æ®ä¸Šå‘ç”Ÿçš„å¤§çš„è®­ç»ƒæ—¶é—´ï¼Œè¿™æ˜¯æ‰§è¡Œç½‘æ ¼æœç´¢çš„å”¯ä¸€åˆç†çš„æ–¹æ³•ã€‚

```py
features = train_full.drop(['ID','product_id','TARGET'], axis=1)
X_columns = train_full.columns.drop(['ID','product_id','TARGET'])
X = features.values
y = train_full['TARGET'].values

from sklearn.cross_validation import train_test_split
X_train, X_test, y_train, y_test = train_test_split(
 Â  Â X, y, test_size=0.2, random_state=0)
```

```py
%%time
from sklearn.ensemble import RandomForestRegressor
from sklearn.cross_validation import cross_val_score

n_estimators = 50 max_depth = 15 max_features = 40 reg = RandomForestRegressor(n_estimators=n_estimators, max_depth=max_depth, max_features=max_features)

reg.fit(X_train, y_train)

y_pred = reg.predict(X_test)print "MAPE :", mean_absolute_percentage_error(y_test, y_pred)
```

```py
CPU times: user 13 Âµs, sys: 0 ns, total: 13 Âµs
Wall time: 16 Âµs
```

ä½¿ç”¨æˆ‘ä»¬è®­ç»ƒå¥½çš„é¢„æµ‹å˜é‡ï¼Œæˆ‘ä»¬ç°åœ¨å¯ä»¥è§‚æµ‹æ¯ä¸ªå˜é‡çš„é‡è¦æ€§ã€‚

```py
ordering = np.argsort(reg.feature_importances_)[::-1]
importances = reg.feature_importances_[ordering]
feature_names = X_columns[ordering]

pd.DataFrame(importances, index=feature_names).plot(kind='bar', figsize=(14, 6), colo
```

æˆ‘ä»¬è¯•å›¾é‡æ–°å¯åŠ¨æ¨¡å‹ï¼Œæ²¡æœ‰æœ€ä½é‡è¦æ€§çš„ç‰¹å¾ï¼Œè¿™å¯èƒ½åœ¨æˆ‘ä»¬çš„å›å½’ä¸­å¸¦æ¥å™ªéŸ³ã€‚ æˆ‘ä»¬å°è¯•å‡ ç§å¯èƒ½æ€§ï¼Œä½†æ˜¯ç»“æœå¹¶ä¸æ˜¯å¾ˆæœ‰è¯´æœåŠ›åœ°å®Œå…¨æ¶ˆé™¤å®ƒä»¬ã€‚

ç„¶è€Œï¼Œæˆ‘ä»¬ä»è¿›ä¸€æ­¥çš„æ¨¡å‹ä¸­åˆ é™¤â€œæ—¥æœŸâ€ï¼Œå› ä¸ºå®ƒåœ¨æ•´ä¸ªå˜é‡æ± ä¸­çš„é‡è¦æ€§ä¸å¤§ã€‚

å¦‚å‰æ‰€è¿°ï¼Œæˆ‘ä»¬å°è¯•äº†å…¶ä»–å‡ ä¸ªæ¨¡å‹ï¼Œæ²¡æœ‰å¾ˆå¤§çš„æˆåŠŸã€‚ ä»¥ä¸‹æ˜¯å¯¹ç›¸åŒæ•°æ®è¿›è¡Œè¯„ä¼°çš„ğ– ğ–½ğ–ºğ–¡ğ—ˆğ—ˆğ—Œğ—ğ–±ğ–¾ğ—€ğ—‹ğ–¾ğ—Œğ—Œğ—ˆğ—‹çš„ç¤ºä¾‹ï¼š

```py
%%time
from sklearn.ensemble import ExtraTreesRegressor
from sklearn.cross_validation import cross_val_scorefrom sklearn.ensemble import AdaBoostRegressor

regto = AdaBoostRegressor(base_estimator=reg, n_estimators=20, learning_rate=0.8)

regto.fit(X_train, y_train)

y_pred = regto.predict(X_test)print "MAPE :", mean_absolute_percentage_error(y_test, y_pred)
```

```py
MAPE : 36.379279734
CPU times: user 6min 59s, sys: 2.79 s, total: 7min 2s
Wall time: 7min 7s
```

**å¤„ç†ç¼ºå¤±å€¼**

**Â A.åˆ é™¤ç¬¬ä¸€ä¸ªæ—¶é—´å˜é‡**

å¦‚å‰æ‰€è¿°ï¼Œç¬¬ä¸€ä¸ªæ—¶é—´å˜é‡'09ï¼š30ï¼š00'å ç”¨äº†å¤§é‡çš„ç¼ºå¤±æ•°æ®ã€‚

æ­¤å¤–ï¼Œæˆ‘ä»¬çœ‹åˆ°è¿™ä¸ªå˜é‡åœ¨æˆ‘ä»¬çš„æ¨¡å‹ä¸­æ˜¯æœ€ä¸é‡è¦çš„ï¼›å› æ­¤ï¼Œæˆ‘ä»¬å†³å®šå°†å…¶ä»æœªæ¥çš„é¢„æµ‹ä¸­åˆ é™¤ã€‚

è¿™ç§åˆ é™¤çš„æ•ˆæœæ˜¯å¢åŠ äº†è®­ç»ƒçš„å¤§å°ï¼Œå› ä¸ºæˆ‘ä»¬åªæ˜¯åœ¨æ²¡æœ‰ä¸¢å¤±å€¼çš„æƒ…å†µä¸‹è¿›è¡Œè§‚å¯Ÿï¼Œè€Œä¸æ”¹å˜æˆ‘ä»¬çš„æ¨¡å‹çš„æ€§èƒ½ã€‚

```py
Training_new = train.copy()
Training_new.drop(["09:30:00"], axis=1, inplace=True)
print train.shapeprint Training_new.shape
```

```py
(613220, 58)
(613220, 57)
```

```py
print 'Number of rows without missing values with the feature 09:30:00 in train set :', train.\
 Â  Â drop(pd.isnull(train).any(1).nonzero()[0]).shape[0]
print 'Number of rows without missing values without the feature 09:30:00 in train set :', Training_new.\
 Â  Â drop(pd.isnull(Training_new).any(1).nonzero()[0]).shape[0]
```

```py
Number of rows without missing values with the feature 09:30:00 
in train set : 513947
Number of rows without missing values without the feature 09:30:00
 in train set : 534215
```

```py
Testing_new = Testing.copy()
Testing_new.drop(["09:30:00"], axis=1, inplace=True)
print Testing.shape
print Testing_new.shape
```

```py
(614098, 57)
(614098, 56)
```

åœ¨æµ‹è¯•é›†å’Œè®­ç»ƒé›†ä¸­ï¼Œè¿™ç§æ–¹æ³•å…è®¸æˆ‘ä»¬è·å¾—å¤§çº¦ 20000 æ¬¡è§‚å¯Ÿè€Œä¸ä¸¢å¤±å€¼ã€‚

æˆ‘ä»¬å› æ­¤è·å¾—äº†è®­ç»ƒé›†çš„å°ºå¯¸å’Œæˆ‘ä»¬å¯ä»¥ç”¨æˆ‘ä»¬çš„æ¨¡å‹é¢„æµ‹çš„æµ‹è¯•é›†çš„è§‚å¯Ÿå€¼ã€‚

```py
train_full = Training_new.drop(pd.isnull(Training_new).any(1).nonzero()[0])
from sklearn.cross_validation import train_test_split

X_train, X_test, y_train, y_test = train_test_split(
 Â  Â X, y, test_size=0.2, random_state=0)
%%time
from sklearn.ensemble import RandomForestRegressor
from sklearn.cross_validation import cross_val_score

n_estimators = 50 max_depth = 15 max_features = 40 reg = RandomForestRegressor(n_estimators=n_estimators, max_depth=max_depth, max_features=max_features)

reg.fit(X_train, y_train)

y_pred = reg.predict(X_test)
print "MAPE :", mean_absolute_percentage_error(y_test, y_pred)
```

```py
MAPE : 29.6441237583
CPU times: user 17min 55s, sys: 12.1 s, total: 18min 7s
Wall time: 19min 7s
```

**è¡¥å……ç¼ºå¤±å€¼**

 æˆ‘ä»¬å°è¯•äº†å‡ ç§æŠ€æœ¯æ¥æ’å€¼ç¼ºå¤±å€¼ï¼šsklearn çš„ğ–¨ğ—†ğ—‰ğ—ğ—ğ–¾ğ—‹è®¡ç®—æœºï¼ˆæˆ‘ä»¬å°è¯•çš„æœ€å·®ï¼‰ï¼Œä»¥åŠä» scipy ç»§æ‰¿çš„ pandas æ’å€¼æŠ€æœ¯çš„æ•´ä¸ªèŒƒå›´ã€‚

æˆ‘ä»¬å°†åœ¨è¿™é‡Œåªæä¾›æˆ‘ä»¬æœ€å¥½çš„ç»“æœï¼šä» scipy çš„æ—¶é—´æ’å€¼æ–¹æ³•ã€‚

æˆ‘ä»¬æŠŠè®­ç»ƒç»„åˆ†æˆä¸¤ä¸ªå­é›†ï¼šæ²¡æœ‰ç¼ºå°‘å€¼çš„è§‚æµ‹å€¼ï¼ˆtrain_filledï¼‰å’Œç¼ºå¤±å€¼çš„è§‚æµ‹å€¼ï¼ˆtrain_missingï¼‰ã€‚

```py
train_filled = Training_new.drop(pd.isnull(Training_new).any(1).nonzero()[0]).reset_index(drop=True)
train_missing = Training_new[~Training_new["ID"].isin(train_filled["ID"].tolist())]
print train_filled.shape
```

```py
row = train_missing.drop(["ID", "date", "product_id", "TARGET"], axis=1).ix[613193] # [62691] row.index = pd.to_datetime(row.index)
print "Missing values :", np.isnan(row).sum()

d = {"time": row.interpolate(method="time"),"none": row}
pd.DataFrame(d).plot(colormap=plt.get_cmap("bwr"), figsize=(14, 6))
```

```py
Missing values : 28
```

![](img/deaff61464e604bd944953cad9514eff.png)

ç›´æ¥å¸–ç»“æœï¼š

```py
Dataset shape after filling :
(613199, 57)
```

```py
MAPE : 29.6429418639
CPU times: user 18min 14s, sys: 13.5 s, total: 18min 28s
Wall time: 19min 53s
```

åœ¨è¿™é‡Œåªæµ‹è¯•äº†ç¼ºå°‘å€¼çš„è§‚å¯Ÿç»“æœï¼Œç»“æœç›¸å½“ç³Ÿç³•ã€‚ 

**æ— æ’å€¼çš„è§£å†³æ–¹æ¡ˆ**

 ä¸€ä¸ªé‡è¦çš„è¯´æ³•æ˜¯ï¼šæˆ‘ä»¬ä½¿ç”¨çš„æŒ‡æ ‡ä¸ä»…æƒ©ç½šäº†çœŸå®å€¼ä¸é¢„æµ‹å€¼ä¹‹é—´çš„è·ç¦»ï¼Œè€Œä¸”è¿˜æƒ©ç½šäº†è¿™ä¸¤è€…ä¹‹é—´çš„ç›¸å¯¹ä½ç½®ã€‚

ä¾‹å¦‚ï¼Œæˆ‘ä»¬ç”¨ ypredypred è¡¨ç¤ºæˆ‘ä»¬çš„é¢„æµ‹å€¼ï¼Œytrueytrue ä¸ºçœŸå€¼ï¼šå¦‚æœ ypred = 10ï¼Œytrue = 100ï¼Œåˆ™ä¸¤è€…ä¹‹é—´çš„è·ç¦»ä¸º 90ï¼ŒMAPE ä¸º 90ï¼…ï¼›ä½†æ˜¯å¦‚æœ ypred = 100ï¼Œytrue = 10ï¼Œè·ç¦»ä»ç„¶ä¸º 90ï¼Œä½† MAPE ç°åœ¨ä¸º 900ï¼…ã€‚ è¿™å°±æ˜¯ä¸ºä»€ä¹ˆæˆ‘ä»¬å†³å®šï¼Œå¦‚æœå…·æœ‰å¤ªå¤šç¼ºå¤±å€¼çš„ç¤ºä¾‹ï¼Œå¦‚æœåœ¨è¯¥äº§å“è®­ç»ƒé›†ä¸­è®¾ç½®äº† TARGETï¼Œåˆ™åˆ†é…æœ€å°å€¼ã€‚ è¿™æ ·ï¼Œæˆ‘ä»¬å¯¹è¿™äº›è§‚å¯Ÿç»“æœæœ€å¤šæœ‰ 100ï¼…çš„è¯¯å·®ã€‚ è¿™æ˜¯ä¸€ç§é™åˆ¶è§‚å¯Ÿå€¼ä¸ç¼ºå¤±å€¼çš„é”™è¯¯çš„æ–¹æ³•ã€‚

æ‰€ä»¥æˆ‘ä»¬ç”¨è®­ç»ƒé›†ç¼ºå¤±å€¼çš„è§‚å¯Ÿç»“æœæ¥æµ‹è¯•ã€‚

```py
train_filled = Training_new.drop(pd.isnull(Training_new).any(1).nonzero()[0]).reset_index(drop=True)
train_missing = Training_new[~Training_new["ID"].isin(train_filled["ID"].tolist())]
print 'train_filled',train_filled.shape
print 'train_missing',train_missing.shape
```

```py
train_filled (534215, 57)
train_missing (79005, 57)
```

```py
MAPE : 83.5862773074
```

æ‰€ä»¥å¯¹äºè¿™äº›è§‚å¯Ÿå€¼æˆ‘ä»¬æœ‰ 84ï¼…çš„ MAPã€‚ ä½†æ˜¯æˆ‘ä»¬ä¹Ÿè§‚å¯Ÿåˆ°ï¼ŒæŒ‰äº§å“ï¼Œç‰¹å¾â€œç›®æ ‡â€å…·æœ‰å¾ˆå¤§çš„æ ‡å‡†åå·®ï¼ˆå‡ ä¹ç­‰äºå¹³å‡å€¼ï¼‰ã€‚

```py
mean_per_product_id = train.groupby("product_id").agg({"TARGET": np.mean})
mean_per_product_id.columns = ['TARGET_mean']

std_per_product_id = train.groupby("product_id").agg({"TARGET": np.std})
std_per_product_id.columns = ['TARGET_std']

pd.concat([mean_per_product_id, std_per_product_id], axis=1).head()
```

![](img/778b35cba5005c281f54aac1a6bb6ae1.png)

è¿™æ„å‘³ç€æœ€ä½é™åº¦ä¸å…¶ä»–ä»·å€¼è§‚ç›¸å·®ç”šè¿œã€‚ æ‰€ä»¥æˆ‘ä»¬å†³å®šä½¿ç”¨ä¸æ˜¯æœ€å°å€¼ï¼Œè€Œæ˜¯ä½¿ç”¨ä¸Šé¢çš„å€¼ï¼Œè€Œä¸æ˜¯é‡æ–°è°ƒæ•´å®ƒã€‚MAPE : 83.5862773074

```py
MAPE : 69.9682064614
MAPE : 62.706405248
MAPE : 61.7711984429
MAPE : 65.5130846126
```

æˆ‘ä»¬ç¡®å®šäº†ä»¥ä¸‹æœ€å°è°ƒæ•´ä»·å€¼ï¼š

```py
MAPE : 61.480089348
``` 

**é¢„æµ‹æµ‹è¯•é›†**

 å¦‚å‰æ‰€è¿°ï¼Œæˆ‘ä»¬å°†æµ‹è¯•é›†åˆ†ä¸ºä¸¤ä¸ªå­é›†ï¼šä¸€ä¸ªæ²¡æœ‰ä»»ä½•ç¼ºå°‘çš„æ¡ç›®ï¼Œå…¶ä½™çš„ã€‚

åœ¨ç¬¬ä¸€ç§æƒ…å†µä¸‹ï¼Œæˆ‘ä»¬å°†ä½¿ç”¨æˆ‘ä»¬è®­ç»ƒå¥½çš„å›å½’å™¨è¿›è¡Œé¢„æµ‹ï¼›åœ¨ç¬¬äºŒç§æƒ…å†µä¸‹ï¼Œæˆ‘ä»¬å°†é€šè¿‡è¾“å‡ºæ¯ä¸ªäº§å“çš„é‡æ–°è°ƒæ•´çš„æœ€å°å€¼æ¥æ§åˆ¶é”™è¯¯ã€‚

```py
test_filled = Testing_new.drop(pd.isnull(Testing_new).any(1).nonzero()[0]).reset_index(drop=True)
test_missing = Testing_new[~Testing_new["ID"].isin(test_filled["ID"].tolist())]

train_full = Training_new.drop(pd.isnull(Training_new).any(1).nonzero()[0])
features = train_full.drop(['ID','product_id','date','TARGET'], axis=1)
X_columns = train_full.columns.drop(['ID','product_id','date','TARGET'])
X = features.values
y = train_full['TARGET'].values

%%time
from sklearn.ensemble import RandomForestRegressor
from sklearn.cross_validation import cross_val_score

n_estimators = 50 max_depth = 15 max_features = 40 reg = RandomForestRegressor(n_estimators=n_estimators, max_depth=max_depth, max_features=max_features)

reg.fit(X, y)

y_pred_filled = reg.predict(test_filled[numeric_cols].values)

submission_filled = test_filled[["ID", "product_id"]].copy()
submission_filled["TARGET"] = y_pred_filled
submission_filled.head()

min_per_product_id = train.groupby("product_id").agg({"TARGET": np.min})
min_per_product_id.head()

submission_missing = test_missing.copy().join(min_per_product_id, on="product_id")
submission_missing.head()

submission_missing["TARGET"] = 3.65 * submission_missing["TARGET"]
```

æˆ‘ä»¬æœ€ç»ˆè¿æ¥ç»“æœï¼ŒæŒ‰ ID æ’åºï¼ŒæŒ‰ç…§æ‰€éœ€çš„æ ¼å¼è¾“å‡ºã€‚

```py
submission = pd.concat([submission_filled, submission_missing], axis=0).sort_values(by="ID")
submission = submission[['ID','TARGET']]
print submission.shape
submission.head()
```

æˆ‘ä»¬ç»ˆäºè·å¾—äº† 37,25ï¼…çš„ MAPã€‚ 

**æŠ•ç¨¿ã€å•†ä¸šåˆä½œ**

**è¯·å‘é‚®ä»¶åˆ°ï¼šlhtzjqxx@163.com**

**å…³æ³¨è€…**

**ä»****1 åˆ° 10000+**

**æˆ‘ä»¬æ¯å¤©éƒ½åœ¨è¿›æ­¥**

**![](img/7a5c3f7044eeba1663dc1243a8dca17a.png)**