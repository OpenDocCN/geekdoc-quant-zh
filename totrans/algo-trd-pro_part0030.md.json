["```pypython\n\nfrom sklearn.linear_model import LogisticRegression\n\n# Assuming `X_train`, `y_train` are our features and binary outcomes\n\nlog_model = LogisticRegression()\n\nlog_model.fit(X_train, y_train)\n\n# The model can now predict probabilities of positive class (e.g., profitable trade)\n\npredicted_probs = log_model.predict_proba(X_test)\n\n```", "```pypython\n\nfrom sklearn.svm import SVC\n\n# We can specify the kernel based on our data's distribution\n\nsvm_model = SVC(kernel='linear', probability=True)\n\nsvm_model.fit(X_train, y_train)\n\n# Predict class probabilities for high-stake trades\n\ntrade_signals = svm_model.predict_proba(X_test)\n\n```", "```pypython\n\nfrom sklearn.tree import DecisionTreeClassifier\n\ntree_model = DecisionTreeClassifier(max_depth=3)\n\ntree_model.fit(X_train, y_train)\n\n# Visualizing the decision tree can provide insights into trade signal formation\n\nfrom sklearn.tree import export_graphviz\n\nexport_graphviz(tree_model, out_file='tree.dot', feature_names=X.columns)\n\n```", "```pypython\n\nfrom sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n\n# Random Forest for capturing a multitude of decision paths\n\nrf_model = RandomForestClassifier(n_estimators=100)\n\nrf_model.fit(X_train, y_train)\n\n# GBM for sequential improvement of prediction errors\n\ngbm_model = GradientBoostingClassifier(n_estimators=100)\n\ngbm_model.fit(X_train, y_train)\n\n```", "```pypython\n\nfrom sklearn.metrics import confusion_matrix, roc_auc_score\n\n# Evaluating the performance of a chosen model, say, the random forest\n\npredictions = rf_model.predict(X_test)\n\nconf_matrix = confusion_matrix(y_test, predictions)\n\nroc_score = roc_auc_score(y_test, rf_model.predict_proba(X_test)[:, 1])\n\nprint(f'Confusion Matrix:\\n{conf_matrix}')\n\nprint(f'ROC AUC Score: {roc_score}')\n\n```", "```pypython\n\nimport numpy as np\n\nimport pandas as pd\n\nfrom sklearn.model_selection import train_test_split\n\nfrom sklearn.linear_model import LogisticRegression\n\n# Assume we have a DataFrame `options_data` with our features and target\n\n# Target column 'ITM' is binary: 1 if option is in the money, 0 otherwise\n\n# Separating features and target variable\n\nX = options_data.drop('ITM', axis=1)\n\ny = options_data['ITM']\n\n# Splitting the dataset for training and testing\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n\n# Creating the logistic regression model\n\nlogistic_model = LogisticRegression()\n\nlogistic_model.fit(X_train, y_train)\n\n# Predicting the probability of options being ITM\n\nprobabilities = logistic_model.predict_proba(X_test)[:, 1]\n\n# Converting probabilities to a binary outcome based on a threshold (e.g., 0.5)\n\npredictions = np.where(probabilities > 0.5, 1, 0)\n\n```", "```pypython\n\nfrom sklearn.metrics import accuracy_score, roc_curve, auc\n\n# Evaluating model accuracy\n\naccuracy = accuracy_score(y_test, predictions)\n\nprint(f'Model Accuracy: {accuracy}')\n\n# Generating ROC curve values: false positives, true positives\n\nfpr, tpr, thresholds = roc_curve(y_test, probabilities)\n\nroc_auc = auc(fpr, tpr)\n\nimport matplotlib.pyplot as plt\n\nplt.figure()\n\nplt.plot(fpr, tpr, color='darkorange', lw=2, label=f'ROC curve (area = {roc_auc})')\n\nplt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n\nplt.xlabel('False Positive Rate')\n\nplt.ylabel('True Positive Rate')\n\nplt.title('Receiver Operating Characteristic')\n\nplt.legend(loc='lower right')\n\nplt.show()\n\n```", "```pypython\n\nfrom sklearn.svm import SVC\n\nfrom sklearn.preprocessing import StandardScaler\n\nfrom sklearn.pipeline import make_pipeline\n\n# Assuming `options_data` contains our features and 'Market_Condition' is our target\n\nX = options_data.drop('Market_Condition', axis=1)\n\ny = options_data['Market_Condition']\n\n# Data normalization is crucial for SVM performance\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n\n# Creating an SVM model with a radial basis function (RBF) kernel\n\nsvm_model = make_pipeline(StandardScaler(), SVC(kernel='rbf'))\n\nsvm_model.fit(X_train, y_train)\n\n# Making predictions on the test set\n\npredictions = svm_model.predict(X_test)\n\n```", "```pypython\n\nfrom sklearn.metrics import classification_report\n\n# Evaluating the model's performance\n\nclass_report = classification_report(y_test, predictions)\n\nprint(class_report)\n\n```", "```pypython\n\nfrom sklearn.model_selection import GridSearchCV\n\n# Parameters grid\n\nparam_grid = {\n\n'svc__C': [0.1, 1, 10],\n\n'svc__gamma': [0.001, 0.01, 0.1, 1]\n\n}\n\n# Grid search with cross-validation\n\ngrid_search = GridSearchCV(svm_model, param_grid, cv=5)\n\ngrid_search.fit(X_train, y_train)\n\n# Best parameters\n\nbest_params = grid_search.best_params_\n\nprint(f'Best parameters: {best_params}')\n\n```", "```pypython\n\nfrom sklearn.tree import DecisionTreeClassifier\n\nfrom sklearn.model_selection import train_test_split\n\n# Let's say `options_features` are our input features and `trade_action` is the target\n\nX = options_features\n\ny = trade_action\n\n# Splitting the dataset into training and testing sets\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=11)\n\n# Initializing the decision tree classifier\n\ntree_classifier = DecisionTreeClassifier(max_depth=3, random_state=11)\n\ntree_classifier.fit(X_train, y_train)\n\n# Predicting trade actions on the test set\n\ntree_predictions = tree_classifier.predict(X_test)\n\n```", "```pypython\n\nfrom sklearn.ensemble import RandomForestClassifier\n\n# Initializing the random forest classifier with 100 trees\n\nforest_classifier = RandomForestClassifier(n_estimators=100, random_state=11)\n\nforest_classifier.fit(X_train, y_train)\n\n# Predicting trade actions using the random forest\n\nforest_predictions = forest_classifier.predict(X_test)\n\n```", "```pypython\n\nfrom sklearn.ensemble import GradientBoostingClassifier\n\nfrom sklearn.metrics import accuracy_score\n\n# Assuming `X_train`, `X_test`, `y_train`, `y_test` are derived as before\n\n# Initialize the Gradient Boosting Classifier\n\ngbm_classifier = GradientBoostingClassifier(n_estimators=100, learning_rate=0.1, max_depth=3, random_state=11)\n\ngbm_classifier.fit(X_train, y_train)\n\n# Predicting the likelihood of option exercise\n\ngbm_predictions = gbm_classifier.predict(X_test)\n\n# Evaluate the model\n\naccuracy = accuracy_score(y_test, gbm_predictions)\n\n```", "```pypython\n\nfrom sklearn.metrics import confusion_matrix, classification_report\n\n# Assuming binary classification predictions\n\n# `y_true` holds true class labels, `y_pred` holds predicted class labels\n\ncm = confusion_matrix(y_true, y_pred)\n\n# Output the confusion matrix\n\nprint(cm)\n\n```", "```pypython\n\nfrom sklearn.metrics import roc_curve, auc\n\nimport matplotlib.pyplot as plt\n\n# Compute ROC curve\n\nfpr, tpr, thresholds = roc_curve(y_true, y_scores)\n\n# Calculate the area under the ROC curve (AUC)\n\nroc_auc = auc(fpr, tpr)\n\n# Plotting the ROC curve\n\nplt.figure()\n\nplt.plot(fpr, tpr, color='darkorange', lw=2, label='ROC curve (area = %0.2f)' % roc_auc)\n\nplt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n\nplt.xlabel('False Positive Rate')\n\nplt.ylabel('True Positive Rate')\n\nplt.title('Receiver Operating Characteristic')\n\nplt.legend(loc=\"lower right\")\n\nplt.show()\n\n```"]