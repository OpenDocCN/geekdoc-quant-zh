- en: '**Bibliography**'
  prefs: []
  type: TYPE_NORMAL
- en: 'American Statistical Society (1999): “Ethical guidelines for statistical practice.”
    Available at [http://www.amstat.org/committees/ethics/index.html](http://www.amstat.org/committees/ethics/index.html)
    .'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Bailey, D., J. Borwein, M. López de Prado, and J. Zhu (2014): “Pseudo-mathematics
    and financial charlatanism: The effects of backtest overfitting on out-of-sample
    performance.” *Notices of the American Mathematical Society* , Vol. 61, No. 5\.
    Available at [http://ssrn.com/abstract= 2308659](http://ssrn.com/abstract=2308659)
    .'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Bailey, D., J. Borwein, M. López de Prado, and J. Zhu (2017): “The probability
    of backtest overfitting.” *Journal of Computational Finance* , Vol. 20, No. 4,
    pp. 39–70\. Available at [http://ssrn. com/abstract=2326253](http://ssrn.com/abstract=2326253)
    .'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Bailey, D. and M. López de Prado (2012): “Balanced baskets: A new approach
    to trading and hedging risks.” *Journal of Investment Strategies (Risk Journals)*
    , Vol. 1, No. 4, pp. 21–62.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Beddall, M. and K. Land (2013): “The hypothetical performance of CTAs.” Working
    paper, Winton Capital Management.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Benjamini, Y. and Y. Hochberg (1995): “Controlling the false discovery rate:
    A practical and powerful approach to multiple testing.” *Journal of the Royal
    Statistical Society, Series B (Methodological)* , Vol. 57, No. 1, pp. 289–300.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Bennet, C., A. Baird, M. Miller, and G. Wolford (2010): “Neural correlates
    of interspecies perspective taking in the post-mortem Atlantic salmon: An argument
    for proper multiple comparisons correction.” *Journal of Serendipitous and Unexpected
    Results* , Vol. 1, No. 1, pp. 1–5.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Bruss, F. (1984): “A unified approach to a class of best choice problems with
    an unknown number of options.” *Annals of Probability* , Vol. 12, No. 3, pp. 882–891.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Dmitrienko, A., A.C. Tamhane, and F. Bretz (2010): *Multiple Testing Problems
    in Pharmaceutical Statistics* , 1st ed. CRC Press.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Dudoit, S. and M.J. van der Laan (2008): *Multiple Testing Procedures with
    Applications to Genomics* , 1st ed. Springer.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Fisher, R.A. (1915): “Frequency distribution of the values of the correlation
    coefficient in samples of an indefinitely large population.” *Biometrika (Biometrika
    Trust)* , Vol. 10, No. 4, pp. 507–521.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Hand, D. J. (2014): *The Improbability Principle* , 1st ed. Scientific American/Farrar,
    Straus and Giroux.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Harvey, C., Y. Liu, and H. Zhu (2013): “. . . And the cross-section of expected
    returns.” Working paper, Duke University. Available at [http://ssrn.com/abstract=2249314](http://ssrn.com/abstract=2249314)
    .'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Harvey, C. and Y. Liu (2014): “Backtesting.” Working paper, Duke University.
    Available at [http://ssrn.com/abstract=2345489](http://ssrn.com/abstract=2345489)
    .'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Hochberg Y. and A. Tamhane (1987): *Multiple Comparison Procedures* , 1st ed.
    John Wiley and Sons.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Holm, S. (1979): “A simple sequentially rejective multiple test procedure.”
    *Scandinavian Journal of Statistics* , Vol. 6, pp. 65–70.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Ioannidis, J.P.A. (2005): “Why most published research findings are false.”
    *PloS Medicine* , Vol. 2, No. 8, pp. 696–701.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Ingersoll, J., M. Spiegel, W. Goetzmann, and I. Welch (2007): “Portfolio performance
    manipulation and manipulation-proof performance measures.” *Review of Financial
    Studies* , Vol. 20, No. 5, pp. 1504–1546.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Lo, A. (2002): “The statistics of Sharpe ratios.” *Financial Analysts Journal*
    , Vol. 58, No. 4 (July/August), pp. 36–52.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'López de Prado M., and A. Peijan (2004): “Measuring loss potential of hedge
    fund strategies.” *Journal of Alternative Investments* , Vol. 7, No. 1 (Summer),
    pp. 7–31\. Available at [http://ssrn.com/abstract=641702](http://ssrn.com/abstract=641702)
    .'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Mertens, E. (2002): “Variance of the IID estimator in Lo (2002).” Working paper,
    University of Basel.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Roulston, M. and D. Hand (2013): “Blinded by optimism.” Working paper, Winton
    Capital Management.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Schorfheide, F. and K. Wolpin (2012): “On the use of holdout samples for model
    selection.” *American Economic Review* , Vol. 102, No. 3, pp. 477–481.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Sharpe, W. (1966): “Mutual fund performance.” *Journal of Business* , Vol.
    39, No. 1, pp. 119–138.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Sharpe, W. (1975): “Adjusting for risk in portfolio performance measurement.”
    *Journal of Portfolio Management* , Vol. 1, No. 2 (Winter), pp. 29–34.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Sharpe, W. (1994): “The Sharpe ratio.” *Journal of Portfolio Management* ,
    Vol. 21, No. 1 (Fall), pp. 49–58.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Studený M. and Vejnarová J. (1999): “The multiinformation function as a tool
    for measuring stochastic dependence,” in M. I. Jordan, ed., *Learning in Graphical
    Models* . MIT Press, pp. 261–296.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Wasserstein R., and Lazar N. (2016) “The ASA''s statement on p-values: Context,
    process, and purpose.” *American Statistician* , Vol. 70, No. 2, pp. 129–133\.
    DOI: 10.1080/00031305.2016\. 1154108.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Watanabe S. (1960): “Information theoretical analysis of multivariate correlation.”
    *IBM Journal of Research and Development* , Vol. 4, pp. 66–82.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Notes**'
  prefs: []
  type: TYPE_NORMAL
- en: ^([1](text00002.html#filepos0000683161))    For further details, visit [https://www.gipsstandards.org.](https://www.gipsstandards.org.)
  prefs: []
  type: TYPE_NORMAL
- en: ^([2](text00002.html#filepos0000691684))    External cash flows are assets (cash
    or investments) that enter or exit a portfolio. Dividend and interest income payments,
    for example, are not considered external cash flows.
  prefs: []
  type: TYPE_NORMAL
- en: ^([3](text00002.html#filepos0000707452))    This could be set to a default value
    of zero (i.e., comparing against no investment skill).
  prefs: []
  type: TYPE_NORMAL
- en: '**CHAPTER 15**'
  prefs: []
  type: TYPE_NORMAL
- en: '**Understanding Strategy Risk**'
  prefs: []
  type: TYPE_NORMAL
- en: '**15.1 Motivation**'
  prefs: []
  type: TYPE_NORMAL
- en: 'As we saw in Chapters 3 and 13, investment strategies are often implemented
    in terms of positions held until one of two conditions are met: (1) a condition
    to exit the position with profits (profit-taking), or (2) a condition to exit
    the position with losses (stop-loss). Even when a strategy does not explicitly
    declare a stop-loss, there is always an implicit stop-loss limit, at which the
    investor can no longer finance her position (margin call) or bear the pain caused
    by an increasing unrealized loss. Because most strategies have (implicitly or
    explicitly) these two exit conditions, it makes sense to model the distribution
    of outcomes through a binomial process. This in turn will help us understand what
    combinations of betting frequency, odds, and payouts are uneconomic. The goal
    of this chapter is to help you evaluate when a strategy is vulnerable to small
    changes in any of these variables.'
  prefs: []
  type: TYPE_NORMAL
- en: '**15.2 Symmetric Payouts**'
  prefs: []
  type: TYPE_NORMAL
- en: 'Consider a strategy that produces *n* IID bets per year, where the outcome
    *X [*i*]* of a bet *i* ∈ [1, *n* ] is a profit π > 0 with probability P[ *X [*i*]*
    = π] = *p* , and a loss − π with probability P[ *X [*i*]* = −π] = 1 − *p* . You
    can think of *p* as the precision of a binary classifier where a positive means
    betting on an opportunity, and a negative means passing on an opportunity: True
    positives are rewarded, false positives are punished, and negatives (whether true
    or false) have no payout. Since the betting outcomes { *X [*i*]* } [*i* = 1, …,
    *n*] are independent, we will compute the expected moments per bet. The expected
    profit from one bet is E[ *X [*i*]* ] = π *p* + ( − π)(1 − *p* ) = π(2 *p* − 1).
    The variance is V[ *X [*i*]* ] = E[ *X ² [*i*]* ] − E[ *X [*i*]* ] ² , where E[
    *X ² [*i*]* ] = π ² *p* + ( − π) ² (1 − *p* ) = π ² , thus V[ *X [*i*]* ] = π
    ² − π ² (2 *p* − 1) ² = π ² [1 − (2 *p* − 1) ² ] = 4π ² *p* (1 − *p* ). For *n*
    IID bets per year, the annualized Sharpe ratio (θ) is'
  prefs: []
  type: TYPE_NORMAL
- en: '![](Image00810.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Note how π cancels out of the above equation, because the payouts are symmetric.
    Just as in the Gaussian case, θ[ *p* , *n* ] can be understood as a re-scaled
    t-value. This illustrates the point that, even for a small ![](Image00048.jpg)
    , the Sharpe ratio can be made high for a sufficiently large *n.* This is the
    economic basis for high-frequency trading, where *p* can be barely above .5, and
    the key to a successful business is to increase *n* . The Sharpe ratio is a function
    of precision rather than accuracy, because passing on an opportunity (a negative)
    is not rewarded or punished directly (although too many negatives may lead to
    a small *n* , which will depress the Sharpe ratio toward zero).
  prefs: []
  type: TYPE_NORMAL
- en: For example, for *p* = .55, ![](Image00141.jpg) , and achieving an annualized
    Sharpe ratio of 2 requires 396 bets per year. Snippet 15.1 verifies this result
    experimentally.  [ Figure 15.1 ](text00003.html#filepos0000747868) plots the Sharpe
    ratio as a function of precision, for various betting frequencies.
  prefs: []
  type: TYPE_NORMAL
- en: '![](Image00235.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '[**Figure 15.1**](text00003.html#filepos0000747559) The relation between precision
    (x-axis) and sharpe ratio (y-axis) for various bet frequencies (n)'
  prefs: []
  type: TYPE_NORMAL
- en: '**SNIPPET 15.1 TARGETING A SHARPE RATIO AS A FUNCTION OF THE NUMBER OF BETS**'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '![](Image00325.jpg)'
  prefs:
  - PREF_BQ
  type: TYPE_IMG
- en: Solving for 0 ≤ *p* ≤ 1, we obtain ![](Image00414.jpg) , with solution
  prefs: []
  type: TYPE_NORMAL
- en: '![](Image00768.jpg)'
  prefs: []
  type: TYPE_IMG
- en: This equation makes explicit the trade-off between precision ( *p* ) and frequency
    ( *n* ) for a given Sharpe ratio (θ). For example, a strategy that only produces
    weekly bets ( *n* = 52) will need a fairly high precision of *p* = 0.6336 to deliver
    an annualized Sharpe of 2.
  prefs: []
  type: TYPE_NORMAL
- en: '**15.3 Asymmetric Payouts**'
  prefs: []
  type: TYPE_NORMAL
- en: Consider a strategy that produces *n* IID bets per year, where the outcome *X
    [*i*]* of a bet *i* ∈ [1, *n* ] is π [+] with probability P[ *X [*i*]* = π [+]
    ] = *p* , and an outcome π [−] , π [−] < π [+] occurs with probability P[ *X [*i*]*
    = π [−] ] = 1 − *p* . The expected profit from one bet is E[ *X [*i*]* ] = *p*
    π [+] + (1 − *p* )π [−] = (π [+] − π [−] ) *p* + π [−] . The variance is V[ *X
    [*i*]* ] = E[ *X ² [*i*]* ] − E[ *X [*i*]* ] ² , where E[ *X ² [*i*]* ] = *p*
    π [+] ² + (1 − *p* )π ² [−] = (π [+] ² − π ² [−] ) *p* + π [−] ² , thus V[ *X
    [*i*]* ] = (π [+] − π [−] ) ² *p* (1 − *p* ). For *n* IID bets per year, the annualized
    Sharpe ratio (θ) is
  prefs: []
  type: TYPE_NORMAL
- en: '![](Image00005.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'And for π [−] = −π [+] we can see that this equation reduces to the symmetric
    case: ![](Image00098.jpg) . For example, for *n* = 260, π [−] = −.01, π [+] =
    .005, *p* = .7, we get θ = 1.173.'
  prefs: []
  type: TYPE_NORMAL
- en: Finally, we can solve the previous equation for 0 ≤ *p* ≤ 1, to obtain
  prefs: []
  type: TYPE_NORMAL
- en: '![](Image00187.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'where:'
  prefs: []
  type: TYPE_NORMAL
- en: '*a* = (*n* + θ ² )(π [+] − π [−] ) ²'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*b* = [2*n* π [−] − θ ² (π [+] − π [−] )](π [+] − π [−] )'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*c* = *n* π ² [−]'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'As a side note, Snippet 15.2 verifies these symbolic operations using SymPy
    Live: [http://live.sympy.org/](http://live.sympy.org/) .'
  prefs: []
  type: TYPE_NORMAL
- en: '**SNIPPET 15.2 USING THE SymPy LIBRARY FOR SYMBOLIC OPERATIONS**'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '![](Image00038.jpg)'
  prefs:
  - PREF_BQ
  type: TYPE_IMG
- en: 'The above equation answers the following question: Given a trading rule characterized
    by parameters {π [−] , π [+] , *n* }, what is the precision rate *p* required
    to achieve a Sharpe ratio of θ*? For example, for *n* = 260, π [−] = −.01, π [+]
    = .005, in order to get θ = 2 we require a *p* = .72 *.* Thanks to the large number
    of bets, a very small change in *p* (from *p* = .7 to *p* = .72) has propelled
    the Sharpe ratio from θ = 1.173 to θ = 2 *.* On the other hand, this also tells
    us that the strategy is vulnerable to small changes in *p.* Snippet 15.3 implements
    the derivation of the implied precision. [Figure 15.2](text00003.html#filepos0000757396)
    displays the implied precision as a function of *n* and π [−] , where π [+] =
    0.1 and θ* = 1.5 *.* As π [−] becomes more negative for a given *n* , a higher
    *p* is required to achieve θ* for a given π [+] . As *n* becomes smaller for a
    given π [−] , a higher *p* is required to achieve θ* for a given π [+] .'
  prefs: []
  type: TYPE_NORMAL
- en: '![](Image00473.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '[**Figure 15.2**](text00003.html#filepos0000756367) Heat-map of the implied
    precision as a function of *n* and π [−] , with π [+] = 0.1 and θ* = 1.5'
  prefs: []
  type: TYPE_NORMAL
- en: '**SNIPPET 15.3 COMPUTING THE IMPLIED PRECISION**'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '![](Image00449.jpg)'
  prefs:
  - PREF_BQ
  type: TYPE_IMG
- en: Snippet 15.4 solves θ[ *p* , *n* , π [−] , π [+] ] for the implied betting frequency,
    *n.* [Figure 15.3](text00003.html#filepos0000759335) plots the implied frequency
    as a function of *p* and π [−] , where π [+] = 0.1 and θ* = 1.5 *.* As π [−] becomes
    more negative for a given *p* , a higher *n* is required to achieve θ* for a given
    π [+] . As *p* becomes smaller for a given π [−] , a higher *n* is required to
    achieve θ* for a given π [+] .
  prefs: []
  type: TYPE_NORMAL
- en: '![](Image00517.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '[**Figure 15.3**](text00003.html#filepos0000758309) Implied frequency as a
    function of *p* and, with = 0.1 and = 1.5'
  prefs: []
  type: TYPE_NORMAL
- en: '**SNIPPET 15.4 COMPUTING THE IMPLIED BETTING FREQUENCY**'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '![](Image00084.jpg)'
  prefs:
  - PREF_BQ
  type: TYPE_IMG
- en: '**15.4 The Probability of Strategy Failure**'
  prefs: []
  type: TYPE_NORMAL
- en: 'In the example above, parameters π [−] = −.01, π [+] = .005 are set by the
    portfolio manager, and passed to the traders with the execution orders. Parameter
    *n* = 260 is also set by the portfolio manager, as she decides what constitutes
    an opportunity worth betting on. The two parameters that are not under the control
    of the portfolio manager are *p* (determined by the market) and θ* (the objective
    set by the investor). Because *p* is unknown, we can model it as a random variable,
    with expected value E[ *p* ]. Let us define ![](Image00703.jpg) as the value of
    *p* below which the strategy will underperform a target Sharpe ratio θ*, that
    is, ![](Image00111.jpg) . We can use the equations above (or the `binHR` function)
    to conclude that for ![](Image00030.jpg) , ![](Image00122.jpg) *.* This highlights
    the risks involved in this strategy, because a relatively small drop in *p* (from
    *p* = .7 to *p* = .67) will wipe out all the profits. The strategy is intrinsically
    risky, even if the holdings are not. That is the critical difference we wish to
    establish with this chapter: *Strategy risk* should not be confused with *portfolio
    risk.*'
  prefs: []
  type: TYPE_NORMAL
- en: Most firms and investors compute, monitor, and report portfolio risk without
    realizing that this tells us nothing about the risk of the strategy itself. Strategy
    risk is not the risk of the underlying portfolio, as computed by the chief risk
    officer. Strategy risk is the risk that the investment strategy will fail to succeed
    over time, a question of far greater relevance to the chief investment officer.
    The answer to the question “What is the probability that this strategy will fail?”
    is equivalent to computing ![](Image00212.jpg) . The following algorithm will
    help us compute the strategy risk.
  prefs: []
  type: TYPE_NORMAL
- en: '**15.4.1 Algorithm**'
  prefs: []
  type: TYPE_NORMAL
- en: 'In this section we will describe a procedure to compute ![](Image00306.jpg)
    . Given a time series of bet outcomes {π [*t*] } [*t*  = 1, …,  *T*] , first we
    estimate π [−] = E[{π [*t*] |π [*t*] ≤ 0} [*t*  = 1, …,  *T*] ], and π [+] = E[{π
    [*t*] |π [*t*] > 0} [*t*  = 1, …,  *T*] ]. Alternatively, {π [−] , π [+] } could
    be derived from fitting a mixture of two Gaussians, using the EF3M algorithm (López
    de Prado and Foreman [2014]). Second, the annual frequency *n* is given by ![](Image00398.jpg)
    , where *y* is the number of years elapsed between *t* = 1 and *t* = *T* . Third,
    we bootstrap the distribution of *p* as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: 'For iterations *i* = 1, …, *I* :'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Draw ⌊*nk* ⌋ samples from {π [*t*] } [*t* = 1, …, *T*] with replacement, where
    *k* is the number of years used by investors to assess a strategy (e.g., 2 years).
    We denote the set of these drawn samples as {π ^(( *i* )) [*j*] } [*j* = 1, …,
    ⌊ *nk* ⌋] .
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Derive the observed precision from iteration *i* as ![](Image00464.jpg) .
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Fit the PDF of *p* , denoted *f* [*p* ], by applying a Kernel Density Estimator
    (KDE) on {*p [*i*]* } [*i* = 1, …, *I*] .
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: For a sufficiently large *k* , we can approximate this third step as ![](Image00543.jpg)
    , where ![](Image00650.jpg) . Fourth, given a threshold θ* (the Sharpe ratio that
    separates failure from success), derive ![](Image00723.jpg) (see Section 15.4).
    Fifth, the strategy risk is computed as ![](Image00814.jpg) .
  prefs: []
  type: TYPE_NORMAL
- en: '**15.4.2 Implementation**'
  prefs: []
  type: TYPE_NORMAL
- en: Snippet 15.5 lists one possible implementation of this algorithm. Typically
    we would disregard strategies where ![](Image00054.jpg) as too risky, even if
    they invest in low volatility instruments. The reason is that even if they do
    not lose much money, the probability that they will fail to achieve their target
    is too high. In order to be deployed, the strategy developer must find a way to
    reduce ![](Image00145.jpg) .
  prefs: []
  type: TYPE_NORMAL
- en: '**SNIPPET 15.5 CALCULATING THE STRATEGY RISK IN PRACTICE**'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '![](Image00240.jpg)'
  prefs:
  - PREF_BQ
  type: TYPE_IMG
- en: 'This approach shares some similarities with PSR (see Chapter 14, and Bailey
    and López de Prado [2012, 2014]). PSR derives the probability that the true Sharpe
    ratio exceeds a given threshold under non-Gaussian returns. Similarly, the method
    introduced in this chapter derives the strategy''s probability of failure based
    on asymmetric binary outcomes. The key difference is that, while PSR does not
    distinguish between parameters under or outside the portfolio manager''s control,
    the method discussed here allows the portfolio manager to study the viability
    of the strategy subject to the parameters under her control: {π [−] , π [+] ,
    *n* }. This is useful when designing or assessing the viability of a trading strategy.'
  prefs: []
  type: TYPE_NORMAL
- en: '**Exercises**'
  prefs: []
  type: TYPE_NORMAL
- en: A portfolio manager intends to launch a strategy that targets an annualized
    SR of 2\. Bets have a precision rate of 60%, with weekly frequency. The exit conditions
    are 2% for profit-taking, and –2% for stop-loss.
  prefs:
  - PREF_OL
  - PREF_BQ
  - PREF_BQ
  type: TYPE_NORMAL
- en: Is this strategy viable?
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '*Ceteris paribus* , what is the required precision rate that would make the
    strategy profitable?'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: For what betting frequency is the target achievable?
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: For what profit-taking threshold is the target achievable?
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: What would be an alternative stop-loss?
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Following up on the strategy from exercise 1.
  prefs:
  - PREF_OL
  - PREF_BQ
  - PREF_BQ
  type: TYPE_NORMAL
- en: What is the sensitivity of SR to a 1% change in each parameter?
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Given these sensitivities, and assuming that all parameters are equally hard
    to improve, which one offers the lowest hanging fruit?
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Does changing any of the parameters in exercise 1 impact the others? For example,
    does changing the betting frequency modify the precision rate, etc.?
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Suppose a strategy that generates monthly bets over two years, with returns
    following a mixture of two Gaussian distributions. The first distribution has
    a mean of –0.1 and a standard deviation of 0.12\. The second distribution has
    a mean of 0.06 and a standard deviation of 0.03\. The probability that a draw
    comes from the first distribution is 0.15.
  prefs:
  - PREF_OL
  - PREF_BQ
  - PREF_BQ
  type: TYPE_NORMAL
- en: Following López de Prado and Peijan [2004] and López de Prado and Foreman [2014],
    derive the first four moments for the mixture's returns.
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: What is the annualized SR?
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Using those moments, compute PSR[1] (see Chapter 14). At a 95% confidence level,
    would you discard this strategy?
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Using Snippet 15.5, compute ![](Image00331.jpg) for the strategy described in
    exercise 3\. At a significance level of 0.05, would you discard this strategy?
    Is this result consistent with *PSR* [θ*]?
  prefs:
  - PREF_OL
  - PREF_BQ
  - PREF_BQ
  type: TYPE_NORMAL
- en: In general, what result do you expect to be more accurate, *PSR* [θ*] or ![](Image00419.jpg)
    ? How are these two methods complementary?
  prefs:
  - PREF_OL
  - PREF_BQ
  - PREF_BQ
  type: TYPE_NORMAL
- en: Re-examine the results from Chapter 13, in light of what you have learned in
    this chapter.
  prefs:
  - PREF_OL
  - PREF_BQ
  - PREF_BQ
  type: TYPE_NORMAL
- en: Does the asymmetry between profit taking and stop-loss thresholds in OTRs make
    sense?
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: What is the range of *p* implied by Figure 13.1, for a daily betting frequency?
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: What is the range of *p* implied by Figure 13.5, for a weekly betting frequency?
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '**References**'
  prefs: []
  type: TYPE_NORMAL
- en: 'Bailey, D. and M. López de Prado (2014): “The deflated Sharpe ratio: Correcting
    for selection bias, backtest overfitting and non-normality.” *Journal of Portfolio
    Management* , Vol. 40, No. 5\. Available at [https://ssrn.com/abstract=2460551](https://ssrn.com/abstract=2460551)
    .'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Bailey, D. and M. López de Prado (2012): “The Sharpe ratio efficient frontier.”
    *Journal of Risk* , Vol. 15, No. 2, pp. 3–44\. Available at [https://ssrn.com/abstract=1821643.](https://ssrn.com/abstract=1821643.)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'López de Prado, M. and M. Foreman (2014): “A mixture of Gaussians approach
    to mathematical portfolio oversight: The EF3M algorithm.” *Quantitative Finance*
    , Vol. 14, No. 5, pp. 913–930\. Available at [https://ssrn.com/abstract=1931734](https://ssrn.com/abstract=1931734)
    .'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'López de Prado, M. and A. Peijan (2004): “Measuring loss potential of hedge
    fund strategies.” *Journal of Alternative Investments* , Vol. 7, No. 1 (Summer),
    pp. 7–31\. Available at [http://ssrn.com/abstract=641702](http://ssrn.com/abstract=641702)
    .'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**CHAPTER 16**'
  prefs: []
  type: TYPE_NORMAL
- en: '**Machine Learning Asset Allocation**'
  prefs: []
  type: TYPE_NORMAL
- en: '**16.1 Motivation**'
  prefs: []
  type: TYPE_NORMAL
- en: 'This chapter introduces the Hierarchical Risk Parity (HRP) approach. ^([1](text00003.html#filepos0000848817))
    HRP portfolios address three major concerns of quadratic optimizers in general
    and Markowitz''s Critical Line Algorithm (CLA) in particular: instability, concentration,
    and underperformance. HRP applies modern mathematics (graph theory and machine
    learning techniques) to build a diversified portfolio based on the information
    contained in the covariance matrix. However, unlike quadratic optimizers, HRP
    does not require the invertibility of the covariance matrix. In fact, HRP can
    compute a portfolio on an ill-degenerated or even a singular covariance matrix,
    an impossible feat for quadratic optimizers. Monte Carlo experiments show that
    HRP delivers lower out-of-sample variance than CLA, even though minimum-variance
    is CLA''s optimization objective. HRP produces less risky portfolios out-of-sample
    compared to traditional risk parity methods. Historical analyses have also shown
    that HRP would have performed better than standard approaches (Kolanovic et al.
    [2017], Raffinot [2017]). A practical application of HRP is to determine allocations
    across multiple machine learning (ML) strategies.'
  prefs: []
  type: TYPE_NORMAL
- en: '**16.2 The Problem with Convex Portfolio Optimization**'
  prefs: []
  type: TYPE_NORMAL
- en: Portfolio construction is perhaps the most recurrent financial problem. On a
    daily basis, investment managers must build portfolios that incorporate their
    views and forecasts on risks and returns. This is the primordial question that
    24-year-old Harry Markowitz attempted to answer more than six decades ago. His
    monumental insight was to recognize that various levels of risk are associated
    with different optimal portfolios in terms of risk-adjusted returns, hence the
    notion of “efficient frontier” (Markowitz [1952]). One implication is that it
    is rarely optimal to allocate all assets to the investments with highest expected
    returns. Instead, we should take into account the correlations across alternative
    investments in order to build a diversified portfolio.
  prefs: []
  type: TYPE_NORMAL
- en: Before earning his PhD in 1954, Markowitz left academia to work for the RAND
    Corporation, where he developed the Critical Line Algorithm. CLA is a quadratic
    optimization procedure specifically designed for inequality-constrained portfolio
    optimization problems. This algorithm is notable in that it guarantees that the
    exact solution is found after a known number of iterations, and that it ingeniously
    circumvents the Karush-Kuhn-Tucker conditions (Kuhn and Tucker [1951]). A description
    and open-source implementation of this algorithm can be found in Bailey and López
    de Prado [2013]. Surprisingly, most financial practitioners still seem unaware
    of CLA, as they often rely on generic-purpose quadratic programming methods that
    do not guarantee the correct solution or a stopping time.
  prefs: []
  type: TYPE_NORMAL
- en: Despite of the brilliance of Markowitz's theory, a number of practical problems
    make CLA solutions somewhat unreliable. A major caveat is that small deviations
    in the forecasted returns will cause CLA to produce very different portfolios
    (Michaud [1998]). Given that returns can rarely be forecasted with sufficient
    accuracy, many authors have opted for dropping them altogether and focusing on
    the covariance matrix. This has led to risk-based asset allocation approaches,
    of which “risk parity” is a prominent example (Jurczenko [2015]). Dropping the
    forecasts on returns improves but does not prevent the instability issues. The
    reason is that quadratic programming methods require the inversion of a positive-definite
    covariance matrix (all eigenvalues must be positive). This inversion is prone
    to large errors when the covariance matrix is numerically ill-conditioned, that
    is, when it has a high condition number (Bailey and López de Prado [2012]).
  prefs: []
  type: TYPE_NORMAL
- en: '**16.3 Markowitz''s Curse**'
  prefs: []
  type: TYPE_NORMAL
- en: 'The condition number of a covariance, correlation (or normal, thus diagonalizable)
    matrix is the absolute value of the ratio between its maximal and minimal (by
    moduli) eigenvalues. [Figure 16.1](text00003.html#filepos0000780859) plots the
    sorted eigenvalues of several correlation matrices, where the condition number
    is the ratio between the first and last values of each line. This number is lowest
    for a diagonal correlation matrix, which is its own inverse. As we add correlated
    (multicollinear) investments, the condition number grows. At some point, the condition
    number is so high that numerical errors make the inverse matrix too unstable:
    A small change on any entry will lead to a very different inverse. This is Markowitz''s
    curse: The more correlated the investments, the greater the need for diversification,
    and yet the more likely we will receive unstable solutions. The benefits of diversification
    often are more than offset by estimation errors.'
  prefs: []
  type: TYPE_NORMAL
- en: '![](Image00484.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '[**Figure 16.1**](text00003.html#filepos0000779901) Visualization of Markowitz''s
    curse'
  prefs: []
  type: TYPE_NORMAL
- en: A diagonal correlation matrix has the lowest condition number. As we add correlated
    investments, the maximum eigenvalue is greater and the minimum eigenvalue is lower.
    The condition number rises quickly, leading to unstable inverse correlation matrices.
    At some point, the benefits of diversification are more than offset by estimation
    errors.
  prefs: []
  type: TYPE_NORMAL
- en: Increasing the size of the covariance matrix will only make matters worse, as
    each covariance coefficient is estimated with fewer degrees of freedom. In general,
    we need at least ![](Image00717.jpg) independent and identically distributed (IID)
    observations in order to estimate a covariance matrix of size *N* that is not
    singular. For example, estimating an invertible covariance matrix of size 50 requires,
    at the very least, 5 years of daily IID data. As most investors know, correlation
    structures do not remain invariant over such long periods by any reasonable confidence
    level. The severity of these challenges is epitomized by the fact that even naïve
    (equally-weighted) portfolios have been shown to beat mean-variance and risk-based
    optimization out-of-sample (De Miguel et al. [2009]).
  prefs: []
  type: TYPE_NORMAL
- en: '**16.4 From Geometric to Hierarchical Relationships**'
  prefs: []
  type: TYPE_NORMAL
- en: These instability concerns have received substantial attention in recent years,
    as Kolm et al. [2014] have carefully documented. Most alternatives attempt to
    achieve robustness by incorporating additional constraints (Clarke et al. [2002]),
    introducing Bayesian priors (Black and Litterman [1992]), or improving the numerical
    stability of the covariance matrix's inverse (Ledoit and Wolf [2003]).
  prefs: []
  type: TYPE_NORMAL
- en: 'All the methods discussed so far, although published in recent years, are derived
    from (very) classical areas of mathematics: geometry, linear algebra, and calculus.
    A correlation matrix is a linear algebra object that measures the cosines of the
    angles between any two vectors in the vector space formed by the returns series
    (see Calkin and López de Prado [2014a, 2015b]). One reason for the instability
    of quadratic optimizers is that the vector space is modelled as a complete (fully
    connected) graph, where every node is a potential candidate to substitute another.
    In algorithmic terms, inverting the matrix means evaluating the partial correlations
    across the complete graph. [Figure 16.2](text00003.html#filepos0000784189) (a)
    visualizes the relationships implied by a covariance matrix of 50 × 50, that is
    50 nodes and 1225 edges. This complex structure magnifies small estimation errors,
    leading to incorrect solutions. Intuitively, it would be desirable to drop unnecessary
    edges.'
  prefs: []
  type: TYPE_NORMAL
- en: '![](Image00669.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '[**Figure 16.2**](text00003.html#filepos0000783703) The complete-graph (top)
    and the tree-graph (bottom) structures'
  prefs: []
  type: TYPE_NORMAL
- en: 'Correlation matrices can be represented as complete graphs, which lack the
    notion of hierarchy: Each investment is substitutable with another. In contrast,
    tree structures incorporate hierarchical relationships.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Let us consider for a moment the practical implications of such a topological
    structure. Suppose that an investor wishes to build a diversified portfolio of
    securities, including hundreds of stocks, bonds, hedge funds, real estate, private
    placements, etc. Some investments seem closer substitutes of one another, and
    other investments seem complementary to one another. For example, stocks could
    be grouped in terms of liquidity, size, industry, and region, where stocks within
    a given group compete for allocations. In deciding the allocation to a large publicly
    traded U.S. financial stock like J. P. Morgan, we will consider adding or reducing
    the allocation to another large publicly traded U.S. bank like Goldman Sachs,
    rather than a small community bank in Switzerland, or a real estate holding in
    the Caribbean. Yet, to a correlation matrix, all investments are potential substitutes
    to one another. In other words, correlation matrices lack the notion of *hierarchy.*
    This lack of hierarchical structure allows weights to vary freely in unintended
    ways, which is a root cause of CLA''s instability. [Figure 16.2](text00003.html#filepos0000784189)
    (b) visualizes a hierarchical structure known as a tree. A tree structure introduces
    two desirable features: (1) It has only *N* − 1 edges to connect *N* nodes, so
    the weights only rebalance among peers at various hierarchical levels; and (2)
    the weights are distributed top-down, consistent with how many asset managers
    build their portfolios (e.g., from asset class to sectors to individual securities).
    For these reasons, hierarchical structures are better designed to give not only
    stable but also intuitive results.'
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter we will study a new portfolio construction method that addresses
    CLA''s pitfalls using modern mathematics: graph theory and machine learning. This
    Hierarchical Risk Parity method uses the information contained in the covariance
    matrix without requiring its inversion or positive-definitiveness. HRP can even
    compute a portfolio based on a singular covariance matrix. The algorithm operates
    in three stages: tree clustering, quasi-diagonalization, and recursive bisection.'
  prefs: []
  type: TYPE_NORMAL
- en: '**16.4.1 Tree Clustering**'
  prefs: []
  type: TYPE_NORMAL
- en: Consider a *TxN* matrix of observations *X* , such as returns series of *N*
    variables over *T* periods. We would like to combine these *N* column-vectors
    into a hierarchical structure of clusters, so that allocations can flow downstream
    through a tree graph.
  prefs: []
  type: TYPE_NORMAL
- en: First, we compute an *NxN* correlation matrix with entries ρ = {ρ [*i* , *j*]
    } [*i* , *j* = 1, …, *N*] , where ρ [*i* , *j*] = ρ[ *X [*i*]* , *X [*j*]* ].
    We define the distance measure ![](Image00747.jpg) , ![](Image00841.jpg) , where
    *B* is the Cartesian product of items in {1, …, *i* , …, *N* }. This allows us
    to compute an *NxN* distance matrix *D* = { *d [*i*  ,  *j*] * } [*i*  ,  *j*  =
    1, …,  *N*] . Matrix *D* is a proper metric space (see Appendix 16.A.1 for a proof),
    in the sense that *d* [ *x* , *y* ] ≥ 0 (non-negativity), *d* [ *x* , *y* ] =
    0⇔ *X* = *Y* (coincidence), *d* [ *x* , *y* ] = *d* [ *Y* , *X* ] (symmetry),
    and *d* [ *X* , *Z* ] ≤ *d* [ *x* , *y* ] + *d* [ *Y* , *Z* ] (sub-additivity).
    See Example 16.1.
  prefs: []
  type: TYPE_NORMAL
- en: '![](Image00077.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '**Example 16.1 Encoding a correlation matrix** ***ρ*** **as a distance matrix**
    ***D***'
  prefs: []
  type: TYPE_NORMAL
- en: Second, we compute the Euclidean distance between any two column-vectors of
    *D* , ![](Image00167.jpg) , ![](Image00263.jpg) . Note the difference between
    distance metrics *d [*i*  ,  *j*] * and ![](Image00353.jpg) . Whereas *d [*i*  ,  *j*]
    * is defined on column-vectors of *X* , ![](Image00436.jpg) is defined on column-vectors
    of *D* (a distance of distances). Therefore, ![](Image00800.jpg) is a distance
    defined over the entire metric space *D* , as each ![](Image00034.jpg) is a function
    of the entire correlation matrix (rather than a particular cross-correlation pair).
    See Example 16.2.
  prefs: []
  type: TYPE_NORMAL
- en: '![](Image00127.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '**Example 16.2 Euclidean distance of correlation distances**'
  prefs: []
  type: TYPE_NORMAL
- en: Third, we cluster together the pair of columns ( *i* *, *j* *) such that ![](Image00218.jpg)
    , and denote this cluster as *u* [1]. See Example 16.3.
  prefs: []
  type: TYPE_NORMAL
- en: '![](Image00310.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '**Example 16.3 Clustering items**'
  prefs: []
  type: TYPE_NORMAL
- en: Fourth, we need to define the distance between a newly formed cluster *u* [1]
    and the single (unclustered) items, so that ![](Image00402.jpg) may be updated.
    In hierarchical clustering analysis, this is known as the “linkage criterion.”
    For example, we can define the distance between an item *i* of ![](Image00470.jpg)
    and the new cluster *u* [1] as ![](Image00549.jpg) (the nearest point algorithm).
    See Example 16.4.
  prefs: []
  type: TYPE_NORMAL
- en: '![](Image00653.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '**Example 16.4 Updating matrix** ![](Image00660.jpg) **with the new cluster**
    ***u***'
  prefs: []
  type: TYPE_NORMAL
- en: Fifth, matrix ![](Image00819.jpg) is updated by appending ![](Image00058.jpg)
    and dropping the clustered columns and rows *j* ∈ *u* [1]. See Example 16.5.
  prefs: []
  type: TYPE_NORMAL
- en: '![](Image00272.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '![](Image00244.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '**Example 16.5 Updating matrix** ![](Image00334.jpg) **with the new cluster**
    ***u***'
  prefs: []
  type: TYPE_NORMAL
- en: Sixth, applied recursively, steps 3, 4, and 5 allow us to append *N* − 1 such
    clusters to matrix *D* , at which point the final cluster contains all of the
    original items, and the clustering algorithm stops. See Example 16.6.
  prefs: []
  type: TYPE_NORMAL
- en: '![](Image00716.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '**Example 16.6 Recursion in search of remaining clusters**'
  prefs: []
  type: TYPE_NORMAL
- en: '[Figure 16.3](text00003.html#filepos0000797066) displays the clusters formed
    at each iteration for this example, as well as the distances ![](Image00487.jpg)
    that triggered every cluster (third step). This procedure can be applied to a
    wide array of distance metrics *d [*i*  ,  *j*] * , ![](Image00737.jpg) and ![](Image00674.jpg)
    , beyond those illustrated in this chapter. See Rokach and Maimon [2005] for alternative
    metrics, the discussion on Fiedler''s vector and Stewart''s spectral clustering
    method in Brualdi [2010], as well as algorithms in the scipy library. ^([  2  ](text00003.html#filepos0000849093))
    Snippet 16.1 provides an example of tree clustering using scipy functionality.'
  prefs: []
  type: TYPE_NORMAL
- en: '![](Image00752.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '[**Figure 16.3**](text00003.html#filepos0000795745) Sequence of cluster formation'
  prefs: []
  type: TYPE_NORMAL
- en: A tree structure derived from our numerical example, here plotted as a dendogram.
    The y-axis measures the distance between the two merging leaves.
  prefs: []
  type: TYPE_NORMAL
- en: '**SNIPPET 16.1 TREE CLUSTERING USING SCIPY FUNCTIONALITY**'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '![](Image00848.jpg)'
  prefs:
  - PREF_BQ
  type: TYPE_IMG
- en: This stage allows us to define a linkage matrix as an ( *N* − 1) *x* 4 matrix
    with structure *Y* = {( *y [*m* , 1]* , *y [*m* , 2]* , *y [*m* , 3]* , *y [*m*
    , 4]* )} [*m* = 1, …, *N* − 1] (i.e., with one 4-tuple per cluster). Items ( *y
    [*m* , 1]* , *y [*m* , 2]* ) report the constituents. Item *y [*m* , 3]* reports
    the distance between *y [*m* , 1]* and *y [*m* , 2]* , that is ![](Image00082.jpg)
    . Item *y [*m*  , 4] * ≤ *N* reports the number of original items included in
    cluster *m* .
  prefs: []
  type: TYPE_NORMAL
- en: '**16.4.2 Quasi-Diagonalization**'
  prefs: []
  type: TYPE_NORMAL
- en: 'This stage reorganizes the rows and columns of the covariance matrix, so that
    the largest values lie along the diagonal. This quasi-diagonalization of the covariance
    matrix (without requiring a change of basis) renders a useful property: Similar
    investments are placed together, and dissimilar investments are placed far apart
    (see [Figures 16.5](text00003.html#filepos0000809164) and [16.6](text00003.html#filepos0000809811)
    for an example). The algorithm works as follows: We know that each row of the
    linkage matrix merges two branches into one. We replace clusters in ( *y [*N*
    − 1, 1]* , *y [*N* − 1, 2]* ) with their constituents recursively, until no clusters
    remain. These replacements preserve the order of the clustering. The output is
    a sorted list of original (unclustered) items. This logic is implemented in Snippet
    16.2.'
  prefs: []
  type: TYPE_NORMAL
- en: '**SNIPPET 16.2 QUASI-DIAGONALIZATION**'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '![](Image00390.jpg)'
  prefs:
  - PREF_BQ
  type: TYPE_IMG
- en: '**16.4.3 Recursive Bisection**'
  prefs: []
  type: TYPE_NORMAL
- en: 'Stage 2 has delivered a quasi-diagonal matrix. The inverse-variance allocation
    is optimal for a diagonal covariance matrix (see Appendix 16.A.2 for a proof).
    We can take advantage of these facts in two different ways: (1) bottom-up, to
    define the variance of a contiguous subset as the variance of an inverse-variance
    allocation; or (2) top-down, to split allocations between adjacent subsets in
    inverse proportion to their aggregated variances. The following algorithm formalizes
    this idea:'
  prefs: []
  type: TYPE_NORMAL
- en: 'The algorithm is initialized by:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'setting the list of items: *L* = {*L [0]* }, with *L [0]* = {*n* } [*n* = 1,
    …, *N*]'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: 'assigning a unit weight to all items: *w [*n*]* = 1, ∀*n* = 1, …, *N*'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: If |*L [*i*]* | = 1,  ∀*L [*i*]* ∈ *L* , then stop.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'For each *L [*i*]* ∈ *L* such that |*L [*i*]* | > 1:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: bisect *L [*i*]* into two subsets, *L ^((1)) [*i*]* ∪*L [*i*] ^((2))* = *L [*i*]*
    , where ![](Image00268.jpg) , and the order is preserved
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: define the variance of *L ^(( *j* )) [*i*]* , *j* = 1, 2, as the quadratic form
    ![](Image00359.jpg) , where *V ^((  *j*  )) [   *i*   ] * is the covariance matrix
    between the constituents of the *L ^((  *j*  )) [   *i*   ] * bisection, and ![](Image00440.jpg)
    , where diag[.] and tr[.] are the diagonal and trace operators
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: 'compute the split factor: ![](Image00503.jpg) , so that 0 ≤ α [*i*] ≤ 1'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: re-scale allocations *w [*n*]* by a factor of α [*i*] , ∀*n* ∈ *L ^((1)) [*i*]*
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: re-scale allocations *w [*n*]* by a factor of (1 − α [*i*] ), ∀*n* ∈ *L ^((2))
    [*i*]*
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Loop to step 2
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Step 3b takes advantage of the quasi-diagonalization bottom-up, because it defines
    the variance of the partition *L ^(( *j* )) [*i*]* using inverse-variance weightings
    ![](Image00607.jpg) . Step 3c takes advantage of the quasi-diagonalization top-down,
    because it splits the weight in inverse proportion to the cluster's variance.
    This algorithm guarantees that 0 ≤ *w [*i*] * ≤ 1, ∀ *i* = 1, …, *N* , and ![](Image00688.jpg)
    , because at each iteration we are splitting the weights received from higher
    hierarchical levels. Constraints can be easily introduced in this stage, by replacing
    the equations in steps 3c, 3d, and 3e according to the user's preferences. Stage
    3 is implemented in Snippet 16.3.
  prefs: []
  type: TYPE_NORMAL
- en: '**SNIPPET 16.3 RECURSIVE BISECTION**'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '![](Image00777.jpg)'
  prefs:
  - PREF_BQ
  type: TYPE_IMG
- en: This concludes a first description of the HRP algorithm, which solves the allocation
    problem in best-case deterministic logarithmic time, ![](Image00460.jpg) , and
    worst-case deterministic linear time, ![](Image00049.jpg) . Next, we will put
    to practice what we have learned, and evaluate the method's accuracy out-of-sample.
  prefs: []
  type: TYPE_NORMAL
- en: '**16.5 A Numerical Example**'
  prefs: []
  type: TYPE_NORMAL
- en: We begin by simulating a matrix of observations *X* , of order (10000 *x* 10).
    The correlation matrix is visualized in [Figure 16.4](text00003.html#filepos0000808681)
    as a heatmap. [Figure 16.5](text00003.html#filepos0000809164) displays the dendogram
    of the resulting clusters (stage 1). [Figure 16.6](text00003.html#filepos0000809811)
    shows the same correlation matrix, reorganized in blocks according to the identified
    clusters (stage 2). Appendix 16.A.3 provides the code used to generate this numerical
    example.
  prefs: []
  type: TYPE_NORMAL
- en: '![](Image00196.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '[**Figure 16.4**](text00003.html#filepos0000808006) Heat-map of original covariance
    matrix'
  prefs: []
  type: TYPE_NORMAL
- en: This correlation matrix has been computed using function `generateData` from
    snippet 16.4 (see Section 16.A.3). The last five columns are partially correlated
    to some of the first five series.
  prefs: []
  type: TYPE_NORMAL
- en: '![](Image00452.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '[**Figure 16.5**](text00003.html#filepos0000800129) Dendogram of cluster formation'
  prefs: []
  type: TYPE_NORMAL
- en: The clustering procedure has correctly identified that series 9 and 10 were
    perturbations of series 2, hence (9, 2, 10) are clustered together. Similarly,
    7 is a perturbation of 1, 6 is a perturbation of 3, and 8 is a perturbation of
    5\. The only original item that was not perturbated is 4, and that is the one
    item for which the clustering algorithm found no similarity.
  prefs: []
  type: TYPE_NORMAL
- en: '![](Image00383.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '[**Figure 16.6**](text00003.html#filepos0000800239) Clustered covariance matrix'
  prefs: []
  type: TYPE_NORMAL
- en: Stage 2 quasi-diagonalizes the correlation matrix, in the sense that the largest
    values lie along the diagonal. However, unlike PCA or similar procedures, HRP
    does not require a change of basis. HRP solves the allocation problem robustly,
    while working with the original investments.
  prefs: []
  type: TYPE_NORMAL
- en: 'On this random data, we compute HRP''s allocations (stage 3), and compare them
    to the allocations from two competing methodologies: (1) Quadratic optimization,
    as represented by CLA''s minimum-variance portfolio (the only portfolio of the
    efficient frontier that does not depend on returns’ means); and (2) traditional
    risk parity, exemplified by the Inverse-Variance Portfolio (IVP). See Bailey and
    López de Prado [2013] for a comprehensive implementation of CLA, and Appendix
    16.A.2 for a derivation of IVP. We apply the standard constraints that 0 ≤ *w
    [*i*]* ≤ 1 (non-negativity), ∀ *i* = 1, …, *N* , and ![](Image00453.jpg) (full
    investment). Incidentally, the condition number for the covariance matrix in this
    example is only 150.9324, not particularly high and therefore not unfavorable
    to CLA.'
  prefs: []
  type: TYPE_NORMAL
- en: 'From the allocations in [Table 16.1](text00003.html#filepos0000812181) , we
    can appreciate a few stylized features: First, CLA concentrates 92.66% of the
    allocation on the top-5 holdings, while HRP concentrates only 62.57%. Second,
    CLA assigns zero weight to 3 investments (without the 0 ≤ *w [*i*]* constraint,
    the allocation would have been negative). Third, HRP seems to find a compromise
    between CLA''s concentrated solution and traditional risk parity''s IVP allocation.
    The reader can use the code in Appendix 16.A.3 to verify that these findings generally
    hold for alternative random covariance matrices.'
  prefs: []
  type: TYPE_NORMAL
- en: '[**Table 16.1**](text00003.html#filepos0000811448) **A Comparison of Three
    Allocations**'
  prefs: []
  type: TYPE_NORMAL
- en: '| **Weight #** | **CLA** | **HRP** | **IVP** |'
  prefs: []
  type: TYPE_TB
- en: '| 1 | 14.44% | 7.00% | 10.36% |'
  prefs: []
  type: TYPE_TB
- en: '| 2 | 19.93% | 7.59% | 10.28% |'
  prefs: []
  type: TYPE_TB
- en: '| 3 | 19.73% | 10.84% | 10.36% |'
  prefs: []
  type: TYPE_TB
- en: '| 4 | 19.87% | 19.03% | 10.25% |'
  prefs: []
  type: TYPE_TB
- en: '| 5 | 18.68% | 9.72% | 10.31% |'
  prefs: []
  type: TYPE_TB
- en: '| 6 | 0.00% | 10.19% | 9.74% |'
  prefs: []
  type: TYPE_TB
- en: '| 7 | 5.86% | 6.62% | 9.80% |'
  prefs: []
  type: TYPE_TB
- en: '| 8 | 1.49% | 9.10% | 9.65% |'
  prefs: []
  type: TYPE_TB
- en: '| 9 | 0.00% | 7.12% | 9.64% |'
  prefs: []
  type: TYPE_TB
- en: '| 10 | 0.00% | 12.79% | 9.61% |'
  prefs: []
  type: TYPE_TB
- en: 'A characteristic outcome of the three methods studied: CLA concentrates weights
    on a few investments, hence becoming exposed to idiosyncratic shocks. IVP evenly
    spreads weights through all investments, ignoring the correlation structure. This
    makes it vulnerable to systemic shocks. HRP finds a compromise between diversifying
    across all investments and diversifying across cluster, which makes it more resilient
    against both types of shocks.'
  prefs: []
  type: TYPE_NORMAL
- en: What drives CLA's extreme concentration is its goal of minimizing the portfolio's
    risk. And yet both portfolios have a very similar standard deviation (σ [*HRP*]
    = 0.4640, σ [*CLA*] = 0.4486). So CLA has discarded half of the investment universe
    in favor of a minor risk reduction. The reality of course is that CLA's portfolio
    is deceitfully diversified, because any distress situation affecting the top-5
    allocations will have a much greater negative impact on CLA's than on HRP's portfolio.
  prefs: []
  type: TYPE_NORMAL
- en: '**16.6 Out-of-Sample Monte Carlo Simulations**'
  prefs: []
  type: TYPE_NORMAL
- en: In our numerical example, CLA's portfolio has lower risk than HRP's in-sample.
    However, the portfolio with minimum variance in-sample is not necessarily the
    one with minimum variance out-of-sample. It would be all too easy for us to pick
    a particular historical dataset where HRP outperforms CLA and IVP (see Bailey
    and López de Prado [2014], and recall our discussion of selection bias in Chapter
    11). Instead, in this section we follow the backtesting paradigm explained in
    Chapter 13, and evaluate via Monte Carlo the performance out-of-sample of HRP
    against CLA's minimum-variance and traditional risk parity's IVP allocations.
    This will also help us understand what features make a method preferable to the
    rest, regardless of anecdotal counter-examples.
  prefs: []
  type: TYPE_NORMAL
- en: First, we generate 10 series of random Gaussian returns (520 observations, equivalent
    to 2 years of daily history), with 0 mean and an arbitrary standard deviation
    of 10%. Real prices exhibit frequent jumps (Merton [1976]) and returns are not
    cross-sectionally independent, so we must add random shocks and a random correlation
    structure to our generated data. Second, we compute HRP, CLA, and IVP portfolios
    by looking back at 260 observations (a year of daily history). These portfolios
    are re-estimated and rebalanced every 22 observations (equivalent to a monthly
    frequency). Third, we compute the out-of-sample returns associated with those
    three portfolios. This procedure is repeated 10,000 times.
  prefs: []
  type: TYPE_NORMAL
- en: 'All mean portfolio returns out-of-sample are essentially 0, as expected. The
    critical difference comes from the variance of the out-of-sample portfolio returns:
    σ ² [*CLA*] = 0.1157, σ ² [*IVP*] = 0.0928, and σ ² [*HRP*] = 0.0671 *.* Although
    CLA''s goal is to deliver the lowest variance (that is the objective of its optimization
    program), its performance happens to exhibit the highest variance out-of-sample,
    and 72.47% greater variance than HRP''s. This experimental finding is consistent
    with the historical evidence in De Miguel et al. [2009]. In other words, HRP would
    improve the out-of-sample Sharpe ratio of a CLA strategy by about 31.3%, a rather
    significant boost. Assuming that the covariance matrix is diagonal brings some
    stability to the IVP; however, its variance is still 38.24% greater than HRP''s.
    This variance reduction out-of-sample is critically important to risk parity investors,
    given their use of substantial leverage. See Bailey et al. [2014] for a broader
    discussion of in-sample vs. out-of-sample performance.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The mathematical proof for HRP''s outperformance over Markowitz''s CLA and
    traditional risk parity''s IVP is somewhat involved and beyond the scope of this
    chapter. In intuitive terms, we can understand the above empirical results as
    follows: Shocks affecting a specific investment penalize CLA''s concentration.
    Shocks involving several correlated investments penalize IVP''s ignorance of the
    correlation structure. HRP provides better protection against both common and
    idiosyncratic shocks by finding a compromise between diversification across all
    investments and diversification across clusters of investments at multiple hierarchical
    levels. [Figure 16.7](text00003.html#filepos0000822841) plots the time series
    of allocations for the first of the 10,000 runs.'
  prefs: []
  type: TYPE_NORMAL
- en: '![](Image00067.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '![](Image00039.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '[**Figure 16.7**](text00003.html#filepos0000822428) (a) Time series of allocations
    for IVP.'
  prefs: []
  type: TYPE_NORMAL
- en: Between the first and second rebalance, one investment receives an idiosyncratic
    shock, which increases its variance. IVP's response is to reduce the allocation
    to that investment, and spread that former exposure across all other investments.
    Between the fifth and sixth rebalance, two investments are affected by a common
    shock. IVP's response is the same. As a result, allocations among the seven unaffected
    investments grow over time, regardless of their correlation.
  prefs: []
  type: TYPE_NORMAL
- en: (b) Time series of allocations for HRP
  prefs: []
  type: TYPE_NORMAL
- en: HRP's response to the idiosyncratic shock is to reduce the allocation to the
    affected investment, and use that reduced amount to increase the allocation to
    a correlated investment that was unaffected. As a response to the common shock,
    HRP reduces allocation to the affected investments and increases allocation to
    uncorrelated ones (with lower variance).
  prefs: []
  type: TYPE_NORMAL
- en: (c) Time series of allocations for CLA
  prefs: []
  type: TYPE_NORMAL
- en: CLA allocations respond erratically to idiosyncratic and common shocks. If we
    had taken into account rebalancing costs, CLA's performance would have been very
    negative.
  prefs: []
  type: TYPE_NORMAL
- en: Appendix 16.A.4 provides the Python code that implements the above study. The
    reader can experiment with different parameter configurations and reach similar
    conclusions. In particular, HRP's out-of-sample outperformance becomes even more
    substantial for larger investment universes, or when more shocks are added, or
    a stronger correlation structure is considered, or rebalancing costs are taken
    into account. Each of these CLA rebalances incurs transaction costs that can accumulate
    into prohibitive losses over time.
  prefs: []
  type: TYPE_NORMAL
- en: '**16.7 Further Research**'
  prefs: []
  type: TYPE_NORMAL
- en: The methodology introduced in this chapter is flexible, scalable and admits
    multiple variations of the same ideas. Using the code provided, readers can research
    and evaluate what HRP configurations work best for their particular problem. For
    example, at stage 1 they can apply alternative definitions of *d [*i* , *j*]*
    , ![](Image00088.jpg) and ![](Image00508.jpg) , or different clustering algorithms,
    like biclustering; at stage 3, they can use different functions for ![](Image00118.jpg)
    and α, or alternative allocation constraints. Instead of carrying out a recursive
    bisection, stage 3 could also split allocations top-down using the clusters from
    stage 1.
  prefs: []
  type: TYPE_NORMAL
- en: It is relatively straightforward to incorporate forecasted returns, Ledoit-Wolf
    shrinkage, and Black-Litterman–style views to this hierarchical approach. In fact,
    the inquisitive reader may have realized that, at its core, HRP is essentially
    a robust procedure to avoid matrix inversions, and the same ideas underlying HRP
    can be used to replace many econometric regression methods, notorious for their
    unstable outputs (like VAR or VECM). [Figure 16.8](text00003.html#filepos0000827020)
    displays (a) a large correlation matrix of fixed income securities before and
    (b) after clustering, with over 2.1 million entries. Traditional optimization
    or econometric methods fail to recognize the hierarchical structure of financial
    Big Data, where the numerical instabilities defeat the benefits of the analysis,
    resulting in unreliable and detrimental outcomes.
  prefs: []
  type: TYPE_NORMAL
- en: '![](Image00278.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '[**Figure 16.8**](text00003.html#filepos0000826427) Correlation matrix before
    and after clustering'
  prefs: []
  type: TYPE_NORMAL
- en: The methodology described in this chapter can be applied to problems beyond
    optimization. For example, a PCA analysis of a large fixed income universe suffers
    the same drawbacks we described for CLA. Small-data techniques developed decades
    and centuries ago (factor models, regression analysis, econometrics) fail to recognize
    the hierarchical nature of financial big data.
  prefs: []
  type: TYPE_NORMAL
- en: Kolanovic et al. [2017] conducted a lengthy study of HRP, concluding that “HRP
    delivers superior risk-adjusted returns. Whilst both the HRP and the MV portfolios
    deliver the highest returns, the HRP portfolios match with volatility targets
    much better than MV portfolios. We also run simulation studies to confirm the
    robustness of our findings, in which HRP consistently deliver a superior performance
    over MV and other risk-based strategies […] HRP portfolios are truly diversified
    with a higher number of uncorrelated exposures, and less extreme weights and risk
    allocations.”
  prefs: []
  type: TYPE_NORMAL
- en: Raffinot [2017] concludes that “empirical results indicate that hierarchical
    clustering based portfolios are robust, truly diversified and achieve statistically
    better risk-adjusted performances than commonly used portfolio optimization techniques.”
  prefs: []
  type: TYPE_NORMAL
- en: '**16.8 Conclusion**'
  prefs: []
  type: TYPE_NORMAL
- en: Exact analytical solutions can perform much worse than approximate ML solutions.
    Although mathematically correct, quadratic optimizers in general, and Markowitz's
    CLA in particular, are known to deliver generally unreliable solutions due to
    their instability, concentration, and underperformance. The root cause for these
    issues is that quadratic optimizers require the inversion of a covariance matrix.
    Markowitz's curse is that the more correlated investments are, the greater is
    the need for a diversified portfolio, and yet the greater are that portfolio's
    estimation errors.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we have exposed a major source of quadratic optimizers’ instability:
    A matrix of size *N* is associated with a complete graph with ![](Image00136.jpg)
    edges. With so many edges connecting the nodes of the graph, weights are allowed
    to rebalance with complete freedom. This lack of hierarchical structure means
    that small estimation errors will lead to entirely different solutions. HRP replaces
    the covariance structure with a tree structure, accomplishing three goals: (1)
    Unlike traditional risk parity methods, it fully utilizes the information contained
    in the covariance matrix, (2) weights’ stability is recovered and (3) the solution
    is intuitive by construction. The algorithm converges in deterministic logarithmic
    (best case) or linear (worst case) time.'
  prefs: []
  type: TYPE_NORMAL
- en: HRP is robust, visual, and flexible, allowing the user to introduce constraints
    or manipulate the tree structure without compromising the algorithm's search.
    These properties are derived from the fact that HRP does not require covariance
    invertibility. Indeed, HRP can compute a portfolio on an ill-degenerated or even
    a singular covariance matrix.
  prefs: []
  type: TYPE_NORMAL
- en: 'This chapter focuses on a portfolio construction application; however, the
    reader will find other practical uses for making decisions under uncertainty,
    particularly in the presence of a nearly singular covariance matrix: capital allocation
    to portfolio managers, allocations across algorithmic strategies, bagging and
    boosting of machine learning signals, forecasts from random forests, replacement
    to unstable econometric models (VAR, VECM), etc.'
  prefs: []
  type: TYPE_NORMAL
- en: Of course, quadratic optimizers like CLA produce the minimum-variance portfolio
    in-sample (that is its objective function). Monte Carlo experiments show that
    HRP delivers lower out-of-sample variance than CLA or traditional risk parity
    methods (IVP). Since Bridgewater pioneered risk parity in the 1990s, some of the
    largest asset managers have launched funds that follow this approach, for combined
    assets in excess of $500 billion. Given their extensive use of leverage, these
    funds should benefit from adopting a more stable risk parity allocation method,
    thus achieving superior risk-adjusted returns and lower rebalance costs.
  prefs: []
  type: TYPE_NORMAL
- en: '**APPENDICES**'
  prefs: []
  type: TYPE_NORMAL
- en: '**16.A.1 Correlation-based Metric**'
  prefs: []
  type: TYPE_NORMAL
- en: Consider two real-valued vectors *X* , *Y* of size *T* , and a correlation variable
    ρ[ *x* , *y* ], with the only requirement that σ[ *x* , *y* ] = ρ[ *x* , *y* ]σ[
    *X* ]σ[ *Y* ], where σ[ *x* , *y* ] is the covariance between the two vectors,
    and σ[.] is the standard deviation. Note that Pearson's is not the only correlation
    to satisfy these requirements.
  prefs: []
  type: TYPE_NORMAL
- en: Let us prove that ![](Image00560.jpg) is a true metric. First, the Euclidean
    distance between the two vectors is ![](Image00159.jpg) . Second, we z-standardize
    those vectors as ![](Image00486.jpg) , ![](Image00179.jpg) . Consequently, 0 ≤
    ρ[ *x* , *y* ] = ρ[ *x* , *y* ]. Third, we derive the Euclidean distance *d* [
    *x* , *y* ] as,
  prefs: []
  type: TYPE_NORMAL
- en: '![](Image00603.jpg)'
  prefs: []
  type: TYPE_IMG
- en: In other words, the distance *d* [ *x* , *y* ] is a linear multiple of the Euclidean
    distance between the vectors { *X* , *Y* } after z-standardization, hence it inherits
    the true-metric properties of the Euclidean distance.
  prefs: []
  type: TYPE_NORMAL
- en: Similarly, we can prove that ![](Image00205.jpg) descends to a true metric on
    the ![](Image00643.jpg) /2 ![](Image00233.jpg) quotient. In order to do that,
    we redefine ![](Image00662.jpg) , where sgn[.] is the sign operator, so that 0
    ≤ ρ[ *x* , *y* ] = |ρ[ *x* , *y* ]|. Then,
  prefs: []
  type: TYPE_NORMAL
- en: '![](Image00254.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '**16.A.2 Inverse Variance Allocation**'
  prefs: []
  type: TYPE_NORMAL
- en: Stage 3 (see Section 16.4.3) splits a weight in inverse proportion to the subset's
    variance. We now prove that such allocation is optimal when the covariance matrix
    is diagonal. Consider the standard quadratic optimization problem of size *N*
    ,
  prefs: []
  type: TYPE_NORMAL
- en: '![](Image00423.jpg)'
  prefs: []
  type: TYPE_IMG
- en: with solution ![](Image00479.jpg) . For the characteristic vector *a* = 1 [*N*]
    , the solution is the minimum variance portfolio. If *V* is diagonal, ![](Image00568.jpg)
    . In the particular case of *N* = 2, ![](Image00665.jpg) , which is how stage
    3 splits a weight between two bisections of a subset.
  prefs: []
  type: TYPE_NORMAL
- en: '**16.A.3 Reproducing the Numerical Example**'
  prefs: []
  type: TYPE_NORMAL
- en: Snippet 16.4 can be used to reproduce our results and simulate additional numerical
    examples. Function `generateData` produces a matrix of time series where a number
    `size0` of vectors are uncorrelated, and a number `size1` of vectors are correlated.
    The reader can change the `np.random.seed` in `generateData` to run alternative
    examples and gain an intuition of how HRP works. Scipy's function `linkage` can
    be used to perform stage 1 (Section 16.4.1), function `getQuasiDiag` performs
    stage 2 (Section 16.4.2), and function `getRecBipart` carries out stage 3 (Section
    16.4.3).
  prefs: []
  type: TYPE_NORMAL
- en: '**SNIPPET 16.4 FULL IMPLEMENTATION OF THE HRP ALGORITHM**'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '![](Image00720.jpg) ![](Image00790.jpg) ![](Image00073.jpg)'
  prefs:
  - PREF_BQ
  type: TYPE_IMG
- en: '**16.A.4 Reproducing the Monte Carlo Experiment**'
  prefs: []
  type: TYPE_NORMAL
- en: 'Snippet 16.5 implements Monte Carlo experiments on three allocation methods:
    HRP, CLA, and IVP. All libraries are standard except for HRP, which is provided
    in Appendix 16.A.3, and CLA, which can be found in Bailey and López de Prado [2013].
    The subroutine `generateData` simulates the correlated data, with two types of
    random shocks: common to various investments and specific to a single investment.
    There are two shocks of each type, one positive and one negative. The variables
    for the experiments are set as arguments of `hrpMC` . They were chosen arbitrarily,
    and the user can experiment with alternative combinations.'
  prefs: []
  type: TYPE_NORMAL
- en: '**SNIPPET 16.5 MONTE CARLO EXPERIMENT ON HRP OUT-OF-SAMPLE PERFORMANCE**'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '![](Image00350.jpg) ![](Image00258.jpg) ![](Image00348.jpg)'
  prefs:
  - PREF_BQ
  type: TYPE_IMG
- en: '**Exercises**'
  prefs: []
  type: TYPE_NORMAL
- en: 'Given the PnL series on *N* investment strategies:'
  prefs:
  - PREF_OL
  - PREF_BQ
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'Align them to the average frequency of their bets (e.g., weekly observations
    for strategies that trade on a weekly basis). Hint: This kind of data alignment
    is sometimes called “downsampling.”'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Compute the covariance of their returns, *V.*
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Identify the hierarchical clusters among the *N* strategies.
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Plot the clustered correlation matrix of the *N* strategies.
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Using the clustered covariance matrix *V* from exercise 1:'
  prefs:
  - PREF_OL
  - PREF_BQ
  - PREF_BQ
  type: TYPE_NORMAL
- en: Compute the HRP allocations.
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Compute the CLA allocations.
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Compute the IVP allocations.
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Using the covariance matrix *V* from exercise 1:'
  prefs:
  - PREF_OL
  - PREF_BQ
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'Perform a spectral decomposition: *VW* = *W* Λ.'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Form an array ϵ by drawing *N* random numbers from a *U* [0, 1] distribution.
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Form an *NxN* matrix ![](Image00795.jpg) , where ![](Image00496.jpg) , *n* =
    1, …, *N* .
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Compute ![](Image00596.jpg) .
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Repeat exercise 2, this time using ![](Image00681.jpg) as covariance matrix.
    What allocation method has been most impacted by the re-scaling of spectral variances?
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: How would you modify the HRP algorithm to produce allocations that add up to
    0, where | *w [*n*]* | ≤ 1, ∀ *n* = 1, …, *N* ?
  prefs:
  - PREF_OL
  - PREF_BQ
  - PREF_BQ
  type: TYPE_NORMAL
- en: Can you think of an easy way to incorporate expected returns in the HRP allocations?
  prefs:
  - PREF_OL
  - PREF_BQ
  - PREF_BQ
  type: TYPE_NORMAL
- en: '**References**'
  prefs: []
  type: TYPE_NORMAL
- en: 'Bailey, D. and M. López de Prado (2012): “Balanced baskets: A new approach
    to trading and hedging risks.” *Journal of Investment Strategies* , Vol. 1, No.
    4, pp. 21–62\. Available at [http://ssrn.com/abstract=2066170](http://ssrn.com/abstract=2066170)
    .'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Bailey, D. and M. López de Prado (2013): “An open-source implementation of
    the critical-line algorithm for portfolio optimization.” *Algorithms* , Vol. 6,
    No. 1, pp. 169–196\. Available at [http://ssrn.com/abstract=2197616](http://ssrn.com/abstract=2197616)
    .'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Bailey, D., J. Borwein, M. López de Prado, and J. Zhu (2014) “Pseudo-mathematics
    and financial charlatanism: The effects of backtest overfitting on out-of-sample
    performance.” *Notices of the American Mathematical Society* , Vol. 61, No. 5,
    pp. 458–471\. Available at [http://ssrn.com/abstract=2308659](http://ssrn.com/abstract=2308659)
    .'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Bailey, D. and M. López de Prado (2014): “The deflated Sharpe ratio: Correcting
    for selection bias, backtest overfitting and non-normality.” *Journal of Portfolio
    Management* , Vol. 40, No. 5, pp. 94–107.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Black, F. and R. Litterman (1992): “Global portfolio optimization.” *Financial
    Analysts Journal* , Vol. 48, pp. 28–43.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Brualdi, R. (2010): “The mutually beneficial relationship of graphs and matrices.”
    Conference Board of the Mathematical Sciences, Regional Conference Series in Mathematics,
    Nr. 115.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Calkin, N. and M. López de Prado (2014): “Stochastic flow diagrams.” *Algorithmic
    Finance* , Vol. 3, No. 1, pp. 21–42\. Availble at [http://ssrn.com/abstract=2379314](http://ssrn.com/abstract=2379314)
    .'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Calkin, N. and M. López de Prado (2014): “The topology of macro financial flows:
    An application of stochastic flow diagrams.” *Algorithmic Finance* , Vol. 3, No.
    1, pp. 43–85\. Available at [http://ssrn.com/abstract=2379319](http://ssrn.com/abstract=2379319)
    .'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Clarke, R., H. De Silva, and S. Thorley (2002): “Portfolio constraints and
    the fundamental law of active management.” *Financial Analysts Journal* , Vol.
    58, pp. 48–66.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'De Miguel, V., L. Garlappi, and R. Uppal (2009): “Optimal versus naive diversification:
    How inefficient is the 1/N portfolio strategy?” *Review of Financial Studies*
    , Vol. 22, pp. 1915–1953.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Jurczenko, E. (2015): *Risk-Based and Factor Investing* , 1st ed. Elsevier
    Science.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Kolanovic, M., A. Lau, T. Lee, and R. Krishnamachari (2017): “Cross asset portfolios
    of tradable risk premia indices. Hierarchical risk parity: Enhancing returns at
    target volatility.” White paper, Global Quantitative & Derivatives Strategy. J.P.
    Morgan, April 26.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Kolm, P., R. Tutuncu and F. Fabozzi (2014): “60 years of portfolio optimization.”
    *European Journal of Operational Research* , Vol. 234, No. 2, pp. 356–371.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Kuhn, H. W. and A. W. Tucker (1951): “Nonlinear programming.” Proceedings of
    2nd Berkeley Symposium. Berkeley, University of California Press, pp. 481–492.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Markowitz, H. (1952): “Portfolio selection.” *Journal of Finance* , Vol. 7,
    pp. 77–91.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Merton, R. (1976): “Option pricing when underlying stock returns are discontinuous.”
    *Journal of Financial Economics* , Vol. 3, pp. 125–144.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Michaud, R. (1998): *Efficient Asset Allocation: A Practical Guide to Stock
    Portfolio Optimization and Asset Allocation* , 1st ed. Harvard Business School
    Press.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Ledoit, O. and M. Wolf (2003): “Improved estimation of the covariance matrix
    of stock returns with an application to portfolio selection.” *Journal of Empirical
    Finance* , Vol. 10, No. 5, pp. 603–621.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Raffinot, T. (2017): “Hierarchical clustering based asset allocation.” *Journal
    of Portfolio Management* , forthcoming.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Rokach, L. and O. Maimon (2005): “Clustering methods,” in Rokach, L. and O.
    Maimon, eds., *Data Mining and Knowledge Discovery Handbook* . Springer, pp. 321–352.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Notes**'
  prefs: []
  type: TYPE_NORMAL
- en: ^([1](text00003.html#filepos0000775451))    A short version of this chapter
    appeared in the *Journal of Portfolio Management,* Vo1\. 42, No. 4, pp. 59–69,
    Summer of 2016.
  prefs: []
  type: TYPE_NORMAL
- en: '^([2](text00003.html#filepos0000796752))    For additional metrics see:'
  prefs: []
  type: TYPE_NORMAL
- en: '[http://docs.scipy.org/doc/scipy/reference/generated/scipy.spatial.distance.pdist.html](http://docs.scipy.org/doc/scipy/reference/generated/scipy.spatial.distance.pdist.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[http://docs.scipy.org/doc/scipy-0.16.0/reference/generated/scipy.cluster.hierarchy.linkage.html](http://docs.scipy.org/doc/scipy-0.16.0/reference/generated/scipy.cluster.hierarchy.linkage.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**PART 4**'
  prefs: []
  type: TYPE_NORMAL
- en: '**Useful Financial Features**'
  prefs: []
  type: TYPE_NORMAL
- en: '[Chapter 17 Structural Breaks](text00003.html#filepos0000850574)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[Chapter 18 Entropy Features](text00003.html#filepos0000901244)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[Chapter 19 Microstructural Features](text00003.html#filepos0000963505)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**CHAPTER 17**'
  prefs: []
  type: TYPE_NORMAL
- en: '**Structural Breaks**'
  prefs: []
  type: TYPE_NORMAL
- en: '**17.1 Motivation**'
  prefs: []
  type: TYPE_NORMAL
- en: In developing an ML-based investment strategy, we typically wish to bet when
    there is a confluence of factors whose predicted outcome offers a favorable risk-adjusted
    return. Structural breaks, like the transition from one market regime to another,
    is one example of such a confluence that is of particular interest. For instance,
    a mean-reverting pattern may give way to a momentum pattern. As this transition
    takes place, most market participants are caught off guard, and they will make
    costly mistakes. This sort of errors is the basis for many profitable strategies,
    because the actors on the losing side will typically become aware of their mistake
    once it is too late. Before they accept their losses, they will act irrationally,
    try to hold the position, and hope for a comeback. Sometimes they will even increase
    a losing position, in desperation. Eventually they will be forced to stop loss
    or stop out. Structural breaks offer some of the best risk/rewards. In this chapter,
    we will review some methods that measure the likelihood of structural breaks,
    so that informative features can be built upon them.
  prefs: []
  type: TYPE_NORMAL
- en: '**17.2 Types of Structural Break Tests**'
  prefs: []
  type: TYPE_NORMAL
- en: 'We can classify structural break tests in two general categories:'
  prefs: []
  type: TYPE_NORMAL
- en: '**CUSUM tests:** These test whether the cumulative forecasting errors significantly
    deviate from white noise.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Explosiveness tests:** Beyond deviation from white noise, these test whether
    the process exhibits exponential growth or collapse, as this is inconsistent with
    a random walk or stationary process, and it is unsustainable in the long run.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Right-tail unit-root tests:** These tests evaluate the presence of exponential
    growth or collapse, while assuming an autoregressive specification.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Sub/super-martingale tests:** These tests evaluate the presence of exponential
    growth or collapse under a variety of functional forms.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**17.3 CUSUM Tests**'
  prefs: []
  type: TYPE_NORMAL
- en: In Chapter 2 we introduced the CUSUM filter, which we applied in the context
    of event-based sampling of bars. The idea was to sample a bar whenever some variable,
    like cumulative prediction errors, exceeded a predefined threshold. This concept
    can be further extended to test for structural breaks.
  prefs: []
  type: TYPE_NORMAL
- en: '**17.3.1 Brown-Durbin-Evans CUSUM Test on Recursive Residuals**'
  prefs: []
  type: TYPE_NORMAL
- en: This test was proposed by Brown, Durbin and Evans [1975]. Let us assume that
    at every observation *t* = 1, …, *T* , we count with an array of features *x [*t*]*
    predictive of a value *y [*t*]* . Matrix *X [*t*]* is composed of the time series
    of features *t* ≤ *T* , { *x [*i*]* } [*i* = 1, …, *t*] . These authors propose
    that we compute recursive least squares (RLS) estimates of β, based on the specification
  prefs: []
  type: TYPE_NORMAL
- en: '![](Image00766.jpg)'
  prefs: []
  type: TYPE_IMG
- en: which is fit on subsamples ([1, *k* + 1], [1, *k* + 2], …, [1, *T* ]), giving
    *T* − *k* least squares estimates ![](Image00004.jpg) . We can compute the standardized
    1-step ahead recursive residuals as
  prefs: []
  type: TYPE_NORMAL
- en: '![](Image00097.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '![](Image00185.jpg)'
  prefs: []
  type: TYPE_IMG
- en: The CUSUM statistic is defined as
  prefs: []
  type: TYPE_NORMAL
- en: '![](Image00280.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '![](Image00241.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Under the null hypothesis that β is some constant value, *H [0]* : β [*t*]
    = β, then *S [*t*]* ∼ *N* [0, *t* − *k* − 1]. One caveat of this procedure is
    that the starting point is chosen arbitrarily, and results may be inconsistent
    due to that.'
  prefs: []
  type: TYPE_NORMAL
- en: '**17.3.2 Chu-Stinchcombe-White CUSUM Test on Levels**'
  prefs: []
  type: TYPE_NORMAL
- en: 'This test follows Homm and Breitung [2012]. It simplifies the previous method
    by dropping { *x [*t*]* } [*t* = 1, …, *T*] , and assuming that *H [0]* : β [*t*]
    = 0, that is, we forecast no change (E [*t* − 1] [Δ *y [*t*]* ] = 0). This will
    allow us to work directly with *y [*t*]* levels, hence reducing the computational
    burden. We compute the standardized departure of log-price *y [*t*]* relative
    to the log-price at *y [*n*]* , *t* > *n* , as'
  prefs: []
  type: TYPE_NORMAL
- en: '![](Image00704.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '![](Image00230.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Under the null hypothesis *H [0]* : β [*t*] = 0, then *S [*n* , *t*]* ∼ *N*
    [0, 1]. The time-dependent critical value for the *one-sided test* is'
  prefs: []
  type: TYPE_NORMAL
- en: '![](Image00661.jpg)'
  prefs: []
  type: TYPE_IMG
- en: These authors derived via Monte Carlo that *b [0.05]* = 4.6\. One disadvantage
    of this method is that the reference level *y [*n*]* is set somewhat arbitrarily.
    To overcome this pitfall, we could estimate *S [*n* , *t*]* on a series of backward-shifting
    windows *n* ∈ [1, *t* ], and pick ![](Image00146.jpg) .
  prefs: []
  type: TYPE_NORMAL
- en: '**17.4 Explosiveness Tests**'
  prefs: []
  type: TYPE_NORMAL
- en: Explosiveness tests can be generally divided between those that test for one
    bubble and those that test for multiple bubbles. In this context, bubbles are
    not limited to price rallies, but they also include sell-offs. Tests that allow
    for multiple bubbles are more robust in the sense that a cycle of bubble-burst-bubble
    will make the series appear to be stationary to single-bubble tests. Maddala and
    Kim [1998], and Breitung [2014] offer good overviews of the literature.
  prefs: []
  type: TYPE_NORMAL
- en: '**17.4.1 Chow-Type Dickey-Fuller Test**'
  prefs: []
  type: TYPE_NORMAL
- en: A family of explosiveness tests was inspired by the work of Gregory Chow, starting
    with Chow [1960]. Consider the first order autoregressive process
  prefs: []
  type: TYPE_NORMAL
- en: '![](Image00345.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'where ϵ [*t*] is white noise. The null hypothesis is that *y [*t*]* follows
    a random walk, *H [0]* : ρ = 1, and the alternative hypothesis is that *y [*t*]*
    starts as a random walk but changes at time τ* *T* , where τ* ∈ (0, 1), into an
    explosive process:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](Image00276.jpg)'
  prefs: []
  type: TYPE_IMG
- en: At time *T* we can test for a switch (from random walk to explosive process)
    having taken place at time τ* *T* (break date). In order to test this hypothesis,
    we fit the following specification,
  prefs: []
  type: TYPE_NORMAL
- en: '![](Image00697.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'where *D [*t*]* [τ*] is a dummy variable that takes zero value if *t* < τ*
    *T* , and takes the value one if *t* ≥ τ* *T* . Then, the null hypothesis *H [0]*
    : δ = 0 is tested against the (one-sided) alternative *H [1]* : δ > 1:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](Image00299.jpg)'
  prefs: []
  type: TYPE_IMG
- en: The main drawback of this method is that τ* is unknown. To address this issue,
    Andrews [1993] proposed a new test where all possible τ* are tried, within some
    interval τ* ∈ [τ [0] , 1 − τ [0] ]. As Breitung [2014] explains, we should leave
    out some of the possible τ* at the beginning and end of the sample, to ensure
    that either regime is fitted with enough observations (there must be enough zeros
    and enough ones in *D [*t*]* [τ*]). The test statistic for an unknown τ* is the
    maximum of all *T* (1 − 2τ [0] ) values of ![](Image00696.jpg) .
  prefs: []
  type: TYPE_NORMAL
- en: '![](Image00533.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Another drawback of Chow's approach is that it assumes that there is only one
    break date τ* *T* , and that the bubble runs up to the end of the sample (there
    is no switch back to a random walk). For situations where three or more regimes
    (random walk → bubble → random walk …) exist, we need to discuss the Supremum
    Augmented Dickey-Fuler (SADF) test.
  prefs: []
  type: TYPE_NORMAL
- en: '**17.4.2 Supremum Augmented Dickey-Fuller**'
  prefs: []
  type: TYPE_NORMAL
- en: In the words of Phillips, Wu and Yu [2011], “standard unit root and cointegration
    tests are inappropriate tools for detecting bubble behavior because they cannot
    effectively distinguish between a stationary process and a periodically collapsing
    bubble model. Patterns of periodically collapsing bubbles in the data look more
    like data generated from a unit root or stationary autoregression than a potentially
    explosive process.” To address this flaw, these authors propose fitting the regression
    specification
  prefs: []
  type: TYPE_NORMAL
- en: '![](Image00740.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'where we test for *H [0]* : β ≤ 0, *H [1]* : β > 0\. Inspired by Andrews [1993],
    Phillips and Yu [2011] and Phillips, Wu and Yu [2011] proposed the Supremum Augmented
    Dickey-Fuller test (SADF). SADF fits the above regression at each end point *t*
    with backwards expanding start points, then computes'
  prefs: []
  type: TYPE_NORMAL
- en: '![](Image00347.jpg)'
  prefs: []
  type: TYPE_IMG
- en: where ![](Image00513.jpg) is estimated on a sample that starts at *t [0] * and
    ends at *t* , τ is the minimum sample length used in the analysis, *t [0] * is
    the left bound of the backwards expanding window, and *t* = τ, …, *T* . For the
    estimation of *SADF [*t*] * , the right side of the window is fixed at *t* . The
    standard ADF test is a special case of *SADF [*t*] * , where τ = *t* − 1.
  prefs: []
  type: TYPE_NORMAL
- en: 'There are two critical differences between *SADF [*t*]* and SDFC: First, *SADF
    [*t*]* is computed at each *t* ∈ [τ, *T* ], whereas SDFC is computed only at *T*
    . Second, instead of introducing a dummy variable, SADF recursively expands the
    beginning of the sample ( *t [0]* ∈ [1, *t* − τ]). By trying all combinations
    of a nested double loop on ( *t [0]* , *t* ), SADF does not assume a known number
    of regime switches or break dates. [Figure 17.1](text00003.html#filepos0000869015)
    displays the series of E-mini S&P 500 futures prices after applying the ETF trick
    (Chapter 2, Section 2.4.1), as well as the SADF derived from that price series.
    The SADF line spikes when prices exhibit a bubble-like behavior, and returns to
    low levels when the bubble bursts. In the following sections, we will discuss
    some enhancements to Phillips’ original SADF method.'
  prefs: []
  type: TYPE_NORMAL
- en: '![](Image00374.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '[**Figure 17.1**](text00003.html#filepos0000868411) Prices (left y-axis) and
    SADF (right y-axis) over time'
  prefs: []
  type: TYPE_NORMAL
- en: '***17.4.2.1 Raw vs. Log Prices***'
  prefs: []
  type: TYPE_NORMAL
- en: It is common to find in the literature studies that carry out structural break
    tests on raw prices. In this section we will explore why log prices should be
    preferred, particularly when working with long time series involving bubbles and
    bursts.
  prefs: []
  type: TYPE_NORMAL
- en: For raw prices { *y [*t*]* }, if ADF's null hypotesis is rejected, it means
    that prices are stationary, with finite variance. The implication is that returns
    ![](Image00792.jpg) are not time invariant, for returns’ volatility must decrease
    as prices rise and increase as prices fall in order to keep the price variance
    constant. When we run ADF on raw prices, we assume that returns’ variance is not
    invariant to price levels. If returns variance happens to be invariant to price
    levels, the model will be structurally heteroscedastic.
  prefs: []
  type: TYPE_NORMAL
- en: In contrast, if we work with log prices, the ADF specification will state that
  prefs: []
  type: TYPE_NORMAL
- en: '![](Image00395.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Let us make a change of variable, *x [*t*]* = *ky [*t*]* . Now, log[ *x [*t*]*
    ] = log[ *k* ] + log[ *y [*t*]* ], and the ADF specification will state that
  prefs: []
  type: TYPE_NORMAL
- en: '![](Image00101.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Under this alternative specification based on log prices, price levels condition
    returns’ mean, not returns’ volatility. The difference may not matter in practice
    for small samples, where *k* ≈ 1, but SADF runs regressions across decades and
    bubbles produce levels that are significantly different between regimes ( *k*
    ≠ 1).
  prefs: []
  type: TYPE_NORMAL
- en: '***17.4.2.2 Computational Complexity***'
  prefs: []
  type: TYPE_NORMAL
- en: The algorithm runs in ![](Image00418.jpg) , as the number of ADF tests that
    SADF requires for a total sample length *T* is
  prefs: []
  type: TYPE_NORMAL
- en: '![](Image00020.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Consider a matrix representation of the ADF specification, where ![](Image00685.jpg)
    and ![](Image00772.jpg) . Solving a single ADF regression involves the floating
    point operations (FLOPs) listed in  [ Table 17.1 ](text00003.html#filepos0000872808)
    .
  prefs: []
  type: TYPE_NORMAL
- en: '[**Table 17.1**](text00003.html#filepos0000872700) **FLOPs per ADF Estimate**'
  prefs: []
  type: TYPE_NORMAL
- en: '| **Matrix Operation** | **FLOPs** |'
  prefs: []
  type: TYPE_TB
- en: '| *o [1]* = *X* ''*y* | (2*T* − 1)*N* |'
  prefs: []
  type: TYPE_TB
- en: '| *o [2]* = *X* ''*X* | (2*T* − 1)*N ²* |'
  prefs: []
  type: TYPE_TB
- en: '| *o [3]* = *o ^(− 1) [2]* | *N ³* + *N ²* + *N* |'
  prefs: []
  type: TYPE_TB
- en: '| *o [4]* = *o [3] o [1]* | 2*N ²* − *N* |'
  prefs: []
  type: TYPE_TB
- en: '| *o [5]* = *y* − *Xo [4]* | *T* + (2*N* − 1)*T* |'
  prefs: []
  type: TYPE_TB
- en: '| *o [6]* = *o ^'' [5] o [5]* | 2*T* − 1 |'
  prefs: []
  type: TYPE_TB
- en: '| ![](Image00009.jpg) | 2 + *N ²* |'
  prefs: []
  type: TYPE_TB
- en: '| ![](Image00103.jpg) | 1 |'
  prefs: []
  type: TYPE_TB
- en: This gives a total of *f* ( *N* , *T* ) = *N ³* + *N ²* (2 *T* + 3) + *N* (4
    *T* − 1) + 2 *T* + 2 FLOPs per ADF estimate. A single SADF update requires ![](Image00466.jpg)
    FLOPs ( *T* − τ operations to find the maximum ADF stat), and the estimation of
    a full SADF series requires ![](Image00284.jpg) .
  prefs: []
  type: TYPE_NORMAL
- en: Consider a dollar bar series on E-mini S&P 500 futures. For ( *T* , *N* ) =
    (356631, 3), an ADF estimate requires 11,412,245 FLOPs, and a SADF update requires
    2,034,979,648,799 operations (roughly 2.035 TFLOPs). A full SADF time series requires
    241,910,974,617,448,672 operations (roughly 242 PFLOPs). This number will increase
    quickly, as the *T* continues to grow. And this estimate excludes notoriously
    expensive operations like alignment, pre-processing of data, I/O jobs, etc. Needless
    to say, this algorithm's double loop requires a large number of operations. An
    HPC cluster running an efficiently parallelized implementation of the algorithm
    may be needed to estimate the SADF series within a reasonable amount of time.
    Chapter 20 will present some parallelization strategies useful in these situations.
  prefs: []
  type: TYPE_NORMAL
- en: '***17.4.2.3 Conditions for Exponential Behavior***'
  prefs: []
  type: TYPE_NORMAL
- en: 'Consider the zero-lag specification on log prices, Δlog[ *y [*t*]* ] = α +
    βlog[ *y [*t* − 1]* ] + ϵ [*t*] . This can be rewritten as ![](Image00485.jpg)
    , where ![](Image00450.jpg) . Rolling back *t* discrete steps, we obtain ![](Image00501.jpg)
    , or ![](Image00580.jpg) . The index *t* can be reset at a given time, to project
    the future trajectory of *y [0] * → *y [*t*] * after the next *t* steps. This
    reveals the conditions that characterize the three states for this dynamic system:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Steady: ![](Image00705.jpg) .'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The disequilibrium is ![](Image00797.jpg) .
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Then ![](Image00032.jpg) at ![](Image00124.jpg) (half-life).
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Unit-root: β = 0, where the system is non-stationary, and behaves as a martingale.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Explosive: β > 0, where ![](Image00215.jpg) .'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '***17.4.2.4 Quantile ADF***'
  prefs: []
  type: TYPE_NORMAL
- en: 'SADF takes the supremum of a series on t-values, ![](Image00308.jpg) . Selecting
    the extreme value introduces some robustness problems, where SADF estimates could
    vary significantly depending on the sampling frequency and the specific timestamps
    of the samples. A more robust estimator of ADF extrema would be the following:
    First, let ![](Image00446.jpg) . Second, we define *Q [*t*  ,  *q*] * = *Q* [
    *s [*t*] * , *q* ] the *q* quantile of *s [*t*] * , as a measure of centrality
    of high ADF values, where *q* ∈ [0, 1]. Third, we define ![](Image00455.jpg) ,
    with 0 < *v* ≤ min{ *q* , 1 − *q* }, as a measure of dispersion of high ADF values.
    For example, we could set *q* = 0.95 and *v* = 0.025\. Note that SADF is merely
    a particular case of QADF, where *SADF [*t*] * = *Q [*t*  , 1] * and ![](Image00471.jpg)
    is not defined because *q* = 1.'
  prefs: []
  type: TYPE_NORMAL
- en: '***17.4.2.5 Conditional ADF***'
  prefs: []
  type: TYPE_NORMAL
- en: Alternatively, we can address concerns on SADF robustness by computing conditional
    moments. Let *f* [ *x* ] be the probability distribution function of ![](Image00828.jpg)
    , with *x* ∈ *s [*t*] * . Then, we define *C [*t*  ,  *q*] * = *K ^(− 1) * ∫ ^∞
    [   *Qt*  ,  *q*   ]  *xf* [ *x* ] *dx* as a measure of centrality of high ADF
    values, and ![](Image00495.jpg) as a measure of dispersion of high ADF values,
    with regularization constant ![](Image00511.jpg) . For example, we could use *q*
    = 0.95.
  prefs: []
  type: TYPE_NORMAL
- en: By construction, *C [*t* , *q*]* ≤ *SADF [*t*]* . A scatter plot of *SADF [*t*]*
    against *C [*t* , *q*]* shows that lower boundary, as an ascending line with approximately
    unit gradient (see [Figure 17.2](text00003.html#filepos0000885567) ). When SADF
    grows beyond −1.5, we can appreciate some horizontal trajectories, consistent
    with a sudden widening of the right fat tail in *s [*t*]* . In other words, ![](Image00528.jpg)
    can reach significantly large values even if *C [*t*  ,  *q*] * is relatively
    small, because *SADF [*t*] * is sensitive to outliers.
  prefs: []
  type: TYPE_NORMAL
- en: '![](Image00550.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '[**Figure 17.2**](text00003.html#filepos0000884641) SADF (x-axis) vs CADF (y-axis)'
  prefs: []
  type: TYPE_NORMAL
- en: '[Figure 17.3](text00003.html#filepos0000886781) (a) plots ![](Image00671.jpg)
    for the E-mini S&P 500 futures prices over time. [ Figure 17.3 ](text00003.html#filepos0000886781)
    (b) is the scatter-plot of ![](Image00490.jpg) against *SADF [*t*] * , computed
    on the E-mini S&P 500 futures prices. It shows evidence that outliers in *s [*t*]
    * bias *SADF [*t*] * upwards.'
  prefs: []
  type: TYPE_NORMAL
- en: '![](Image00617.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '[**Figure 17.3**](text00003.html#filepos0000885716) (a) ![](Image00638.jpg)
    over time (b) ![](Image00654.jpg) (y-axis) as a function of *SADF [*t*] * (x-axis)'
  prefs: []
  type: TYPE_NORMAL
- en: '***17.4.2.6 Implementation of SADF***'
  prefs: []
  type: TYPE_NORMAL
- en: 'This section presents an implementation of the SADF algorithm. The purpose
    of this code is not to estimate SADF quickly, but to clarify the steps involved
    in its estimation. Snippet 17.1 lists SADF''s inner loop. That is the part that
    estimates ![](Image00668.jpg) , which is the backshifting component of the algorithm.
    The outer loop (not shown here) repeats this calculation for an advancing *t*
    , { *SADF [*t*] * } [*t*  = 1, …,  *T*] . The arguments are:'
  prefs: []
  type: TYPE_NORMAL
- en: '`logP` : a pandas series containing log-prices'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`minSL` : the minimum sample length (τ), used by the final regression'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`constant` : the regression''s time trend component'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`''nc''` : no time trend, only a constant'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`''ct''` : a constant plus a linear time trend'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`''ctt''` : a constant plus a second-degree polynomial time trend'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`lags` : the number of lags used in the ADF specification'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**SNIPPET 17.1 SADF''S INNER LOOP**'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '![](Image00693.jpg)'
  prefs:
  - PREF_BQ
  type: TYPE_IMG
- en: Snippet 17.2 lists function `getXY` , which prepares the numpy objects needed
    to conduct the recursive tests.
  prefs: []
  type: TYPE_NORMAL
- en: '**SNIPPET 17.2 PREPARING THE DATASETS**'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '![](Image00695.jpg)'
  prefs:
  - PREF_BQ
  type: TYPE_IMG
- en: Snippet 17.3 lists function `lagDF` , which applies to a dataframe the lags
    specified in its argument `lags` .
  prefs: []
  type: TYPE_NORMAL
- en: '**SNIPPET 17.3 APPLY LAGS TO DATAFRAME**'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '![](Image00711.jpg)'
  prefs:
  - PREF_BQ
  type: TYPE_IMG
- en: Finally, Snippet 17.4 lists function `getBetas` , which carries out the actual
    regressions.
  prefs: []
  type: TYPE_NORMAL
- en: '**SNIPPET 17.4 FITTING THE ADF SPECIFICATION**'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '![](Image00728.jpg)'
  prefs:
  - PREF_BQ
  type: TYPE_IMG
- en: '**17.4.3 Sub- and Super-Martingale Tests**'
  prefs: []
  type: TYPE_NORMAL
- en: 'In this section we will introduce explosiveness tests that do not rely on the
    standard ADF specification. Consider a process that is either a sub- or super-martingale.
    Given some observations { *y [*t*]* }, we would like to test for the existence
    of an explosive time trend, *H [0]* : β = 0, *H [1]* : β ≠ 0, under alternative
    specifications:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Polynomial trend (SM-Poly1):'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](Image00746.jpg)'
  prefs:
  - PREF_IND
  - PREF_BQ
  - PREF_BQ
  type: TYPE_IMG
- en: 'Polynomial trend (SM-Poly2):'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](Image00509.jpg)'
  prefs:
  - PREF_IND
  - PREF_BQ
  - PREF_BQ
  type: TYPE_IMG
- en: 'Exponential trend (SM-Exp):'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](Image00756.jpg)'
  prefs:
  - PREF_IND
  - PREF_BQ
  - PREF_BQ
  type: TYPE_IMG
- en: 'Power trend (SM-Power):'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](Image00804.jpg)'
  prefs:
  - PREF_IND
  - PREF_BQ
  - PREF_BQ
  type: TYPE_IMG
- en: Similar to SADF, we fit any of these specifications to each end point *t* =
    τ, …, *T* , with backwards expanding start points, then compute
  prefs: []
  type: TYPE_NORMAL
- en: '![](Image00820.jpg)'
  prefs: []
  type: TYPE_IMG
- en: The reason for the absolute value is that we are equally interested in explosive
    growth and collapse. In the simple regression case (Greene [2008], p. 48), the
    variance of β is ![](Image00840.jpg) , hence ![](Image00002.jpg) . The same result
    is generalizable to the multivariate linear regression case (Greene [2008], pp.
    51–52). The ![](Image00022.jpg) of a weak long-run bubble may be smaller than
    the ![](Image00296.jpg) of a strong short-run bubble, hence biasing the method
    towards long-run bubbles. To correct for this bias, we can penalize large sample
    lengths by determining the coefficient φ ∈ [0, 1] that yields best explosiveness
    signals.
  prefs: []
  type: TYPE_NORMAL
- en: '![](Image00059.jpg)'
  prefs: []
  type: TYPE_IMG
- en: For instance, when φ = 0.5, we compensate for the lower ![](Image00076.jpg)
    associated with longer sample lengths, in the simple regression case. For φ →
    0, *SMT [*t*] * will exhibit longer trends, as that compensation wanes and long-run
    bubbles mask short-run bubbles. For φ → 1, *SMT [*t*] * becomes noisier, because
    more short-run bubbles are selected over long-run bubbles. Consequently, this
    is a natural way to adjust the explosiveness signal, so that it filters opportunities
    targeting a particular holding period. The features used by the ML algorithm may
    include *SMT [*t*] * estimated from a wide range of φ values.
  prefs: []
  type: TYPE_NORMAL
- en: '**Exercises**'
  prefs: []
  type: TYPE_NORMAL
- en: On a dollar bar series on E-mini S&P 500 futures,
  prefs:
  - PREF_OL
  - PREF_BQ
  - PREF_BQ
  type: TYPE_NORMAL
- en: Apply the Brown-Durbin-Evans method. Does it recognize the dot-com bubble?
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Apply the Chu-Stinchcombe-White method. Does it find a bubble in 2007–2008?
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: On a dollar bar series on E-mini S&P 500 futures,
  prefs:
  - PREF_OL
  - PREF_BQ
  - PREF_BQ
  type: TYPE_NORMAL
- en: Compute the *SDFC* (Chow-type) explosiveness test. What break date does this
    method select? Is this what you expected?
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Compute and plot the SADF values for this series. Do you observe extreme spikes
    around the dot-com bubble and before the Great Recession? Did the bursts also
    cause spikes?
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Following on exercise 2,
  prefs:
  - PREF_OL
  - PREF_BQ
  - PREF_BQ
  type: TYPE_NORMAL
- en: Determine the periods where the series exhibited
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Steady conditions
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Unit-Root conditions
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Explosive conditions
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Compute QADF.
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Compute CADF.
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: On a dollar bar series on E-mini S&P 500 futures,
  prefs:
  - PREF_OL
  - PREF_BQ
  - PREF_BQ
  type: TYPE_NORMAL
- en: Compute SMT for SM-Poly1 and SM-Poly 2, where φ = 1\. What is their correlation?
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Compute SMT for SM-Exp, where φ = 1 and φ = 0.5\. What is their correlation?
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Compute SMT for SM-Power, where φ = 1 and φ = 0.5\. What is their correlation?
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: If you compute the reciprocal of each price, the series { *y ^(− 1) [*t*]* }
    turns bubbles into bursts and bursts into bubbles.
  prefs:
  - PREF_OL
  - PREF_BQ
  - PREF_BQ
  type: TYPE_NORMAL
- en: Is this transformation needed, to identify bursts?
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: What methods in this chapter can identify bursts without requiring this transformation?
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '**References**'
  prefs: []
  type: TYPE_NORMAL
- en: 'Andrews, D. (1993): “Tests for parameter instability and structural change
    with unknown change point.” *Econometrics* , Vol. 61, No. 4 (July), pp. 821–856.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Breitung, J. and R. Kruse (2013): “When Bubbles Burst: Econometric Tests Based
    on Structural Breaks.” *Statistical Papers* , Vol. 54, pp. 911–930.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Breitung, J. (2014): “Econometric tests for speculative bubbles.” *Bonn Journal
    of Economics* , Vol. 3, No. 1, pp. 113–127.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Brown, R.L., J. Durbin, and J.M. Evans (1975): “Techniques for Testing the
    Constancy of Regression Relationships over Time.” *Journal of the Royal Statistical
    Society, Series B* , Vol. 35, pp. 149–192.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Chow, G. (1960). “Tests of equality between sets of coefficients in two linear
    regressions.” *Econometrica* , Vol. 28, No. 3, pp. 591–605.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Greene, W. (2008): *Econometric Analysis* , 6th ed. Pearson Prentice Hall.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Homm, U. and J. Breitung (2012): “Testing for speculative bubbles in stock
    markets: A comparison of alternative methods.” *Journal of Financial Econometrics*
    , Vol. 10, No. 1, 198–231.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Maddala, G. and I. Kim (1998): *Unit Roots, Cointegration and Structural Change*
    , 1st ed. Cambridge University Press.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Phillips, P., Y. Wu, and J. Yu (2011): “Explosive behavior in the 1990s Nasdaq:
    When did exuberance escalate asset values?” *International Economic Review* ,
    Vol. 52, pp. 201–226.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Phillips, P. and J. Yu (2011): “Dating the timeline of financial bubbles during
    the subprime crisis.” *Quantitative Economics* , Vol. 2, pp. 455–491.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Phillips, P., S. Shi, and J. Yu (2013): “Testing for multiple bubbles 1: Historical
    episodes of exuberance and collapse in the S&P 500.” Working paper 8–2013, Singapore
    Management University.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**CHAPTER 18**'
  prefs: []
  type: TYPE_NORMAL
- en: '**Entropy Features**'
  prefs: []
  type: TYPE_NORMAL
- en: '**18.1 Motivation**'
  prefs: []
  type: TYPE_NORMAL
- en: Price series convey information about demand and supply forces. In perfect markets,
    prices are unpredictable, because each observation transmits everything that is
    known about a product or service. When markets are not perfect, prices are formed
    with partial information, and as some agents know more than others, they can exploit
    that informational asymmetry. It would be helpful to estimate the informational
    content of price series, and form features on which ML algorithms can learn the
    likely outcomes. For example, the ML algorithm may find that momentum bets are
    more profitable when prices carry little information, and that mean-reversion
    bets are more profitable when prices carry a lot of information. In this chapter,
    we will explore ways to determine the amount of information contained in a price
    series.
  prefs: []
  type: TYPE_NORMAL
- en: '**18.2 Shannon''s Entropy**'
  prefs: []
  type: TYPE_NORMAL
- en: In this section we will review a few concepts from information theory that will
    be useful in the remainder of the chapter. The reader can find a complete exposition
    in MacKay [2003]. The father of information theory, Claude Shannon, defined entropy
    as the average amount of information (over long messages) produced by a stationary
    source of data. It is the smallest number of bits per character required to describe
    the message in a uniquely decodable way. Mathematically, Shannon [1948] defined
    the entropy of a discrete random variable *X* with possible values *x* ∈ *A* as
  prefs: []
  type: TYPE_NORMAL
- en: '![](Image00095.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'with 0 ≤ *H* [ *X* ] ≤ log [2] [|| *A* ||] where: *p* [ *x* ] is the probability
    of *x* ; *H* [ *X* ] = 0⇔∃ *x* | *p* [ *x* ] = 1; ![](Image00641.jpg) for all
    *x* ; and || *A* || is the size of the set *A* . This can be interpreted as the
    probability weighted average of informational content in *X* , where the bits
    of information are measured as ![](Image00715.jpg) . The rationale for measuring
    information as ![](Image00151.jpg) comes from the observation that low-probability
    outcomes reveal more information than high-probability outcomes. In other words,
    we learn when something unexpected happens. Similarly, redundancy is defined as'
  prefs: []
  type: TYPE_NORMAL
- en: '![](Image00045.jpg)'
  prefs: []
  type: TYPE_IMG
- en: with 0 ≤ *R* [ *X* ] ≤ 1\. Kolmogorov [1965] formalized the connection between
    redundancy and complexity of a Markov information source. The mutual information
    between two variables is defined as the Kullback-Leibler divergence from the joint
    probability density to the product of the marginal probability densities.
  prefs: []
  type: TYPE_NORMAL
- en: '![](Image00184.jpg)'
  prefs: []
  type: TYPE_IMG
- en: The mutual information (MI) is always non-negative, symmetric, and equals zero
    if and only if *X* and *Y* are independent. For normally distributed variables,
    the mutual information is closely related to the familiar Pearson correlation,
    ρ.
  prefs: []
  type: TYPE_NORMAL
- en: '![](Image00494.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Therefore, mutual information is a natural measure of the association between
    variables, regardless of whether they are linear or nonlinear in nature (Hausser
    and Strimmer [2009]). The normalized variation of information is a metric derived
    from mutual information. For several entropy estimators, see:'
  prefs: []
  type: TYPE_NORMAL
- en: 'In R: [http://cran.r-project.org/web/packages/entropy/entropy.pdf](http://cran.r-project.org/web/packages/entropy/entropy.pdf)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'In Python: [https://code.google.com/archive/p/pyentropy/](https://code.google.com/archive/p/pyentropy/)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**18.3 The Plug-in (or Maximum Likelihood) Estimator**'
  prefs: []
  type: TYPE_NORMAL
- en: In this section we will follow the exposition of entropy's maximum likelihood
    estimator in Gao et al. [2008]. The nomenclature may seem a bit peculiar at first
    (no pun intended), but once you become familiar with it you will find it convenient.
    Given a data sequence *x ^(*n*) [1]* , comprising the string of values starting
    in position 1 and ending in position *n* , we can form a dictionary of all words
    of length *w* < *n* in that sequence, *A ^(*w*)* . Consider an arbitrary word
    *y ^(*w*) [1]* ∈ *A ^(*w*)* of length *w* . We denote ![](Image00591.jpg) the
    empirical probability of the word *y ^(*w*) [  1  ] * in *x ^(*n*) [  1  ] * ,
    which means that ![](Image00678.jpg) is the frequency with which *y ^(*w*) [  1  ]
    * appears in *x ^(*n*) [  1  ] * . Assuming that the data is generated by a stationary
    and ergodic process, then the law of large numbers guarantees that, for a fixed
    *w* and large *n* , the empirical distribution ![](Image00760.jpg) will be close
    to the true distribution *p [*w*] * . Under these circumstances, a natural estimator
    for the entropy rate (i.e., average entropy per bit) is
  prefs: []
  type: TYPE_NORMAL
- en: '![](Image00292.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Since the empirical distribution is also the maximum likelihood estimate of
    the true distribution, this is also often referred to as the maximum likelihood
    entropy estimator. The value *w* should be large enough for ![](Image00093.jpg)
    to be acceptably close to the true entropy *H.* The value of *n* needs to be much
    larger than *w* , so that the empirical distribution of order *w* is close to
    the true distribution. Snippet 18.1 implements the plug-in entropy estimator.
  prefs: []
  type: TYPE_NORMAL
- en: '**SNIPPET 18.1 PLUG-IN ENTROPY ESTIMATOR**'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '![](Image00181.jpg)'
  prefs:
  - PREF_BQ
  type: TYPE_IMG
- en: '**18.4 Lempel-Ziv Estimators**'
  prefs: []
  type: TYPE_NORMAL
- en: Entropy can be interpreted as a measure of complexity. A complex sequence contains
    more information than a regular (predictable) sequence. The Lempel-Ziv (LZ) algorithm
    efficiently decomposes a message into non-redundant substrings (Ziv and Lempel
    [1978]). We can estimate the compression rate of a message as a function of the
    number of items in a Lempel-Ziv dictionary relative to the length of the message.
    The intuition here is that complex messages have high entropy, which will require
    large dictionaries relative to the length of the string to be transmitted. Snippet
    18.2 shows an implementation of the LZ compression algorithm.
  prefs: []
  type: TYPE_NORMAL
- en: '**SNIPPET 18.2 A LIBRARY BUILT USING THE LZ ALGORITHM**'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '![](Image00734.jpg)'
  prefs:
  - PREF_BQ
  type: TYPE_IMG
- en: Kontoyiannis [1998] attempts to make a more efficient use of the information
    available in a message. What follows is a faithful summary of the exposition in
    Gao et al. [2008]. We will reproduce the steps in that paper, while complementing
    them with code snippets that implement their ideas. Let us define *L ^(*n*) [*i*]*
    as 1 plus the length of the longest match found in the *n* bits prior to *i* ,
  prefs: []
  type: TYPE_NORMAL
- en: '![](Image00371.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Snippet 18.3 implements the algorithm that determines the length of the longest
    match. A few notes worth mentioning:'
  prefs: []
  type: TYPE_NORMAL
- en: The value *n* is constant for a sliding window, and *n* = *i* for an expanding
    window.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Computing *L ^(*n*) [*i*]* requires data *x ^(*i* + *n* − 1) [*i* − *n*]* .
    In other words, index *i* must be at the center of the window. This is important
    in order to guarantee that both matching strings are of the same length. If they
    are not of the same length, *l* will have a limited range and its maximum will
    be underestimated.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Some overlap between the two substrings is allowed, although obviously both
    cannot start at *i.*
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**SNIPPET 18.3 FUNCTION THAT COMPUTES THE LENGTH OF THE LONGEST MATCH**'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '![](Image00447.jpg)'
  prefs:
  - PREF_BQ
  type: TYPE_IMG
- en: Ornstein and Weiss [1993] formally established that
  prefs: []
  type: TYPE_NORMAL
- en: '![](Image00514.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Kontoyiannis uses this result to estimate Shannon's entropy rate. He estimates
    the average ![](Image00620.jpg) , and uses the reciprocal of that average to estimate
    *H.* The general intuition is, as we increase the available history, we expect
    that messages with high entropy will produce relatively shorter non-redundant
    substrings. In contrast, messages with low entropy will produce relatively longer
    non-redundant substrings as we parse through the message. Given a data realization
    *x ^∞ [  − ∞  ] * , a window length *n* ≥ 1, and a number of matches *k* ≥ 1,
    the sliding-window LZ estimator ![](Image00388.jpg) is defined by
  prefs: []
  type: TYPE_NORMAL
- en: '![](Image00787.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Similarly, the increasing window LZ estimator ![](Image00024.jpg) , is defined
    by
  prefs: []
  type: TYPE_NORMAL
- en: '![](Image00831.jpg)'
  prefs: []
  type: TYPE_IMG
- en: The window size *n* is constant when computing ![](Image00456.jpg) , thus *L
    ^(*n*) [   *i*   ] * . However, when computing ![](Image00853.jpg) , the window
    size increases with *i* , thus *L ^(*i*) [   *i*   ] * , with ![](Image00445.jpg)
    . In this expanding window case the length of the message *N* should be an even
    number to ensure that all bits are parsed (recall that *x [*i*] * is at the center,
    so for an odd-length message the last bit would not be read).
  prefs: []
  type: TYPE_NORMAL
- en: 'The above expressions have been derived under the assumptions of: stationarity,
    ergodicity, that the process takes finitely many values, and that the process
    satisfies the Doeblin condition. Intuitively, this condition requires that, after
    a finite number of steps *r* , no matter what has occurred before, anything can
    happen with positive probability. It turns out that this Doeblin condition can
    be avoided altogether if we consider a modified version of the above estimators:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](Image00457.jpg)'
  prefs: []
  type: TYPE_IMG
- en: One practical question when estimating ![](Image00537.jpg) is how to determine
    the window size *n.* Gao et al. [2008] argue that *k* + *n* = *N* should be approximately
    equal to the message length. Considering that the bias of *L ^(*n*) [   *i*   ]
    * is of order ![](Image00645.jpg) and the variance of *L ^(*n*) [   *i*   ] *
    is order ![](Image00718.jpg) , the bias/variance trade-off is balanced at around
    ![](Image00069.jpg) . That is, *n* could be chosen such that *N* ≈ *n* + (log 
    [2] [ *n* ]) ² . For example, for *N* = 2 ⁸ , a balanced bias/variance window
    size would be *n* ≈ 198, in which case *k* ≈ 58.
  prefs: []
  type: TYPE_NORMAL
- en: Kontoyiannis [1998] proved that ![](Image00047.jpg) converges to Shannon's entropy
    rate with probability 1 as *n* approaches infinity. Snippet 18.4 implements the
    ideas discussed in Gao et al. [2008], which improve on Kontoyiannis [1997] by
    looking for the maximum redundancy between two substrings of the same size.
  prefs: []
  type: TYPE_NORMAL
- en: '**SNIPPET 18.4 IMPLEMENTATION OF ALGORITHMS DISCUSSED IN GAO ET AL. [2008]**'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '![](Image00525.jpg)'
  prefs:
  - PREF_BQ
  type: TYPE_IMG
- en: One caveat of this method is that entropy rate is defined in the limit. In the
    words of Kontoyiannis, “we fix a large integer *N* as the size of our database.”
    The theorems used by Kontoyiannis’ paper prove asymptotic convergence; however,
    nowhere is a monotonicity property claimed. When a message is short, a solution
    may be to repeat the same message multiple times.
  prefs: []
  type: TYPE_NORMAL
- en: A second caveat is that, because the window for matching must be symmetric (same
    length for the dictionary as for the substring being matched), the last bit is
    only considered for matching if the message's length corresponds to an even number.
    One solution is to remove the first bit of a message with odd length.
  prefs: []
  type: TYPE_NORMAL
- en: A third caveat is that some final bits will be dismissed when preceded by irregular
    sequences. This is also a consequence of the symmetric matching window. For example,
    the entropy rate for “10000111” equals the entropy rate for “10000110,” meaning
    that the final bit is irrelevant due to the unmatchable “11” in the sixth and
    seventh bit. When the end of the message is particularly relevant, a good solution
    may be to analyze the entropy of the reversed message. This not only ensures that
    the final bits (i.e., the initial ones after the reversing) are used, but actually
    they will be used to potentially match every bit. Following the previous example,
    the entropy rate of “11100001” is 0.96, while the entropy rate for “01100001’
    is 0.84.
  prefs: []
  type: TYPE_NORMAL
- en: '**18.5 Encoding Schemes**'
  prefs: []
  type: TYPE_NORMAL
- en: Estimating entropy requires the encoding of a message. In this section we will
    review a few encoding schemes used in the literature, which are based on returns.
    Although not discussed in what follows, it is advisable to encode information
    from fractionally (rather than integer) differentiated series (Chapter 4), as
    they still contain some memory.
  prefs: []
  type: TYPE_NORMAL
- en: '**18.5.1 Binary Encoding**'
  prefs: []
  type: TYPE_NORMAL
- en: Entropy rate estimation requires the discretization of a continuous variable,
    so that each value can be assigned a code from a finite alphabet. For example,
    a stream of returns *r [*t*]* can be encoded according to the sign, 1 for *r [*t*]*
    > 0, 0 for *r [*t*]* < 0, removing cases where *r [*t*]* = 0\. Binary encoding
    arises naturally in the case of returns series sampled from price bars (i.e.,
    bars that contain prices fluctuating between two symmetric horizontal barriers,
    centered around the start price), because | *r [*t*]* | is approximately constant.
  prefs: []
  type: TYPE_NORMAL
- en: When | *r [*t*]* | can adopt a wide range of outcomes, binary encoding discards
    potentially useful information. That is particularly the case when working with
    intraday time bars, which are affected by the heteroscedasticity that results
    from the inhomogeneous nature of tick data. One way to partially address this
    heteroscedasticity is to sample prices according to a subordinated stochastic
    process. Examples of that are trade bars and volume bars, which contain a fixed
    number of trades or trades for a fixed amount of volume (see Chapter 2). By operating
    in this non-chronological, market-driven clock, we sample more frequently during
    highly active periods, and less frequently during periods of less activity, hence
    regularizing the distribution of | *r [*t*]* | and reducing the need for a large
    alphabet.
  prefs: []
  type: TYPE_NORMAL
- en: '**18.5.2 Quantile Encoding**'
  prefs: []
  type: TYPE_NORMAL
- en: Unless price bars are used, it is likely that more than two codes will be needed.
    One approach consists in assigning a code to each *r [*t*]* according to the quantile
    it belongs to. The quantile boundaries are determined using an in-sample period
    (training set). There will be the same number of observations assigned to each
    letter for the overall in-sample, and close to the same number of observations
    per letter out-of-sample. When using the method, some codes span a greater fraction
    of *r [*t*]* ’s range than others. This uniform (in-sample) or close to uniform
    (out-of-sample) distribution of codes tends to increase entropy readings on average.
  prefs: []
  type: TYPE_NORMAL
- en: '**18.5.3 Sigma Encoding**'
  prefs: []
  type: TYPE_NORMAL
- en: As an alternative approach, rather than fixing the number of codes, we could
    let the price stream determine the actual dictionary. Suppose we fix a discretization
    step, σ. Then, we assign the value 0 to *r [*t*]* ∈ [.min{ *r* }, min{ *r* } +
    σ)., 1 to *r [*t*]* ∈ [.min{ *r* } + σ, min{ *r* } + 2σ). and so on until every
    observation has been encoded with a total of ![](Image00234.jpg) codes, where
    ceil[.] is the ceiling function. Unlike quantile encoding, now each code covers
    the same fraction of *r [*t*] * ’s range. Because codes are not uniformly distributed,
    entropy readings will tend to be smaller than in quantile encoding on average;
    however, the appearance of a “rare” code will cause spikes in entropy readings.
  prefs: []
  type: TYPE_NORMAL
- en: '**18.6 Entropy of a Gaussian Process**'
  prefs: []
  type: TYPE_NORMAL
- en: The entropy of an IID Normal random process (see Norwich [2003]) can be derived as
  prefs: []
  type: TYPE_NORMAL
- en: '![](Image00324.jpg)'
  prefs: []
  type: TYPE_IMG
- en: For the standard Normal, *H* ≈ 1.42\. There are at least two uses of this result.
    First, it allows us to benchmark the performance of an entropy estimator. We can
    draw samples from a standard normal distribution, and find what combination of
    estimator, message length, and encoding gives us an entropy estimate ![](Image00328.jpg)
    sufficiently close to the theoretically derived value *H.* For example,  [ Figure
    18.1 ](text00003.html#filepos0000929074) plots the bootstrapped distributions
    of entropy estimates under 10, 7, 5, and 2 letter encodings, on messages of length
    100, using Kontoyiannis’ method. For alphabets of at least 10 letters, the algorithm
    in Snippet 18.4 delivers the correct answer. When alphabets are too small, information
    is discarded and entropy is underestimated.
  prefs: []
  type: TYPE_NORMAL
- en: '![](Image00478.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '![](Image00566.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '[**Figure 18.1**](text00003.html#filepos0000928394) Distribution of entropy
    estimates under 10 (top), 7 (bottom), letter encodings, on messages of length
    100'
  prefs: []
  type: TYPE_NORMAL
- en: Distribution of entropy estimates under 5 (top), and 2 (bottom) letter encodings,
    on messages of length 100
  prefs: []
  type: TYPE_NORMAL
- en: Second, we can use the above equation to connect entropy with volatility, by
    noting that ![](Image00664.jpg) . This gives us an entropy-implied volatility
    estimate, provided that returns are indeed drawn from a Normal distribution.
  prefs: []
  type: TYPE_NORMAL
- en: '**18.7 Entropy and the Generalized Mean**'
  prefs: []
  type: TYPE_NORMAL
- en: Here is an interesting way of thinking about entropy. Consider a set of real
    numbers *x* = { *x [*i*]* } [*i* = 1, …, *n*] and weights *p* = { *p [*i*]* }
    [*i* = 1, …, *n*] , such that 0 ≤ *p [*i*]* ≤ 1, ∀ *i* and ![](Image00739.jpg)
    . The generalized weighted mean of *x* with weights *p* on a power *q* ≠ 0 is
    defined as
  prefs: []
  type: TYPE_NORMAL
- en: '![](Image00835.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'For *q* < 0, we must require that *x [*i*]* > 0, ∀ *i* . The reason this is
    a generalized mean is that other means can be obtained as special cases:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Minimum: ![](Image00072.jpg)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Harmonic mean: ![](Image00162.jpg)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Geometric mean: ![](Image00257.jpg)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Arithmetic mean: ![](Image00627.jpg)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Weighted mean: ![](Image00701.jpg)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Quadratic mean: ![](Image00793.jpg)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Maximum: ![](Image00029.jpg)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In the context of information theory, an interesting special case is *x* = {
    *p [*i*]* } [*i* = 1, …, *n*] , hence
  prefs: []
  type: TYPE_NORMAL
- en: '![](Image00121.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Let us define the quantity ![](Image00211.jpg) , for some *q* ≠ 1\. Again, for
    *q* < 1 in *N [*q*] * [ *p* ], we must have *p [*i*] * > 0, ∀ *i* . If ![](Image00305.jpg)
    for *k* ∈ [1, *n* ] different indices and *p [*i*] * = 0 elsewhere, then the weight
    is spread evenly across *k* different items, and *N [*q*] * [ *p* ] = *k* for
    *q* > 1\. In other words, *N [*q*] * [ *p* ] gives us the *effective number* or
    *diversity* of items in *p* , according to some weighting scheme set by *q* .
  prefs: []
  type: TYPE_NORMAL
- en: Using Jensen's inequality, we can prove that ![](Image00397.jpg) , hence ![](Image00462.jpg)
    . Smaller values of *q* assign a more uniform weight to elements of the partition,
    giving relatively more weight to less common elements, and ![](Image00476.jpg)
    is simply the total number of nonzero *p [*i*] * .
  prefs: []
  type: TYPE_NORMAL
- en: Shannon's entropy is ![](Image00649.jpg) . This shows that entropy can be interpreted
    as the logarithm of the *effective number* of items in a list *p* , where *q*
    → 1.  [ Figure 18.2 ](text00003.html#filepos0000936629) illustrates how the log
    effective numbers for a family of randomly generated *p* arrays converge to Shannon's
    entropy as *q* approaches 1\. Notice, as well, how their behavior stabilizes as
    *q* grows large.
  prefs: []
  type: TYPE_NORMAL
- en: '![](Image00722.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '[**Figure 18.2**](text00003.html#filepos0000936083) Log effective numbers for
    a family of randomly generated *p* arrays'
  prefs: []
  type: TYPE_NORMAL
- en: Intuitively, entropy measures information as the level of *diversity* contained
    in a random variable. This intuition is formalized through the notion of generalized
    mean. The implication is that Shannon's entropy is a special case of a diversity
    measure (hence its connection with volatility). We can now define and compute
    alternative measures of diversity, other than entropy, where *q* ≠ 1.
  prefs: []
  type: TYPE_NORMAL
- en: '**18.8 A Few Financial Applications of Entropy**'
  prefs: []
  type: TYPE_NORMAL
- en: In this section we will introduce a few applications of entropy to the modelling
    of financial markets.
  prefs: []
  type: TYPE_NORMAL
- en: '**18.8.1 Market Efficiency**'
  prefs: []
  type: TYPE_NORMAL
- en: When arbitrage mechanisms exploit the complete set of opportunities, prices
    instantaneously reflect the full amount of available information, becoming unpredictable
    (i.e., a martingale), with no discernable patterns. Conversely, when arbitrage
    is not perfect, prices contain incomplete amounts of information, which gives
    rise to predictable patterns. Patterns occur when a string contains redundant
    information, which enables its compression. The entropy rate of a string determines
    its optimal compression rate. The higher the entropy, the lower the redundancy
    and the greater the informational content. Consequently, the entropy of a price
    string tells us the degree of market efficiency at a given point in time. A “decompressed”
    market is an efficient market, because price information is non-redundant. A “compressed”
    market is an inefficient market, because price information is redundant. Bubbles
    are formed in compressed (low entropy) markets.
  prefs: []
  type: TYPE_NORMAL
- en: '**18.8.2 Maximum Entropy Generation**'
  prefs: []
  type: TYPE_NORMAL
- en: In a series of papers, Fiedor [2014a, 2014b, 2014c] proposes to use Kontoyiannis
    [1997] to estimate the amount of entropy present in a price series. He argues
    that, out of the possible future outcomes, the one that maximizes entropy may
    be the most profitable, because it is the one that is least predictable by frequentist
    statistical models. It is the black swan scenario most likely to trigger stop
    losses, thus generating a feedback mechanism that will reinforce and exacerbate
    the move, resulting in runs in the signs of the returns time series.
  prefs: []
  type: TYPE_NORMAL
- en: '**18.8.3 Portfolio Concentration**'
  prefs: []
  type: TYPE_NORMAL
- en: Consider an *NxN* covariance matrix *V* , computed on returns. First, we compute
    an eigenvalue decomposition of the matrix, *VW* = *W* Λ. Second, we obtain the
    factor loadings vector as *f [ω]* = *W* 'ω, where ω is the vector of allocations,
    ![](Image00813.jpg) . ^([  1  ](text00003.html#filepos0000963253)) Third, we derive
    the portion of risk contributed by each principal component (Bailey and López
    de Prado [2012]) as
  prefs: []
  type: TYPE_NORMAL
- en: '![](Image00512.jpg)'
  prefs: []
  type: TYPE_IMG
- en: where ![](Image00144.jpg) , and θ [*i*] ∈ [0, 1], ∀ *i* = 1, …, *N* . Fourth,
    Meucci [2009] proposed the following entropy-inspired definition of portfolio
    concentration,
  prefs: []
  type: TYPE_NORMAL
- en: '![](Image00238.jpg)'
  prefs: []
  type: TYPE_IMG
- en: At first, this definition of portfolio concentration may sound striking, because
    θ [*i*] is not a probability. The connection between this notion of concentration
    and entropy is due to the generalized mean, which we discussed in Chapter 18,
    Section 18.7.
  prefs: []
  type: TYPE_NORMAL
- en: '**18.8.4 Market Microstructure**'
  prefs: []
  type: TYPE_NORMAL
- en: Easley et al. [1996, 1997] showed that, when the odds of good news / bad news
    are even, the probability of informed trading (PIN) can be derived as
  prefs: []
  type: TYPE_NORMAL
- en: '![](Image00329.jpg)'
  prefs: []
  type: TYPE_IMG
- en: where μ is the rate of arrival of informed traders, ϵ is the rate of arrival
    of uninformed traders, and α is the probability of an informational event. PIN
    can be interpreted as the fraction of orders that arise from informed traders
    relative to the overall order flow.
  prefs: []
  type: TYPE_NORMAL
- en: Within a volume bar of size *V* , we can classify ticks as buy or sell according
    to some algorithm, such as the tick rule or the Lee-Ready algorithm. Let *V ^(*B*)
    [τ]* be the sum of the volumes from buy ticks included in volume bar τ, and *V
    ^(*S*) [τ]* the sum of the volumes from sell ticks within volume bar τ. Easley
    et al. [2012a, 2012b] note that E[| *V ^(*B*) [τ]* − *V [τ] ^(*S*)* |] ≈ αμ and
    that the expected total volume is E[ *V ^(*B*) [τ]* + *V [τ] ^(*S*)* ] = αμ +
    2ϵ. By using a volume clock (Easley et al. [2012c]), we can set the value of E[
    *V ^(*B*) [τ]* + *V [τ] ^(*S*)* ] = αμ + 2ϵ = *V* exogenously. This means that,
    under a volume clock, PIN reduces to
  prefs: []
  type: TYPE_NORMAL
- en: '![](Image00564.jpg)'
  prefs: []
  type: TYPE_IMG
- en: where ![](Image00483.jpg) . Note that 2 *v ^(*B*) [  τ  ] * − 1 represents the
    order flow imbalance, *OI [τ] * , which is a bounded real-valued variable, where
    *OI [τ] * ∈ [ − 1, 1]. The VPIN theory thus provides a formal link between the
    probability of informed trading (PIN) and the persistency of order flow imbalances
    under a volume clock. See Chapter 19 for further details on this microstructural
    theory.
  prefs: []
  type: TYPE_NORMAL
- en: Persistent order flow imbalance is a necessary, non-sufficient condition for
    adverse selection. For market makers to provide liquidity to informed traders,
    that order flow imbalance | *OI [τ]* | must also have been relatively unpredictable.
    In other words, market makers are not adversely selected when their prediction
    of order flow imbalance is accurate, even if | *OI [τ]* | ≫ 0\. In order to determine
    the probability of adverse selection, we must determine how unpredictable the
    order flow imbalance is. We can determine this by applying information theory.
  prefs: []
  type: TYPE_NORMAL
- en: Consider a long sequence of symbols. When that sequence contains few redundant
    patterns, it encompasses a level of complexity that makes it hard to describe
    and predict. Kolmogorov [1965] formulated this connection between redundancy and
    complexity. In information theory, lossless compression is the task of perfectly
    describing a sequence with as few bits as possible. The more redundancies a sequence
    contains, the greater compression rates can be achieved. Entropy characterizes
    the redundancy of a source, hence its Kolmogorov complexity and its predictability.
    We can use this connection between the redundancy of a sequence and its unpredictability
    (by market makers) to derive the probability of adverse selection.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here we will discuss one particular procedure that derives the probability
    of adverse selection as a function of the complexity ingrained in the order flow
    imbalance. First, given a sequence of volume bars indexed by τ = 1, …, *N* , each
    bar of size *V* , we determine the portion of volume classified as buy, *v ^(*B*)
    [τ]* ∈ [0, 1]. Second, we compute the *q* -quantiles on { *v ^(*B*) [τ]* } that
    define a set *K* of *q* disjoint subsets, *K* = { *K [1]* , …, *K [*q*]* }. Third,
    we produce a mapping from each *v ^(*B*) [τ]* to one of the disjoint subsets,
    *f* : *v ^(*B*) [τ]* → {1, …, *q* }, where *f* [ *v ^(*B*) [τ]* ] = *i* ⇔ *v [τ]
    ^(*B*)* ∈ *K [*i*]* , ∀ *i* ∈ [1, *q* ]. Fourth, we quantize { *v ^(*B*) [τ]*
    } by assigning to each value *v ^(*B*) [τ]* the index of the subset *K* it belongs
    to, *f* [ *v ^(*B*) [τ]* ]. This results in a translation of the set of order
    imbalances { *v ^(*B*) [τ]* } into a quantized message *X* = [ *f* [ *v ^(*B*)
    [1]* ], *f* [ *v [2] ^(*B*)* ], …, *f* [ *v ^(*B*) [*N*]* ]]. Fifth, we estimate
    the entropy *H* [ *X* ] using Kontoyiannis’ Lempel-Ziv algorithm. Sixth, we derive
    the cumulative distribution function, *F* [ *H* [ *X* ]], and use the time series
    of { *F* [ *H* [ *X [τ]* ]]} [τ = 1, …, *N*] as a feature to predict adverse selection.'
  prefs: []
  type: TYPE_NORMAL
- en: '**Exercises**'
  prefs: []
  type: TYPE_NORMAL
- en: 'Form dollar bars on E-mini S&P 500 futures:'
  prefs:
  - PREF_OL
  - PREF_BQ
  - PREF_BQ
  type: TYPE_NORMAL
- en: Quantize the returns series using the binary method.
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Quantize the returns series using the quantile encoding, using 10 letters.
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Quantize the returns series using the sigma encoding, where σ is the standard
    deviation of all bar returns.
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Compute the entropy of the three encoded series, using the plug-in method.
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Compute the entropy of the three encoded series, using Kontoyiannis’ method,
    with a window size of 100.
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Using the bars from exercise 1:'
  prefs:
  - PREF_OL
  - PREF_BQ
  - PREF_BQ
  type: TYPE_NORMAL
- en: Compute the returns series, {*r [*t*]* }.
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Encode the series as follows: 0 if *r [*t*] r [*t* − 1]* < 0, and 1 if *r [*t*]
    r [*t* − 1]* ≥ 0.'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Partition the series into 1000 non-overlapping subsets of equal size (you may
    have to drop some observations at the beginning).
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Compute the entropy of each of the 1000 encoded subsets, using the plug-in method.
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Compute the entropy of each of the 1000 encoded subsets, using the Kontoyiannis
    method, with a window size of 100.
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Compute the correlation between results 2.d and 2.e.
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Draw 1000 observations from a standard Normal distribution:'
  prefs:
  - PREF_OL
  - PREF_BQ
  - PREF_BQ
  type: TYPE_NORMAL
- en: What is the true entropy of this process?
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Label the observations according to 8 quantiles.
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Estimate the entropy using the plug-in method.
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Estimate the entropy using the Kontoyiannis method:'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: using a window size of 10.
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: using a window size of 100.
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Using the draws from exercise 3, { *x [*t*]* } [*t* = 1, …, 1000] :'
  prefs:
  - PREF_OL
  - PREF_BQ
  - PREF_BQ
  type: TYPE_NORMAL
- en: Compute *y [*t*]* = ρ*y [*t* − 1]* + *x [*t*]* , where ρ = .5, *y [0]* = 0.
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Label {*y [*t*]* } the observations according to 8 quantiles.
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Estimate the entropy using the plug-in method.
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Estimate the entropy using the Kontoyiannis method
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: using a window size of 10.
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: using a window size of 100.
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Suppose a portfolio of 10 holdings with equal dollar allocations.
  prefs:
  - PREF_OL
  - PREF_BQ
  - PREF_BQ
  type: TYPE_NORMAL
- en: The portion of the total risk contributed by the *i* th principal component
    is ![](Image00572.jpg) , *i* = 1, …, 10\. What is the portfolio's entropy?
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: The portion of the total risk contributed by the *i* th principal component
    is ![](Image00667.jpg) , *i* = 1, …, 10\. What is the portfolio's entropy?
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: The portion of the total risk contributed by the *i* th principal component
    is ![](Image00745.jpg) , *i* = 1, …, 10, α ∈ [0, 1]. Plot the portolio's entropy
    as a function of α.
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '**References**'
  prefs: []
  type: TYPE_NORMAL
- en: 'Bailey, D. and M. López de Prado (2012): “Balanced baskets: A new approach
    to trading and hedging risks.” *Journal of Investment Strategies* , Vol. 1, No.
    4, pp. 21–62\. Available at [https://ssrn.com/abstract=2066170.](https://ssrn.com/abstract=2066170.)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Easley D., M. Kiefer, M. O''Hara, and J. Paperman (1996): “Liquidity,information,
    and infrequently traded stocks.” *Journal of Finance* , Vol. 51, No. 4, pp. 1405–1436.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Easley D., M. Kiefer and, M. O''Hara (1997): “The information content of the
    trading process.” *Journal of Empirical Finance* , Vol. 4, No. 2, pp. 159–185.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Easley, D., M. López de Prado, and M. O''Hara (2012a): “Flow toxicity and liquidity
    in a high frequency world.” *Review of Financial Studies* , Vol. 25, No. 5, pp.
    1547–1493.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Easley, D., M. López de Prado, and M. O''Hara (2012b): “The volume clock: Insights
    into the high frequency paradigm.” *Journal of Portfolio Management* , Vol. 39,
    No. 1, pp. 19–29.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Gao, Y., I. Kontoyiannis and E. Bienestock (2008): “Estimating the entropy
    of binary time series: Methodology, some theory and a simulation study.” Working
    paper, arXiv. Available at [https://arxiv.org/abs/0802.4363v1.](https://arxiv.org/abs/0802.4363v1.)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Fiedor, Pawel (2014a): “Mutual information rate-based networks in financial
    markets.” Working paper, arXiv. Available at [https://arxiv.org/abs/1401.2548.](https://arxiv.org/abs/1401.2548.)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Fiedor, Pawel (2014b): “Information-theoretic approach to lead-lag effect on
    financial markets.” Working paper, arXiv. Available at [https://arxiv.org/abs/1402.3820.](https://arxiv.org/abs/1402.3820.)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Fiedor, Pawel (2014c): “Causal non-linear financial networks.” Working paper,
    arXiv. Available at [https://arxiv.org/abs/1407.5020.](https://arxiv.org/abs/1407.5020.)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Hausser, J. and K. Strimmer (2009): “Entropy inference and the James-Stein
    estimator, with application to nonlinear gene association networks,” *Journal
    of Machine Learning Research* , Vol. 10, pp. 1469–1484\. [http://www.jmlr.org/papers/volume10/hausser09a/hausser09a.pdf.](http://www.jmlr.org/papers/volume10/hausser09a/hausser09a.pdf.)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Kolmogorov, A. (1965): “Three approaches to the quantitative definition of
    information.” *Problems in Information Transmission* , Vol. 1, No. 1, pp. 1–7.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Kontoyiannis, I. (1997): “The complexity and entropy of literary styles”, *NSF
    Technical Report* # 97.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Kontoyiannis (1998): “Asymptotically optimal lossy Lempel-Ziv coding,” *ISIT*
    , Cambridge, MA, August 16–August 21.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'MacKay, D. (2003): *Information Theory, Inference, and Learning Algorithms,
    1st ed* . Cambridge University Press.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Meucci, A. (2009): “Managing diversification.” *Risk Magazine* , Vol. 22, pp.
    74–79.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Norwich, K. (2003): *Information, Sensation and Perception, 1st ed* . Academic
    Press.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Ornstein, D.S. and B. Weiss (1993): “Entropy and data compression schemes.”
    *IEEE Transactions on Information Theory* , Vol. 39, pp. 78–83.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Shannon, C. (1948): “A mathematical theory of communication.” *Bell System
    Technical Journal* , Vol. 27, No. 3, pp. 379–423.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Ziv, J. and A. Lempel (1978): “Compression of individual sequences via variable-rate
    coding.” *IEEE Transactions on Information Theory* , Vol. 24, No. 5, pp. 530–536.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Bibliography**'
  prefs: []
  type: TYPE_NORMAL
- en: 'Easley, D., R. Engle, M. O''Hara, and L. Wu (2008): “Time-varying arrival rates
    of informed and uninformed traders.” *Journal of Financial Econometrics* , Vol.
    6, No. 2, pp. 171–207.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Easley, D., M. López de Prado, and M. O''Hara (2011): “The microstructure of
    the flash crash.” *Journal of Portfolio Management* , Vol. 37, No. 2, pp. 118–128.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Easley, D., M. López de Prado, and M. O''Hara (2012c): “Optimal execution horizon.”
    *Mathematical Finance* , Vol. 25, No. 3, pp. 640–672.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Gnedenko, B. and I. Yelnik (2016): “Minimum entropy as a measure of effective
    dimensionality.” Working paper. Available at [https://ssrn.com/abstract=2767549.](https://ssrn.com/abstract=2767549.)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Note**'
  prefs: []
  type: TYPE_NORMAL
- en: ^([1](text00003.html#filepos0000940268))    Alternatively, we could have worked
    with a vector of holdings, should the covariance matrix had been computed on price
    changes.
  prefs: []
  type: TYPE_NORMAL
- en: '**CHAPTER 19**'
  prefs: []
  type: TYPE_NORMAL
- en: '**Microstructural Features**'
  prefs: []
  type: TYPE_NORMAL
- en: '**19.1 Motivation**'
  prefs: []
  type: TYPE_NORMAL
- en: Market microstructure studies “the process and outcomes of exchanging assets
    under explicit trading rules” (O'Hara [1995]). Microstructural datasets include
    primary information about the auctioning process, like order cancellations, double
    auction book, queues, partial fills, aggressor side, corrections, replacements,
    etc. The main source is Financial Information eXchange (FIX) messages, which can
    be purchased from exchanges. The level of detail contained in FIX messages provides
    researchers with the ability to understand how market participants conceal and
    reveal their intentions. That makes microstructural data one of the most important
    ingredients for building predictive ML features.
  prefs: []
  type: TYPE_NORMAL
- en: '**19.2 Review of the Literature**'
  prefs: []
  type: TYPE_NORMAL
- en: The depth and complexity of market microstructure theories has evolved over
    time, as a function of the amount and variety of the data available. The first
    generation of models used solely price information. The two foundational results
    from those early days are trade classification models (like the tick rule) and
    the Roll [1984] model. The second generation of models came after volume datasets
    started to become available, and researchers shifted their attention to study
    the impact that volume has on prices. Two examples for this generation of models
    are Kyle [1985] and Amihud [2002].
  prefs: []
  type: TYPE_NORMAL
- en: The third generation of models came after 1996, when Maureen O'Hara, David Easley,
    and others published their “probability of informed trading” (PIN) theory (Easley
    et al. [1996]). This constituted a major breakthrough, because PIN explained the
    bid-ask spread as the consequence of a sequential strategic decision between liquidity
    providers (market makers) and position takers (informed traders). Essentially,
    it illustrated that market makers were sellers of the option to be adversely selected
    by informed traders, and the bid-ask spread is the premium they charge for that
    option. Easley et al. [2012a, 2012b] explain how to estimate VPIN, a high-frequency
    estimate of PIN under volume-based sampling.
  prefs: []
  type: TYPE_NORMAL
- en: These are the main theoretical frameworks used by the microstructural literature.
    O'Hara [1995] and Hasbrouck [2007] offer a good compendium of low-frequency microstructural
    models. Easley et al. [2013] present a modern treatment of high-frequency microstructural
    models.
  prefs: []
  type: TYPE_NORMAL
