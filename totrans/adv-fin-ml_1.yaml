- en: '**3.5 Learning Side and Size**'
  id: totrans-0
  prefs: []
  type: TYPE_NORMAL
  zh: '**3.5 学习方向和大小**'
- en: In this section we will discuss how to label examples so that an ML algorithm
    can learn both the side and the size of a bet. We are interested in learning the
    side of a bet when we do not have an underlying model to set the sign of our position
    (long or short). Under such circumstance, we cannot differentiate between a profit-taking
    barrier and a stop-loss barrier, since that requires knowledge of the side. Learning
    the side implies that either there are no horizontal barriers or that the horizontal
    barriers must be symmetric.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将讨论如何标记示例，以便机器学习算法可以学习赌注的方向和大小。当我们没有基础模型来设置头寸的符号（多头或空头）时，我们对学习赌注的方向感兴趣。在这种情况下，我们无法区分获利了结障碍和止损障碍，因为这需要对方向的了解。学习方向意味着要么没有横向障碍，要么横向障碍必须是对称的。
- en: 'Snippet 3.3 implements the function `getEvents` , which finds the time of the
    first barrier touch. The function receives the following arguments:'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 片段 3.3 实现了函数 `getEvents`，该函数找到第一个障碍触及的时间。该函数接收以下参数：
- en: '`close` : A pandas series of prices.'
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`close` : 一个价格的 pandas 系列。'
- en: '`tEvents` : The pandas timeindex containing the timestamps that will seed every
    triple barrier. These are the timestamps selected by the sampling procedures discussed
    in Chapter 2, Section 2.5.'
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`tEvents` : 包含将为每个三重障碍播种的时间戳的 pandas 时间索引。这些是第2章第2.5节讨论的采样程序选定的时间戳。'
- en: '`ptSl` : A non-negative float that sets the width of the two barriers. A 0
    value means that the respective horizontal barrier (profit taking and/or stop
    loss) will be disabled.'
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`ptSl` : 一个非负浮点数，用于设置两个障碍的宽度。0 值表示相应的横向障碍（获利了结和/或止损）将被禁用。'
- en: '`t1` : A pandas series with the timestamps of the vertical barriers. We pass
    a `False` when we want to disable vertical barriers.'
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`t1` : 一个包含垂直障碍时间戳的 pandas 系列。当我们想禁用垂直障碍时传递 `False`。'
- en: '`trgt` : A pandas series of targets, expressed in terms of absolute returns.'
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`trgt` : 一个表示绝对收益的 pandas 系列。'
- en: '`minRet` : The minimum target return required for running a triple barrier
    search.'
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`minRet` : 进行三重障碍搜索所需的最低目标收益。'
- en: '`numThreads` : The number of threads concurrently used by the function.'
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`numThreads` : 函数同时使用的线程数。'
- en: '**SNIPPET 3.3 GETTING THE TIME OF FIRST TOUCH**'
  id: totrans-10
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '**片段 3.3 获取第一次触及的时间**'
- en: '![](Image00624.jpg)'
  id: totrans-11
  prefs:
  - PREF_BQ
  type: TYPE_IMG
  zh: '![](Image00624.jpg)'
- en: Suppose that *I* = 1 *E* 6 and *h* = 1 *E* 3, then the number of conditions
    to evaluate is up to one billion on a single instrument. Many ML tasks are computationally
    expensive unless you are familiar with multi-threading, and this is one of them.
    Here is where parallel computing comes into play. Chapter 20 discusses a few multiprocessing
    functions that we will use throughout the book.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 假设 *I* = 1 *E* 6 和 *h* = 1 *E* 3，那么在单个工具上评估的条件数量可高达十亿。许多机器学习任务计算上很昂贵，除非你熟悉多线程，而这就是其中之一。并行计算在这里发挥作用。第20章讨论了一些我们将在整本书中使用的多进程函数。
- en: 'Function `mpPandasObj` calls a multiprocessing engine, which is explained in
    depth in Chapter 20\. For the moment, you simply need to know that this function
    will execute `applyPtSlOnT1` in parallel. Function `applyPtSlOnT1` returns the
    timestamps at which each barrier is touched (if any). Then, the time of the first
    touch is the earliest time among the three returned by `applyPtSlOnT1` . Because
    we must learn the side of the bet, we have passed `ptSl = [ptSl,ptSl]` as argument,
    and we arbitrarily set the side to be always long (the horizontal barriers are
    symmetric, so the side is irrelevant to determining the time of the first touch).
    The output from this function is a pandas dataframe with columns:'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 函数 `mpPandasObj` 调用一个多进程引擎，这在第20章中有深入解释。此时，你只需知道该函数将并行执行 `applyPtSlOnT1`。函数
    `applyPtSlOnT1` 返回每个障碍被触及的时间戳（如果有的话）。然后，第一个触及的时间是 `applyPtSlOnT1` 返回的三个时间中最早的时间。因为我们必须学习赌注的方向，我们传递
    `ptSl = [ptSl, ptSl]` 作为参数，并且我们任意将方向设置为始终为多头（横向障碍是对称的，因此方向与确定第一次触及的时间无关）。该函数的输出是一个包含以下列的
    pandas 数据框：
- en: '`t1` : The timestamp at which the first barrier is touched.'
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`t1` : 第一个障碍被触及的时间戳。'
- en: '`trgt` : The target that was used to generate the horizontal barriers.'
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`trgt` : 用于生成横向障碍的目标。'
- en: Snippet 3.4 shows one way to define a vertical barrier. For each index in `tEvents`
    , it finds the timestamp of the next price bar at or immediately after a number
    of days `numDays` . This vertical barrier can be passed as optional argument `t1`
    in `getEvents` .
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 代码片段 3.4 展示了定义垂直障碍的一种方法。对于 `tEvents` 中的每个索引，它会找到在 `numDays` 天数之后或立即之后的下一个价格柱的时间戳。这个垂直障碍可以作为可选参数
    `t1` 传递给 `getEvents`。
- en: '**SNIPPET 3.4 ADDING A VERTICAL BARRIER**'
  id: totrans-17
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '**代码片段 3.4 添加垂直障碍**'
- en: '![](Image00625.jpg)'
  id: totrans-18
  prefs:
  - PREF_BQ
  type: TYPE_IMG
  zh: '![](Image00625.jpg)'
- en: 'Finally, we can label the observations using the `getBins` function defined
    in Snippet 3.5\. The arguments are the `events` dataframe we just discussed, and
    the `close` pandas series of prices. The output is a dataframe with columns:'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们可以使用代码片段 3.5 中定义的 `getBins` 函数对观察进行标记。参数是我们刚刚讨论的 `events` 数据框和 `close`
    pandas 价格序列。输出是一个包含以下列的数据框：
- en: '`ret` : The return realized at the time of the first touched barrier.'
  id: totrans-20
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`ret` : 第一个触及障碍时实现的回报。'
- en: '`bin` : The label, { − 1, 0, 1}, as a function of the sign of the outcome.
    The function can be easily adjusted to label as 0 those events when the vertical
    barrier was touched first, which we leave as an exercise.'
  id: totrans-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`bin` : 标签，{−1, 0, 1}，作为结果符号的函数。该函数可以很容易地调整为将触及垂直障碍的事件标记为 0，这留作练习。'
- en: '**SNIPPET 3.5 LABELING FOR SIDE AND SIZE**'
  id: totrans-22
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '**代码片段 3.5 侧面和大小的标记**'
- en: '![](Image00628.jpg)'
  id: totrans-23
  prefs:
  - PREF_BQ
  type: TYPE_IMG
  zh: '![](Image00628.jpg)'
- en: '**3.6 Meta-Labeling**'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: '**3.6 元标记**'
- en: Suppose that you have a model for setting the side of the bet (long or short).
    You just need to learn the size of that bet, which includes the possibility of
    no bet at all (zero size). This is a situation that practitioners face regularly.
    We often know whether we want to buy or sell a product, and the only remaining
    question is how much money we should risk in such a bet. We do not want the ML
    algorithm to learn the side, just to tell us what is the appropriate size. At
    this point, it probably does not surprise you to hear that no book or paper has
    so far discussed this common problem. Thankfully, that misery ends here. I call
    this problem meta-labeling because we want to build a secondary ML model that
    learns how to use a primary exogenous model.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 假设你有一个模型来设置投注方向（做多或做空）。你只需要了解该投注的大小，包括没有投注（零大小）的可能性。这是实践者经常面临的情况。我们通常知道我们想买入还是卖出某种产品，唯一剩下的问题是我们应该在这种投注中冒多大风险。我们不希望机器学习算法学习方向，只希望告诉我们适当的大小。在这一点上，听到没有书籍或论文讨论这一常见问题可能不会让你感到惊讶。幸运的是，这种困境在这里结束。我称这个问题为元标记，因为我们想构建一个次级机器学习模型，学习如何使用一个主要的外生模型。
- en: Rather than writing an entirely new `getEvents` function, we will make some
    adjustments to the previous code, in order to handle meta-labeling. First, we
    accept a new `side` optional argument (with default `None` ), which contains the
    side of our bets as decided by the primary model. When `side` is not `None` ,
    the function understands that meta-labeling is in play. Second, because now we
    know the side, we can effectively discriminate between profit taking and stop
    loss. The horizontal barriers do not need to be symmetric, as in Section 3.5\.
    Argument `ptSl` is a list of two non-negative float values, where `ptSl[0]` is
    the factor that multiplies `trgt` to set the width of the upper barrier, and `ptSl[1]`
    is the factor that multiplies `trgt` to set the width of the lower barrier. When
    either is 0, the respective barrier is disabled. Snippet 3.6 implements these
    enhancements.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将对之前的代码进行一些调整，而不是编写一个全新的 `getEvents` 函数，以处理元标记。首先，我们接受一个新的 `side` 可选参数（默认值为
    `None`），该参数包含由主模型决定的我们的投注方向。当 `side` 不是 `None` 时，函数理解元标记在起作用。其次，由于我们现在知道方向，我们可以有效地区分获利和平仓。水平障碍不需要对称，如第
    3.5 节所述。参数 `ptSl` 是一个包含两个非负浮点值的列表，其中 `ptSl[0]` 是乘以 `trgt` 的因子，用于设置上障碍的宽度，而 `ptSl[1]`
    是乘以 `trgt` 的因子，用于设置下障碍的宽度。当任一值为 0 时，相应的障碍被禁用。代码片段 3.6 实现了这些增强功能。
- en: '**SNIPPET 3.6 EXPANDING** `**GETEVENTS**` **TO INCORPORATE META-LABELING**'
  id: totrans-27
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '**代码片段 3.6 扩展 `**GETEVENTS**` 以整合元标记**'
- en: '![](Image00337.jpg)'
  id: totrans-28
  prefs:
  - PREF_BQ
  type: TYPE_IMG
  zh: '![](Image00337.jpg)'
- en: Likewise, we need to expand the `getBins` function, so that it handles meta-labeling.
    Snippet 3.7 implements the necessary changes.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 同样，我们需要扩展 `getBins` 函数，以便它处理元标记。代码片段 3.7 实现了必要的更改。
- en: '**SNIPPET 3.7 EXPANDING** `**GETBINS**` **TO INCORPORATE META-LABELING**'
  id: totrans-30
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '**代码片段 3.7 扩展 `**GETBINS**` 以整合元标记**'
- en: '![](Image00633.jpg)'
  id: totrans-31
  prefs:
  - PREF_BQ
  type: TYPE_IMG
  zh: '![](Image00633.jpg)'
- en: Now the possible values for labels in `out[‘bin’]` are {0,1}, as opposed to
    the previous feasible values {−1,0,1}. The ML algorithm will be trained to decide
    whether to take the bet or pass, a purely binary prediction. When the predicted
    label is 1, we can use the probability of this secondary prediction to derive
    the size of the bet, where the side (sign) of the position has been set by the
    primary model.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，`out[‘bin’]`中标签的可能值为{0,1}，与之前的可行值{−1,0,1}不同。机器学习算法将被训练来决定是下注还是放弃，这是一种纯粹的二元预测。当预测标签为1时，我们可以利用这个二次预测的概率来推导下注的大小，其中位置的方向（符号）由主要模型设定。
- en: '**3.7 How to Use Meta-Labeling**'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: '**3.7 如何使用元标签**'
- en: Binary classification problems present a trade-off between type-I errors (false
    positives) and type-II errors (false negatives). In general, increasing the true
    positive rate of a binary classifier will tend to increase its false positive
    rate. The receiver operating characteristic (ROC) curve of a binary classifier
    measures the cost of increasing the true positive rate, in terms of accepting
    higher false positive rates.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 二元分类问题在假阳性（类型I错误）和假阴性（类型II错误）之间存在权衡。一般来说，提高二元分类器的真正阳性率会倾向于提高其假阳性率。二元分类器的接收者操作特征（ROC）曲线衡量提高真正阳性率的成本，表现为接受更高假阳性率。
- en: '[Figure 3.2](text00001.html#filepos0000256656) illustrates the so-called “confusion
    matrix.” On a set of observations, there are items that exhibit a condition (positives,
    left rectangle), and items that do not exhibit a condition (negative, right rectangle).
    A binary classifier predicts that some items exhibit the condition (ellipse),
    where the TP area contains the true positives and the TN area contains the true
    negatives. This leads to two kinds of errors: false positives (FP) and false negatives
    (FN). “Precision” is the ratio between the TP area and the area in the ellipse.
    “Recall” is the ratio between the TP area and the area in the left rectangle.
    This notion of recall (aka true positive rate) is in the context of classification
    problems, the analogous to “power” in the context of hypothesis testing. “Accuracy”
    is the sum of the TP and TN areas divided by the overall set of items (square).
    In general, decreasing the FP area comes at a cost of increasing the FN area,
    because higher precision typically means fewer calls, hence lower recall. Still,
    there is some combination of precision and recall that maximizes the overall efficiency
    of the classifier. The F1-score measures the efficiency of a classifier as the
    harmonic average between precision and recall (more on this in Chapter 14).'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: '[图 3.2](text00001.html#filepos0000256656) 说明了所谓的“混淆矩阵”。在一组观察中，有一些项目表现出一种条件（正类，左矩形），而有些项目则没有表现出这种条件（负类，右矩形）。一个二元分类器预测某些项目表现出该条件（椭圆），其中TP区域包含真正的正类，TN区域包含真正的负类。这导致了两种错误：假阳性（FP）和假阴性（FN）。“精确率”是TP区域与椭圆区域的比率。“召回率”是TP区域与左矩形区域的比率。这种召回率（即真正阳性率）是在分类问题的背景下，类似于假设检验中的“检验力”。“准确率”是TP和TN区域的总和除以总体项目（方形）。一般来说，降低FP区域的代价是增加FN区域，因为较高的精确率通常意味着较少的调用，因此召回率较低。尽管如此，仍然存在一种精确率和召回率的组合，可以最大化分类器的整体效率。F1分数测量分类器的效率，作为精确率和召回率之间的调和平均值（更多内容见第14章）。'
- en: '![](Image00637.jpg)'
  id: totrans-36
  prefs: []
  type: TYPE_IMG
  zh: '![](Image00637.jpg)'
- en: '[**Figure 3.2**](text00001.html#filepos0000255126) A visualization of the “confusion
    matrix”'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: '[**图 3.2**](text00001.html#filepos0000255126) “混淆矩阵”的可视化'
- en: Meta-labeling is particularly helpful when you want to achieve higher F1-scores.
    First, we build a model that achieves high recall, even if the precision is not
    particularly high. Second, we correct for the low precision by applying meta-labeling
    to the positives predicted by the primary model.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 元标签在你想要实现更高的F1分数时特别有用。首先，我们建立一个即使精确率不是特别高也能实现高召回率的模型。其次，我们通过对主要模型预测的正类应用元标签来修正低精确率。
- en: Meta-labeling will increase your F1-score by filtering out the false positives,
    where the majority of positives have already been identified by the primary model.
    Stated differently, the role of the secondary ML algorithm is to determine whether
    a positive from the primary (exogenous) model is true or false. It is *not* its
    purpose to come up with a betting opportunity. Its purpose is to determine whether
    we should act or pass on the opportunity that has been presented.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 元标签将通过过滤掉假阳性来提高你的F1-score，其中大多数阳性已经被主要模型识别。换句话说，次要机器学习算法的作用是确定来自主要（外生）模型的阳性是真还是假。它*并不是*为了提供投注机会。它的目的是确定我们是采取行动还是放弃所呈现的机会。
- en: Meta-labeling is a very powerful tool to have in your arsenal, for four additional
    reasons. First, ML algorithms are often criticized as black boxes (see Chapter
    1). Meta-labeling allows you to build an ML system on top of a white box (like
    a fundamental model founded on economic theory). This ability to transform a fundamental
    model into an ML model should make meta-labeling particularly useful to “quantamental”
    firms. Second, the effects of overfitting are limited when you apply meta-labeling,
    because ML will not decide the side of your bet, only the size. Third, by decoupling
    the side prediction from the size prediction, meta-labeling enables sophisticated
    strategy structures. For instance, consider that the features driving a rally
    may differ from the features driving a sell-off. In that case, you may want to
    develop an ML strategy exclusively for long positions, based on the buy recommendations
    of a primary model, and an ML strategy exclusively for short positions, based
    on the sell recommendations of an entirely different primary model. Fourth, achieving
    high accuracy on small bets and low accuracy on large bets will ruin you. As important
    as identifying good opportunities is to size them properly, so it makes sense
    to develop an ML algorithm solely focused on getting that critical decision (sizing)
    right. We will retake this fourth point in Chapter 10\. In my experience, meta-labeling
    ML models can deliver more robust and reliable outcomes than standard labeling
    models.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 元标签是你武器库中一个非常强大的工具，原因有四个。首先，机器学习算法常常被批评为黑箱（见第一章）。元标签允许你在白箱之上构建一个机器学习系统（如基于经济理论的基础模型）。将基础模型转化为机器学习模型的能力使得元标签对于“量化基础”公司特别有用。其次，当应用元标签时，过拟合的影响被限制，因为机器学习不会决定你下注的方向，只会决定大小。第三，通过将方向预测与大小预测分开，元标签能够实现复杂的策略结构。例如，驱动上涨的特征可能与驱动下跌的特征不同。在这种情况下，你可能想要专门开发一个基于主要模型买入推荐的长期头寸机器学习策略，以及一个基于完全不同主要模型卖出推荐的短期头寸机器学习策略。第四，在小额投注上实现高准确率而在大额投注上实现低准确率会毁了你。识别良好机会的同时，合理地确定其规模同样重要，因此开发一个专注于准确判断（规模）的机器学习算法是有意义的。我们将在第十章中重提这一第四点。根据我的经验，元标签机器学习模型能够提供比标准标签模型更稳健和可靠的结果。
- en: '**3.8 The Quantamental Way**'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: '**3.8 量化基础方法**'
- en: You may have read in the press that many hedge funds are embracing the quantamental
    approach. A simple Google search will show reports that many hedge funds, including
    some of the most traditional ones, are investing tens of millions of dollars in
    technologies designed to combine human expertise with quantitative methods. It
    turns out, meta-labeling is exactly what these people have been waiting for. Let
    us see why.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 你可能在媒体上看到许多对冲基金正在采纳量化基础的方法。简单的谷歌搜索会显示报告，许多对冲基金，包括一些最传统的基金，正在投资数千万美元于旨在结合人类专业知识与定量方法的技术。事实证明，元标签正是这些人一直在等待的。让我们看看原因。
- en: 'Suppose that you have a series of features that you believe can forecast some
    prices, you just do not know how. Since you do not have a model to determine the
    side of each bet, you need to learn both side and size. You apply what you have
    learned in Section 3.5, and produce some labels based on the triple-barrier method
    with symmetric horizontal barriers. Now you are ready to fit your algorithm on
    a training set, and evaluate the accuracy of your forecasts on a testing set.
    Alternatively, you could do the following:'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 假设你有一系列特征，相信可以预测某些价格，只是不知道如何做。由于你没有模型来确定每次下注的方向，你需要学习方向和大小。你应用在第3.5节中学到的知识，并基于对称水平边界的三重障碍法生成一些标签。现在你准备在训练集上拟合你的算法，并在测试集上评估预测的准确性。或者，你可以这样做：
- en: Use your forecasts from the primary model, and generate meta-labels. Remember,
    horizontal barriers do not need to be symmetric in this case.
  id: totrans-44
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用主要模型的预测，并生成元标签。请记住，在这种情况下，水平障碍不需要对称。
- en: Fit your model again on the same training set, but this time using the meta-labels
    you just generated.
  id: totrans-45
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 再次在相同的训练集上拟合你的模型，但这次使用你刚生成的元标签。
- en: Combine the “sides” from the first ML model with the “sizes” from the second
    ML model.
  id: totrans-46
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将第一个机器学习模型的“边”与第二个机器学习模型的“大小”结合起来。
- en: You can always add a meta-labeling layer to any primary model, whether that
    is an ML algorithm, an econometric equation, a technical trading rule, a fundamental
    analysis, etc. That includes forecasts generated by a human, solely based on his
    intuition. In that case, meta-labeling will help us figure out when we should
    pursue or dismiss a discretionary PM's call. The features used by such meta-labeling
    ML algorithm could range from market information to biometric statistics to psychological
    assessments. For example, the meta-labeling ML algorithm could find that discretionary
    PMs tend to make particularly good calls when there is a structural break (Chapter
    17), as they may be quicker to grasp a change in the market regime. Conversely,
    it may find that PMs under stress, as evidenced by fewer hours of sleep, fatigue,
    change in weight, etc. tend to make inaccurate predictions. ^([1](text00001.html#filepos0000278970))
    Many professions require regular psychological exams, and an ML meta-labeling
    algorithm may find that those scores are also relevant to assess our current degree
    of confidence on a PM's predictions. Perhaps none of these factors affect discretionary
    PMs, and their brains operate independently from their emotional being, like cold
    calculating machines. My guess is that this is not the case, and therefore meta-labeling
    should become an essential ML technique for every discretionary hedge fund. In
    the near future, every discretionary hedge fund will become a quantamental firm,
    and meta-labeling offers them a clear path to make that transition.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 你始终可以为任何主要模型添加一个元标签层，无论它是机器学习算法、计量经济方程、技术交易规则、基本分析等。这也包括仅基于人类直觉生成的预测。在这种情况下，元标签将帮助我们判断何时应追求或拒绝一个自由裁量基金经理的建议。这样的元标签机器学习算法使用的特征可以从市场信息到生物统计数据再到心理评估。例如，元标签机器学习算法可能会发现，当发生结构性变化时，自由裁量基金经理往往做出特别好的判断（第17章），因为他们可能更快地理解市场机制的变化。相反，它可能会发现，由于睡眠不足、疲劳、体重变化等原因，压力下的基金经理往往做出不准确的预测。^([1](text00001.html#filepos0000278970))
    许多职业要求定期进行心理考试，而机器学习元标签算法可能会发现这些分数也与评估我们对基金经理预测的信心程度相关。也许这些因素都不会影响自由裁量基金经理，他们的大脑独立于情感，像冷酷的计算机器一样运作。我猜这不是情况，因此元标签应成为每个自由裁量对冲基金的重要机器学习技术。在不久的将来，每个自由裁量对冲基金都将成为量化与基本结合的公司，元标签为它们提供了明确的转型路径。
- en: '**3.9 Dropping Unnecessary Labels**'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: '**3.9 删除不必要的标签**'
- en: Some ML classifiers do not perform well when classes are too imbalanced. In
    those circumstances, it is preferable to drop extremely rare labels and focus
    on the more common outcomes. Snippet 3.8 presents a procedure that recursively
    drops observations associated with extremely rare labels. Function `dropLabels`
    recursively eliminates those observations associated with classes that appear
    less than a fraction `minPct` of cases, unless there are only two classes left.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 一些机器学习分类器在类别严重不平衡时表现不佳。在这种情况下，最好删除极为稀有的标签，专注于更常见的结果。片段 3.8 提供了一种递归删除与极稀有标签相关的观察值的过程。函数
    `dropLabels` 递归消除与出现频率低于 `minPct` 的类相关的观察值，除非只剩下两个类。
- en: '**SNIPPET 3.8 DROPPING UNDER-POPULATED LABELS**'
  id: totrans-50
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '**片段 3.8 删除人口不足的标签**'
- en: '![](Image00640.jpg)'
  id: totrans-51
  prefs:
  - PREF_BQ
  type: TYPE_IMG
  zh: '![](Image00640.jpg)'
- en: 'Incidentally, another reason you may want to drop unnecessary labels is this
    known sklearn bug: [https://github.com/scikit-learn/scikit-learn/issues/8566](https://github.com/scikit-learn/scikit-learn/issues/8566)
    . This sort of bug is a consequence of very fundamental assumptions in sklearn
    implementation, and resolving them is far from trivial. In this particular instance,
    the error stems from sklearn''s decision to operate with standard numpy arrays
    rather than structured arrays or pandas objects. It is unlikely that there will
    be a fix by the time you are reading this chapter, or in the near future. In later
    chapters, we will study ways to circumvent these sorts of implementation errors,
    by building your own classes and expanding sklearn''s functionality.'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 顺便提一下，您可能希望去掉不必要的标签的另一个原因是这个已知的sklearn bug：[https://github.com/scikit-learn/scikit-learn/issues/8566](https://github.com/scikit-learn/scikit-learn/issues/8566)。这种bug是sklearn实现中非常基本假设的结果，解决它们远非易事。在这个特定实例中，错误源于sklearn选择使用标准numpy数组，而不是结构化数组或pandas对象。您在阅读本章时不太可能会有修复，或在不久的将来有修复。在后面的章节中，我们将研究通过构建您自己的类和扩展sklearn功能来规避这些实现错误的方法。
- en: '**Exercises**'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: '**练习**'
- en: 'Form dollar bars for E-mini S&P 500 futures:'
  id: totrans-54
  prefs:
  - PREF_OL
  - PREF_BQ
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 为E-mini S&P 500期货形成美元条：
- en: Apply a symmetric CUSUM filter (Chapter 2, Section 2.5.2.1) where the threshold
    is the standard deviation of daily returns (Snippet 3.1).
  id: totrans-55
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 应用对称CUSUM过滤器（第2章，第2.5.2.1节），阈值为日收益率的标准差（片段3.1）。
- en: Use Snippet 3.4 on a pandas series `t1` , where `numDays = 1` .
  id: totrans-56
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在一个pandas序列`t1`上使用片段3.4，其中`numDays = 1`。
- en: On those sampled features, apply the triple-barrier method, where `ptSl = [1,1]`
    and `t1` is the series you created in point 1.b.
  id: totrans-57
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在这些采样特征上，应用三重障碍法，其中`ptSl = [1,1]`，`t1`是您在第1.b点创建的序列。
- en: Apply `getBins` to generate the labels.
  id: totrans-58
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 应用`getBins`生成标签。
- en: From exercise 1, use Snippet 3.8 to drop rare labels.
  id: totrans-59
  prefs:
  - PREF_OL
  - PREF_BQ
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 从第1个练习中，使用片段3.8去掉稀有标签。
- en: Adjust the `getBins` function (Snippet 3.5) to return a 0 whenever the vertical
    barrier is the one touched first.
  id: totrans-60
  prefs:
  - PREF_OL
  - PREF_BQ
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 调整`getBins`函数（片段3.5），以便在垂直障碍首先被触及时返回0。
- en: Develop a trend-following strategy based on a popular technical analysis statistic
    (e.g., crossing moving averages). For each observation, the model suggests a side,
    but not a size of the bet.
  id: totrans-61
  prefs:
  - PREF_OL
  - PREF_BQ
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 基于流行的技术分析指标（例如交叉移动平均）开发趋势跟随策略。对于每个观察值，模型建议一个方向，但不建议下注的大小。
- en: Derive meta-labels for `ptSl = [1,2]` and `t1` where `numDays = 1` . Use as
    `trgt` the daily standard deviation as computed by Snippet 3.1.
  id: totrans-62
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 为`ptSl = [1,2]`和`t1`推导元标签，其中`numDays = 1`。将日标准差（由片段3.1计算得出）作为`trgt`。
- en: 'Train a random forest to decide whether to trade or not. Note: The decision
    is whether to trade or not, {0,1}, since the underlying model (the crossing moving
    average) has decided the side, {−1,1}.'
  id: totrans-63
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 训练一个随机森林来决定是否进行交易。注意：决定是是否交易，{0,1}，因为基础模型（交叉移动平均）已决定方向，{−1,1}。
- en: Develop a mean-reverting strategy based on Bollinger bands. For each observation,
    the model suggests a side, but not a size of the bet.
  id: totrans-64
  prefs:
  - PREF_OL
  - PREF_BQ
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 基于布林带开发均值回归策略。对于每个观察值，模型建议一个方向，但不建议下注的大小。
- en: Derive meta-labels for `ptSl = [0,2]` and `t1` where `numDays = 1` . Use as
    `trgt` the daily standard deviation as computed by Snippet 3.1.
  id: totrans-65
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 为`ptSl = [0,2]`和`t1`推导元标签，其中`numDays = 1`。将日标准差（由片段3.1计算得出）作为`trgt`。
- en: 'Train a random forest to decide whether to trade or not. Use as features: volatility,
    serial correlation, and the crossing moving averages from exercise 2.'
  id: totrans-66
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 训练一个随机森林来决定是否进行交易。特征包括：波动性、序列相关性以及第2个练习中的交叉移动平均线。
- en: What is the accuracy of predictions from the primary model (i.e., if the secondary
    model does not filter the bets)? What are the precision, recall, and F1-scores?
  id: totrans-67
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 主模型的预测准确率是多少（即，如果次级模型不过滤投注）？精确率、召回率和F1分数是多少？
- en: What is the accuracy of predictions from the secondary model? What are the precision,
    recall, and F1-scores?
  id: totrans-68
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 次级模型的预测准确率是多少？精确率、召回率和F1分数是多少？
- en: '**Bibliography**'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: '**参考文献**'
- en: 'Ahmed, N., A. Atiya, N. Gayar, and H. El-Shishiny (2010): “An empirical comparison
    of machine learning models for time series forecasting.” *Econometric Reviews*
    , Vol. 29, No. 5–6, pp. 594–621.'
  id: totrans-70
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 'Ahmed, N., A. Atiya, N. Gayar, 和 H. El-Shishiny (2010): “机器学习模型在时间序列预测中的实证比较。”
    *计量经济学评论*，第29卷，第5–6期，第594–621页。'
- en: 'Ballings, M., D. van den Poel, N. Hespeels, and R. Gryp (2015): “Evaluating
    multiple classifiers for stock price direction prediction.” *Expert Systems with
    Applications* , Vol. 42, No. 20, pp. 7046–7056.'
  id: totrans-71
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 'Ballings, M., D. van den Poel, N. Hespeels 和 R. Gryp (2015): “评估多种分类器用于股票价格方向预测。”
    *应用专家系统* ，第42卷，第20期，页码7046–7056。'
- en: 'Bontempi, G., S. Taieb, and Y. Le Borgne (2012): “Machine learning strategies
    for time series forecasting.” *Lecture Notes in Business Information Processing*
    , Vol. 138, No. 1, pp. 62–77.'
  id: totrans-72
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 'Bontempi, G., S. Taieb 和 Y. Le Borgne (2012): “时间序列预测的机器学习策略。” *商业信息处理讲义* ，第138卷，第1期，页码62–77。'
- en: 'Booth, A., E. Gerding and F. McGroarty (2014): “Automated trading with performance
    weighted random forests and seasonality.” *Expert Systems with Applications* ,
    Vol. 41, No. 8, pp. 3651–3661.'
  id: totrans-73
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 'Booth, A., E. Gerding 和 F. McGroarty (2014): “使用性能加权随机森林和季节性进行自动交易。” *应用专家系统*
    ，第41卷，第8期，页码3651–3661。'
- en: 'Cao, L. and F. Tay (2001): “Financial forecasting using support vector machines.”
    *Neural Computing & Applications* , Vol. 10, No. 2, pp. 184–192.'
  id: totrans-74
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 'Cao, L. 和 F. Tay (2001): “使用支持向量机进行金融预测。” *神经计算与应用* ，第10卷，第2期，页码184–192。'
- en: 'Cao, L., F. Tay and F. Hock (2003): “Support vector machine with adaptive parameters
    in financial time series forecasting.” *IEEE Transactions on Neural Networks*
    , Vol. 14, No. 6, pp. 1506–1518.'
  id: totrans-75
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 'Cao, L., F. Tay 和 F. Hock (2003): “在金融时间序列预测中使用自适应参数的支持向量机。” *IEEE神经网络学报* ，第14卷，第6期，页码1506–1518。'
- en: 'Cervelló-Royo, R., F. Guijarro, and K. Michniuk (2015): “Stock market trading
    rule based on pattern recognition and technical analysis: Forecasting the DJIA
    index with intraday data.” *Expert Systems with Applications* , Vol. 42, No. 14,
    pp. 5963–5975.'
  id: totrans-76
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 'Cervelló-Royo, R., F. Guijarro 和 K. Michniuk (2015): “基于模式识别和技术分析的股票市场交易规则：使用日内数据预测道琼斯工业平均指数。”
    *应用专家系统* ，第42卷，第14期，页码5963–5975。'
- en: 'Chang, P., C. Fan and J. Lin (2011): “Trend discovery in financial time series
    data using a case-based fuzzy decision tree.” *Expert Systems with Applications*
    , Vol. 38, No. 5, pp. 6070–6080.'
  id: totrans-77
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 'Chang, P., C. Fan 和 J. Lin (2011): “使用案例基础模糊决策树发现金融时间序列数据的趋势。” *应用专家系统* ，第38卷，第5期，页码6070–6080。'
- en: 'Kuan, C. and L. Tung (1995): “Forecasting exchange rates using feedforward
    and recurrent neural networks.” *Journal of Applied Econometrics* , Vol. 10, No.
    4, pp. 347–364.'
  id: totrans-78
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 'Kuan, C. 和 L. Tung (1995): “使用前馈和递归神经网络进行汇率预测。” *应用计量经济学期刊* ，第10卷，第4期，页码347–364。'
- en: 'Creamer, G. and Y. Freund (2007): “A boosting approach for automated trading.”
    *Journal of Trading* , Vol. 2, No. 3, pp. 84–96.'
  id: totrans-79
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 'Creamer, G. 和 Y. Freund (2007): “自动交易的提升方法。” *交易期刊* ，第2卷，第3期，页码84–96。'
- en: 'Creamer, G. and Y. Freund (2010): “Automated trading with boosting and expert
    weighting.” *Quantitative Finance* , Vol. 10, No. 4, pp. 401–420.'
  id: totrans-80
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 'Creamer, G. 和 Y. Freund (2010): “使用提升和专家加权的自动交易。” *量化金融* ，第10卷，第4期，页码401–420。'
- en: 'Creamer, G., Y. Ren, Y. Sakamoto, and J. Nickerson (2016): “A textual analysis
    algorithm for the equity market: The European case.” *Journal of Investing* ,
    Vol. 25, No. 3, pp. 105–116.'
  id: totrans-81
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 'Creamer, G., Y. Ren, Y. Sakamoto 和 J. Nickerson (2016): “针对股票市场的文本分析算法：欧洲案例。”
    *投资期刊* ，第25卷，第3期，页码105–116。'
- en: 'Dixon, M., D. Klabjan, and J. Bang (2016): “Classification-based financial
    markets prediction using deep neural networks.” *Algorithmic Finance* , forthcoming
    (2017). Available at SSRN: [https://ssrn.com/abstract=2756331](https://ssrn.com/abstract=2756331)
    .'
  id: totrans-82
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 'Dixon, M., D. Klabjan 和 J. Bang (2016): “基于分类的金融市场预测使用深度神经网络。” *算法金融* ，即将出版
    (2017)。可在SSRN获取: [https://ssrn.com/abstract=2756331](https://ssrn.com/abstract=2756331)
    。'
- en: 'Dunis, C., and M. Williams (2002): “Modelling and trading the euro/US dollar
    exchange rate: Do neural network models perform better?” *Journal of Derivatives
    & Hedge Funds* , Vol. 8, No. 3, pp. 211–239.'
  id: totrans-83
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 'Dunis, C. 和 M. Williams (2002): “建模和交易欧元/美元汇率：神经网络模型表现得更好吗？” *衍生品与对冲基金期刊* ，第8卷，第3期，页码211–239。'
- en: 'Feuerriegel, S. and H. Prendinger (2016): “News-based trading strategies.”
    *Decision Support Systems* , Vol. 90, pp. 65–74.'
  id: totrans-84
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 'Feuerriegel, S. 和 H. Prendinger (2016): “基于新闻的交易策略。” *决策支持系统* ，第90卷，页码65–74。'
- en: 'Hsu, S., J. Hsieh, T. Chih, and K. Hsu (2009): “A two-stage architecture for
    stock price forecasting by integrating self-organizing map and support vector
    regression.” *Expert Systems with Applications* , Vol. 36, No. 4, pp. 7947–7951.'
  id: totrans-85
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 'Hsu, S., J. Hsieh, T. Chih 和 K. Hsu (2009): “通过整合自组织映射和支持向量回归进行股票价格预测的两阶段架构。”
    *应用专家系统* ，第36卷，第4期，页码7947–7951。'
- en: 'Huang, W., Y. Nakamori, and S. Wang (2005): “Forecasting stock market movement
    direction with support vector machine.” *Computers & Operations Research* , Vol.
    32, No. 10, pp. 2513–2522.'
  id: totrans-86
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 'Huang, W.、Y. Nakamori 和 S. Wang (2005): “使用支持向量机预测股市变动方向。” *计算机与运筹研究* , 第 32
    卷, 第 10 期, 第 2513–2522 页。'
- en: 'Kara, Y., M. Boyacioglu, and O. Baykan (2011): “Predicting direction of stock
    price index movement using artificial neural networks and support vector machines:
    The sample of the Istanbul Stock Exchange.” *Expert Systems with Applications*
    , Vol. 38, No. 5, pp. 5311–5319.'
  id: totrans-87
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 'Kara, Y.、M. Boyacioglu 和 O. Baykan (2011): “使用人工神经网络和支持向量机预测股票价格指数变动的方向：以伊斯坦布尔证券交易所为例。”
    *专家系统与应用* , 第 38 卷, 第 5 期, 第 5311–5319 页。'
- en: 'Kim, K. (2003): “Financial time series forecasting using support vector machines.”
    *Neurocomputing* , Vol. 55, No. 1, pp. 307–319.'
  id: totrans-88
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 'Kim, K. (2003): “使用支持向量机进行金融时间序列预测。” *神经计算* , 第 55 卷, 第 1 期, 第 307–319 页。'
- en: 'Krauss, C., X. Do, and N. Huck (2017): “Deep neural networks, gradient-boosted
    trees, random forests: Statistical arbitrage on the S&P 500.” *European Journal
    of Operational Research* , Vol. 259, No. 2, pp. 689–702.'
  id: totrans-89
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 'Krauss, C.、X. Do 和 N. Huck (2017): “深度神经网络、梯度增强树、随机森林：对标普500的统计套利。” *欧洲运筹学杂志*
    , 第 259 卷, 第 2 期, 第 689–702 页。'
- en: 'Laborda, R. and J. Laborda (2017): “Can tree-structured classifiers add value
    to the investor?” *Finance Research Letters* , Vol. 22 (August), pp. 211–226.'
  id: totrans-90
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 'Laborda, R. 和 J. Laborda (2017): “树结构分类器能为投资者增值吗？” *金融研究快报* , 第 22 卷 (8 月),
    第 211–226 页。'
- en: 'Nakamura, E. (2005): “Inflation forecasting using a neural network.” *Economics
    Letters* , Vol. 86, No. 3, pp. 373–378.'
  id: totrans-91
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 'Nakamura, E. (2005): “使用神经网络进行通货膨胀预测。” *经济学信函* , 第 86 卷, 第 3 期, 第 373–378 页。'
- en: 'Olson, D. and C. Mossman (2003): “Neural network forecasts of Canadian stock
    returns using accounting ratios.” *International Journal of Forecasting* , Vol.
    19, No. 3, pp. 453–465.'
  id: totrans-92
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 'Olson, D. 和 C. Mossman (2003): “使用会计比率的加拿大股票收益的神经网络预测。” *国际预测杂志* , 第 19 卷,
    第 3 期, 第 453–465 页。'
- en: 'Patel, J., S. Sha, P. Thakkar, and K. Kotecha (2015): “Predicting stock and
    stock price index movement using trend deterministic data preparation and machine
    learning techniques.” *Expert Systems with Applications* , Vol. 42, No. 1, pp.
    259–268.'
  id: totrans-93
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 'Patel, J.、S. Sha、P. Thakkar 和 K. Kotecha (2015): “使用趋势确定数据准备和机器学习技术预测股票及股票价格指数的变动。”
    *专家系统与应用* , 第 42 卷, 第 1 期, 第 259–268 页。'
- en: 'Patel, J., S. Sha, P. Thakkar, and K. Kotecha (2015): “Predicting stock market
    index using fusion of machine learning techniques.” *Expert Systems with Applications*
    , Vol. 42, No. 4, pp. 2162–2172.'
  id: totrans-94
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 'Patel, J.、S. Sha、P. Thakkar 和 K. Kotecha (2015): “使用机器学习技术的融合预测股票市场指数。” *专家系统与应用*
    , 第 42 卷, 第 4 期, 第 2162–2172 页。'
- en: 'Qin, Q., Q. Wang, J. Li, and S. Shuzhi (2013): “Linear and nonlinear trading
    models with gradient boosted random forests and application to Singapore Stock
    Market.” *Journal of Intelligent Learning Systems and Applications* , Vol. 5,
    No. 1, pp. 1–10.'
  id: totrans-95
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 'Qin, Q.、Q. Wang、J. Li 和 S. Shuzhi (2013): “线性与非线性交易模型与梯度增强随机森林及其在新加坡股市的应用。”
    *智能学习系统与应用杂志* , 第 5 卷, 第 1 期, 第 1–10 页。'
- en: 'Sorensen, E., K. Miller, and C. Ooi (2000): “The decision tree approach to
    stock selection.” *Journal of Portfolio Management* , Vol. 27, No. 1, pp. 42–52.'
  id: totrans-96
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 'Sorensen, E.、K. Miller 和 C. Ooi (2000): “股票选择的决策树方法。” *投资组合管理杂志* , 第 27 卷,
    第 1 期, 第 42–52 页。'
- en: 'Theofilatos, K., S. Likothanassis, and A. Karathanasopoulos (2012): “Modeling
    and trading the EUR/USD exchange rate using machine learning techniques.” *Engineering,
    Technology & Applied Science Research* , Vol. 2, No. 5, pp. 269–272.'
  id: totrans-97
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 'Theofilatos, K.、S. Likothanassis 和 A. Karathanasopoulos (2012): “使用机器学习技术建模和交易欧元/美元汇率。”
    *工程、技术与应用科学研究* , 第 2 卷, 第 5 期, 第 269–272 页。'
- en: 'Trafalis, T. and H. Ince (2000): “Support vector machine for regression and
    applications to financial forecasting.” *Neural Networks* , Vol. 6, No. 1, pp.
    348–353.'
  id: totrans-98
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 'Trafalis, T. 和 H. Ince (2000): “回归的支持向量机及其在金融预测中的应用。” *神经网络* , 第 6 卷, 第 1 期,
    第 348–353 页。'
- en: 'Trippi, R. and D. DeSieno (1992): “Trading equity index futures with a neural
    network.” *Journal of Portfolio Management* , Vol. 19, No. 1, pp. 27–33.'
  id: totrans-99
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 'Trippi, R. 和 D. DeSieno (1992): “使用神经网络进行股票指数期货交易。” *投资组合管理杂志* , 第 19 卷, 第
    1 期, 第 27–33 页。'
- en: 'Tsai, C. and S. Wang (2009): “Stock price forecasting by hybrid machine learning
    techniques.” *Proceedings of the International Multi-Conference of Engineers and
    Computer Scientists* , Vol. 1, No. 1, pp. 755–760.'
  id: totrans-100
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 'Tsai, C. 和 S. Wang (2009): “通过混合机器学习技术进行股票价格预测。” *国际工程师和计算机科学家多学科会议录* , 第 1
    卷, 第 1 期, 第 755–760 页。'
- en: 'Tsai, C., Y. Lin, D. Yen, and Y. Chen (2011): “Predicting stock returns by
    classifier ensembles.” *Applied Soft Computing* , Vol. 11, No. 2, pp. 2452–2459.'
  id: totrans-101
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Tsai, C., Y. Lin, D. Yen 和 Y. Chen (2011)： “通过分类器集预测股票收益。” *应用软计算*，第11卷，第2期，页2452–2459。
- en: 'Wang, J. and S. Chan (2006): “Stock market trading rule discovery using two-layer
    bias decision tree.” *Expert Systems with Applications* , Vol. 30, No. 4, pp.
    605–611.'
  id: totrans-102
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Wang, J. 和 S. Chan (2006)： “使用双层偏差决策树发现股票市场交易规则。” *专家系统与应用*，第30卷，第4期，页605–611。
- en: 'Wang, Q., J. Li, Q. Qin, and S. Ge (2011): “Linear, adaptive and nonlinear
    trading models for Singapore Stock Market with random forests.” Proceedings of
    the 9th IEEE International Conference on Control and Automation, pp. 726–731.'
  id: totrans-103
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Wang, Q., J. Li, Q. Qin 和 S. Ge (2011)： “用于新加坡股票市场的线性、自适应和非线性交易模型，采用随机森林。” 第9届IEEE国际控制与自动化会议论文集，页726–731。
- en: 'Wei, P. and N. Wang (2016): “Wikipedia and stock return: Wikipedia usage pattern
    helps to predict the individual stock movement.” Proceedings of the 25th International
    Conference Companion on World Wide Web, Vol. 1, pp. 591–594.'
  id: totrans-104
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Wei, P. 和 N. Wang (2016)： “维基百科与股票收益：维基百科使用模式有助于预测个别股票的走势。” 第25届国际互联网大会会议论文集，第1卷，页591–594。
- en: '![](Image00642.jpg) bikowski, K. (2015): “Using volume weighted support vector
    machines with walk forward testing and feature selection for the purpose of creating
    stock trading strategy.” *Expert Systems with Applications* , Vol. 42, No. 4,
    pp. 1797–1805.'
  id: totrans-105
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '![](Image00642.jpg) bikowski, K. (2015)： “使用体积加权支持向量机与步进测试和特征选择创建股票交易策略。” *专家系统与应用*，第42卷，第4期，页1797–1805。'
- en: 'Zhang, G., B. Patuwo, and M. Hu (1998): “Forecasting with artificial neural
    networks: The state of the art.” *International Journal of Forecasting* , Vol.
    14, No. 1, pp. 35–62.'
  id: totrans-106
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Zhang, G., B. Patuwo 和 M. Hu (1998)： “使用人工神经网络进行预测：最新进展。” *国际预测杂志*，第14卷，第1期，页35–62。
- en: 'Zhu, M., D. Philpotts and M. Stevenson (2012): “The benefits of tree-based
    models for stock selection.” *Journal of Asset Management* , Vol. 13, No. 6, pp.
    437–448.'
  id: totrans-107
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Zhu, M., D. Philpotts 和 M. Stevenson (2012)： “基于树的模型在股票选择中的好处。” *资产管理杂志*，第13卷，第6期，页437–448。
- en: 'Zhu, M., D. Philpotts, R. Sparks, and J. Stevenson, Maxwell (2011): “A hybrid
    approach to combining CART and logistic regression for stock ranking.” *Journal
    of Portfolio Management* , Vol. 38, No. 1, pp. 100–109.'
  id: totrans-108
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Zhu, M., D. Philpotts, R. Sparks 和 J. Stevenson, Maxwell (2011)： “结合CART和逻辑回归的混合方法用于股票排名。”
    *投资组合管理杂志*，第38卷，第1期，页100–109。
- en: '**Note**'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: '**备注**'
- en: ^([1](text00001.html#filepos0000262020))    You are probably aware of at least
    one large hedge fund that monitors the emotional state of their research analysts
    on a daily basis.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: ^([1](text00001.html#filepos0000262020))    你可能知道至少有一家大型对冲基金每天监测其研究分析师的情绪状态。
- en: '**CHAPTER 4**'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: '**第4章**'
- en: '**Sample Weights**'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: '**样本权重**'
- en: '**4.1 Motivation**'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: '**4.1 动机**'
- en: Chapter 3 presented several new methods for labeling financial observations.
    We introduced two novel concepts, the triple-barrier method and meta-labeling,
    and explained how they are useful in financial applications, including quantamental
    investment strategies. In this chapter you will learn how to use sample weights
    to address another problem ubiquitous in financial applications, namely that observations
    are not generated by independent and identically distributed (IID) processes.
    Most of the ML literature is based on the IID assumption, and one reason many
    ML applications fail in finance is because those assumptions are unrealistic in
    the case of financial time series.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 第3章介绍了几种标记金融观察的新方法。我们引入了两个新概念，三重障碍法和元标记法，并解释了它们在金融应用中的作用，包括量化投资策略。在本章中，您将学习如何使用样本权重来解决另一个在金融应用中普遍存在的问题，即观察并非由独立同分布（IID）过程生成。大多数机器学习文献基于IID假设，许多机器学习应用在金融中失败的原因之一是这些假设在金融时间序列的情况下不切实际。
- en: '**4.2 Overlapping Outcomes**'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: '**4.2 重叠结果**'
- en: In Chapter 3 we assigned a label *y [*i*]* to an observed feature *X [*i*]*
    , where *y [*i*]* was a function of price bars that occurred over an interval
    [ *t [*i* , 0]* , *t [*i* , 1]* ]. When *t [*i* , 1]* > *t [*j* , 0]* and *i*
    < *j* , then *y [*i*]* and *y [*j*]* will both depend on a common return ![](Image00644.jpg)
    , that is, the return over the interval [ *t [*j*  , 0] * , min{ *t [*i*  , 1]
    * , *t [*j*  , 1] * }]. The implication is that the series of labels, { *y [*i*]
    * } [*i*  = 1, …,  *I*] , are not IID whenever there is an overlap between any
    two consecutive outcomes, ∃ *i* | *t [*i*  , 1] * > *t [*i*  + 1, 0] * ..
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 在第三章中，我们给观察到的特征 *X [*i*]* 指定了一个标签 *y [*i*]*，其中 *y [*i*]* 是发生在区间 [ *t [*i* ,
    0]* , *t [*i* , 1]* ] 的价格条的函数。当 *t [*i* , 1]* > *t [*j* , 0]* 且 *i* < *j* 时，*y
    [*i*]* 和 *y [*j*]* 都将依赖于一个共同的收益 ![](Image00644.jpg)，即在区间 [ *t [*j* , 0]* , min{
    *t [*i* , 1]* , *t [*j* , 1]* } ] 上的收益。这意味着，标签系列 { *y [*i*]* } [*i* = 1, …, *I*]
    在任意两个连续结果之间存在重叠时，均不满足独立同分布（IID）条件，∃ *i* | *t [*i* , 1]* > *t [*i* + 1, 0]* ..
- en: Suppose that we circumvent this problem by restricting the bet horizon to *t
    [*i* , 1]* ≤ *t [*i* + 1, 0]* . In this case there is no overlap, because every
    feature outcome is determined before or at the onset of the next observed feature.
    That would lead to coarse models where the features’ sampling frequency would
    be limited by the horizon used to determine the outcome. On one hand, if we wished
    to investigate outcomes that lasted a month, features would have to be sampled
    with a frequency up to monthly. On the other hand, if we increased the sampling
    frequency to let's say daily, we would be forced to reduce the outcome's horizon
    to one day. Furthermore, if we wished to apply a path-dependent labeling technique,
    like the triple-barrier method, the sampling frequency would be subordinated to
    the first barrier's touch. No matter what you do, restricting the outcome's horizon
    to eliminate overlaps is a terrible solution. We must allow *t [*i* , 1]* > *t
    [*i* + 1, 0]* , which brings us back to the problem of overlapping outcomes described
    earlier.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 假设我们通过将投注时间限制为 *t [*i* , 1]* ≤ *t [*i* + 1, 0]* 来规避这个问题。在这种情况下，没有重叠，因为每个特征的结果在下一个观察到的特征开始之前或开始时就已经确定。这将导致粗糙的模型，其中特征的采样频率将受到用于确定结果的时间范围的限制。一方面，如果我们希望研究持续一个月的结果，特征就必须以每月的频率进行采样。另一方面，如果我们将采样频率提高到每日，那么我们将被迫将结果的时间范围缩短为一天。此外，如果我们希望应用路径依赖的标签技术，比如三重障碍法，采样频率将从属于第一个障碍的触及。无论你做什么，将结果的时间范围限制以消除重叠都是一个糟糕的解决方案。我们必须允许
    *t [*i* , 1]* > *t [*i* + 1, 0]*，这又让我们回到了之前描述的重叠结果的问题。
- en: 'This situation is characteristic of financial applications. Most non-financial
    ML researchers can assume that observations are drawn from IID processes. For
    example, you can obtain blood samples from a large number of patients, and measure
    their cholesterol. Of course, various underlying common factors will shift the
    mean and standard deviation of the cholesterol distribution, but the samples are
    still independent: There is one observation per subject. Suppose you take those
    blood samples, and someone in your laboratory spills blood from each tube into
    the following nine tubes to their right. That is, tube 10 contains blood for patient
    10, but also blood from patients 1 through 9\. Tube 11 contains blood from patient
    11, but also blood from patients 2 through 10, and so on. Now you need to determine
    the features predictive of high cholesterol (diet, exercise, age, etc.), without
    knowing for sure the cholesterol level of each patient. That is the equivalent
    challenge that we face in financial ML, with the additional handicap that the
    spillage pattern is non-deterministic and unknown. Finance is not a plug-and-play
    subject as it relates to ML applications. Anyone who tells you otherwise will
    waste your time and money.'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 这种情况在金融应用中是典型的。大多数非金融的机器学习研究人员可以假设观察结果来自独立同分布的过程。例如，你可以从大量患者中提取血样并测量他们的胆固醇。当然，各种潜在的共同因素会影响胆固醇分布的均值和标准差，但样本仍然是独立的：每个受试者只有一个观察结果。假设你取这些血样，并且实验室里有人把每个试管的血液倒入右侧的九个试管中。也就是说，试管10包含患者10的血液，但也包含患者1到9的血液。试管11包含患者11的血液，但也包含患者2到10的血液，依此类推。现在你需要确定能够预测高胆固醇的特征（饮食、运动、年龄等），而无法确切知道每个患者的胆固醇水平。这正是我们在金融机器学习中面临的挑战，且额外的障碍在于溢出模式是非确定性的且未知的。金融与机器学习应用并不是一种即插即用的主题。任何告诉你相反的人都会浪费你的时间和金钱。
- en: There are several ways to attack the problem of non-IID labels, and in this
    chapter we will address it by designing sampling and weighting schemes that correct
    for the undue influence of overlapping outcomes.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 解决非 IID 标签问题有几种方法，本章将通过设计采样和加权方案来应对这一问题，以纠正重叠结果的不当影响。
- en: '**4.3 Number of Concurrent Labels**'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: '**4.3 并发标签的数量**'
- en: Two labels *y [*i*]* and *y [*j*]* are concurrent at *t* when both are a function
    of at least one common return, ![](Image00112.jpg) *.* The overlap does not need
    to be perfect, in the sense of both labels spanning the same time interval. In
    this section we are going to compute the number of labels that are a function
    of a given return, *r [*t*  − 1,  *t*] * . First, for each time point *t* = 1,
    …, *T* , we form a binary array, {1 [*t*  ,  *i*] } [*i*  = 1, …,  *I*] , where
    1 [*t*  ,  *i*] ∈ {0, 1}. Variable 1 [*t*  ,  *i*] = 1 if and only if [ *t [*i*  ,
    0] * , *t [*i*  , 1] * ] overlaps with [ *t* − 1, *t* ] and 1 [*t*  ,  *i*] =
    0 otherwise. Recall that the labels’ spans {[ *t [*i*  , 0] * , *t [*i*  , 1]
    * ]} [*i*  = 1, …,  *I*] are defined by the `t1` object introduced in Chapter
    3\. Second, we compute the number of labels concurrent at *t* , ![](Image00672.jpg)
    . Snippet 4.1 illustrates an implementation of this logic.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 当两个标签 *y [*i*]* 和 *y [*j*]* 在 *t* 时都是至少一个共同回报的函数时，它们是并发的。![图像](Image00112.jpg)。重叠不需要是完美的，意味着两个标签跨越相同的时间区间。在这一部分，我们将计算给定回报
    *r [*t* − 1,  *t*]* 的标签数量。首先，对于每个时间点 *t* = 1, …, *T*，我们形成一个二进制数组，{1 [*t*  ,  *i*]}
    [*i*  = 1, …,  *I*]，其中 1 [*t*  ,  *i*] ∈ {0, 1}。变量 1 [*t*  ,  *i*] = 1 当且仅当 [*t
    [*i*  , 0] * , *t [*i*  , 1] *] 与 [*t* − 1, *t*] 重叠，且 1 [*t*  ,  *i*] = 0 否则。回想一下，标签的跨度
    {[ *t [*i*  , 0] * , *t [*i*  , 1] *]} [*i*  = 1, …,  *I*] 是由第三章中介绍的 `t1` 对象定义的。其次，我们计算在
    *t* 时并发的标签数量。![图像](Image00672.jpg)。代码片段 4.1 说明了此逻辑的实现。
- en: '**SNIPPET 4.1 ESTIMATING THE UNIQUENESS OF A LABEL**'
  id: totrans-122
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '**代码片段 4.1 估计标签的唯一性**'
- en: '![](Image00682.jpg)'
  id: totrans-123
  prefs:
  - PREF_BQ
  type: TYPE_IMG
  zh: '![图像](Image00682.jpg)'
- en: '**4.4 Average Uniqueness of a Label**'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: '**4.4 标签的平均唯一性**'
- en: In this section we are going to estimate a label's uniqueness (non-overlap)
    as its average uniqueness over its lifespan. First, the uniqueness of a label
    *i* at time *t* is *u [*t* , *i*]* = 1 [*t* , *i*] *c ^(− 1) [*t*]* . Second,
    the average uniqueness of label *i* is the average *u [*t* , *i*]* over the label's
    lifespan, ![](Image00698.jpg) . This average uniqueness can also be interpreted
    as the reciprocal of the harmonic average of *c [*t*] * over the event's lifespan.  [
    Figure 4.1 ](text00001.html#filepos0000290928) plots the histogram of uniqueness
    values derived from an object `t1` . Snippet 4.2 implements this calculation.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一部分，我们将估计标签的唯一性（非重叠性），作为其生命周期内的平均唯一性。首先，标签 *i* 在时间 *t* 的唯一性为 *u [*t* , *i*]*
    = 1 [*t* , *i*] *c ^(− 1) [*t*]*。其次，标签 *i* 的平均唯一性是标签生命周期内 *u [*t* , *i*]* 的平均值。![图像](Image00698.jpg)。这个平均唯一性也可以解释为事件生命周期内
    *c [*t*]* 的调和平均的倒数。[ **图 4.1**](text00001.html#filepos0000290928) 绘制了从对象 `t1`
    得出的唯一性值的直方图。代码片段 4.2 实现了此计算。
- en: '![](Image00714.jpg)'
  id: totrans-126
  prefs: []
  type: TYPE_IMG
  zh: '![图像](Image00714.jpg)'
- en: '[**Figure 4.1**](text00001.html#filepos0000290555) Histogram of uniqueness
    values'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: '[**图 4.1**](text00001.html#filepos0000290555) 唯一性值的直方图'
- en: '**SNIPPET 4.2 ESTIMATING THE AVERAGE UNIQUENESS OF A LABEL**'
  id: totrans-128
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '**代码片段 4.2 估计标签的平均唯一性**'
- en: '![](Image00732.jpg)'
  id: totrans-129
  prefs:
  - PREF_BQ
  type: TYPE_IMG
  zh: '![图像](Image00732.jpg)'
- en: Note that we are making use again of the function `mpPandasObj` , which speeds
    up calculations via multiprocessing (see Chapter 20). Computing the average uniqueness
    associated with label *i* , ![](Image00750.jpg) , requires information that is
    not available until a future time, `events['t1']` . This is not a problem, because
    ![](Image00769.jpg) are used on the training set in combination with label information,
    and not on the testing set. These ![](Image00788.jpg) are not used for forecasting
    the label, hence there is no information leakage. This procedure allows us to
    assign a uniqueness score between 0 and 1 for each observed feature, in terms
    of non-overlapping outcomes.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，我们再次使用了函数 `mpPandasObj`，它通过多处理加速计算（见第 20 章）。计算与标签 *i* 相关的平均唯一性，![图像](Image00750.jpg)，需要在未来时间
    `events['t1']` 可用的信息。这不是问题，因为![图像](Image00769.jpg) 在训练集上结合标签信息使用，而不在测试集上使用。这些![图像](Image00788.jpg)不用于预测标签，因此不存在信息泄漏。这个过程允许我们为每个观察到的特征分配一个在
    0 到 1 之间的唯一性评分，基于非重叠结果。
- en: '**4.5 Bagging Classifiers and Uniqueness**'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: '**4.5 装袋分类器和唯一性**'
- en: The probability of not selecting a particular item *i* after *I* draws with
    replacement on a set of *I* items is (1 − *I ^(− 1)* ) ^(*I*) . As the sample
    size grows, that probability converges to the asymptotic value ![](Image00807.jpg)
    . That means that the number of unique observations drawn is expected to be ![](Image00824.jpg)
    .
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 在对*I*个项目进行*I*次有放回抽样后，未选择特定项目*i*的概率为(1 − *I ^(− 1)* ) ^(*I*)。随着样本量的增加，该概率收敛于渐近值![](Image00807.jpg)。这意味着抽取的独特观察数量预计为![](Image00824.jpg)。
- en: Suppose that the maximum number of non-overlapping outcomes is *K* ≤ *I* . Following
    the same argument, the probability of not selecting a particular item *i* after
    *I* draws with replacement on a set of *I* items is (1 − *K ^(− 1)* ) ^(*I*) .
    As the sample size grows, that probability can be approximated as ![](Image00499.jpg)
    . That means that the number of unique observations drawn is expected to be ![](Image00601.jpg)
    . The implication is that incorrectly assuming IID draws leads to oversampling.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 假设最大的不重叠结果数量为*K* ≤ *I*。根据同样的论点，在对*I*个项目进行*I*次有放回抽样后，未选择特定项目*i*的概率为(1 − *K ^(−
    1)* ) ^(*I*)。随着样本量的增加，该概率可以近似为![](Image00499.jpg)。这意味着抽取的独特观察数量预计为![](Image00601.jpg)。这暗示着错误地假设独立同分布抽样会导致过采样。
- en: When sampling with replacement (bootstrap) on observations with ![](Image00152.jpg)
    , it becomes increasingly likely that in-bag observations will be (1) redundant
    to each other, and (2) very similar to out-of-bag observations. Redundancy of
    draws makes the bootstrap inefficient (see Chapter 6). For example, in the case
    of a random forest, all trees in the forest will essentially be very similar copies
    of a single overfit decision tree. And because the random sampling makes out-of-bag
    examples very similar to the in-bag ones, out-of-bag accuracy will be grossly
    inflated. We will address this second issue in Chapter 7, when we study cross-validation
    under non-IID observations. For the moment, let us concentrate on the first issue,
    namely bagging under observations where ![](Image00773.jpg) *.*
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 在对具有![](Image00152.jpg)的观察进行有放回抽样（自助法）时，袋内观察相互（1）冗余且（2）与袋外观察非常相似的可能性越来越高。抽样的冗余性使得自助法效率低下（见第6章）。例如，在随机森林的情况下，森林中的所有树实际上都是单一过拟合决策树的非常相似的副本。由于随机抽样使得袋外样本与袋内样本非常相似，袋外准确率将被极大夸大。我们将在第7章中讨论这个第二个问题，当时我们将研究非独立同分布观察下的交叉验证。此刻，让我们集中于第一个问题，即在观察中进行装袋，当![](Image00773.jpg)
    *。*
- en: A first solution is to drop overlapping outcomes before performing the bootstrap.
    Because overlaps are not perfect, dropping an observation just because there is
    a partial overlap will result in an extreme loss of information. I do not advise
    you to follow this solution.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 第一个解决方案是在执行自助法之前删除重叠结果。由于重叠并不完美，仅仅因为部分重叠而删除观察将导致信息的极大损失。我不建议您遵循这个解决方案。
- en: A second and better solution is to utilize the average uniqueness, ![](Image00010.jpg)
    , to reduce the undue influence of outcomes that contain redundant information.
    Accordingly, we could sample only a fraction `out['tW'].mean()` of the observations,
    or a small multiple of that. In sklearn, the `sklearn.ensemble.BaggingClassifier`
    class accepts an argument `max_samples` , which can be set to `max_samples=out['tW'].mean()`
    . In this way, we enforce that the in-bag observations are not sampled at a frequency
    much higher than their uniqueness. Random forests do not offer that `max_samples`
    functionality, however, a solution is to bag a large number of decision trees.
    We will discuss this solution further in Chapter 6.
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 第二个更好的解决方案是利用平均独特性![](Image00010.jpg)，以减少包含冗余信息结果的不当影响。因此，我们可以仅抽取观察的一部分`out['tW'].mean()`，或者是其小倍数。在sklearn中，`sklearn.ensemble.BaggingClassifier`类接受一个参数`max_samples`，可以设置为`max_samples=out['tW'].mean()`。这样，我们确保袋内观察的抽样频率不会远高于其独特性。然而，随机森林并不提供该`max_samples`功能，解决方案是装袋大量决策树。我们将在第6章进一步讨论此解决方案。
- en: '**4.5.1 Sequential Bootstrap**'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: '**4.5.1 顺序自助法**'
- en: A third and better solution is to perform a sequential bootstrap, where draws
    are made according to a changing probability that controls for redundancy. Rao
    et al. [1997] propose sequential resampling with replacement until *K* distinct
    original observations appear. Although interesting, their scheme does not fully
    apply to our financial problem. In the following sections we introduce an alternative
    method that addresses directly the problem of overlapping outcomes.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 第三种更好的解决方案是执行顺序自助法，根据控制冗余的变化概率进行抽取。Rao等人[1997]提出了在出现 *K* 个不同原始观察值之前进行有替换的顺序重抽样。尽管有趣，但他们的方案并不完全适用于我们的金融问题。在接下来的部分中，我们将介绍一种直接解决重叠结果问题的替代方法。
- en: First, an observation *X [*i*]* is drawn from a uniform distribution, *i* ∼
    *U* [1, *I* ], that is, the probability of drawing any particular value *i* is
    originally δ ^((1)) [*i*] = *I ^(− 1)* . For the second draw, we wish to reduce
    the probability of drawing an observation *X [*j*]* with a highly overlapping
    outcome. Remember, a bootstrap allows sampling with repetition, so it is still
    possible to draw *X [*i*]* again, but we wish to reduce its likelihood, since
    there is an overlap (in fact, a perfect overlap) between *X [*i*]* and itself.
    Let us denote as φ the sequence of draws so far, which may include repetitions.
    Until now, we know that φ ^((1)) = { *i* }. The uniqueness of *j* at time *t*
    is ![](Image00160.jpg) , as that is the uniqueness that results from adding alternative
    *j* ’s to the existing sequence of draws φ ^((1)) . The average uniqueness of
    *j* is the average *u ^((2)) [   *t*  ,  *j*   ] * over *j* ’s lifespan, ![](Image00191.jpg)
    . We can now make a second draw based on the updated probabilities {δ ^((2)) [   *j*   ]
    } [*j*  = 1, …,  *I*] ,
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，从均匀分布中抽取一个观察值 *X [*i*]*，*i* ∼ *U* [1, *I*]，即抽取任何特定值 *i* 的概率最初为 δ ^((1)) [*i*]
    = *I ^(− 1)*。在第二次抽取中，我们希望减少抽取高度重叠结果的观察值 *X [*j*]* 的概率。请记住，自助法允许重复抽样，因此仍然可以再次抽取
    *X [*i*]*，但我们希望降低其可能性，因为 *X [*i*]* 和自身之间存在重叠（实际上是完全重叠）。我们将到目前为止的抽样序列表示为 φ，可能包括重复项。到现在为止，我们知道
    φ ^((1)) = { *i* }。在时间 *t* 时 *j* 的唯一性是 ![](Image00160.jpg)，因为这是通过向现有的抽样序列 φ ^((1))
    添加替代 *j* 得到的唯一性。*j* 的平均唯一性是 *j* 生命周期内的平均 *u ^((2)) [*t*，*j*]*，![](Image00191.jpg)。现在我们可以根据更新后的概率
    {δ ^((2)) [*j*]} [*j* = 1, …, *I*] 进行第二次抽取，
- en: '![](Image00165.jpg)'
  id: totrans-140
  prefs: []
  type: TYPE_IMG
  zh: '![](Image00165.jpg)'
- en: where {δ ^((2)) [*j*] } [*j* = 1, .., *I*] are scaled to add up to 1, ![](Image00135.jpg)
    *.* We can now do a second draw, update φ ^((2)) and re-evaluate {δ ^((3)) [   *j*   ]
    } [*j*  = 1, …,  *I*] . The process is repeated until *I* draws have taken place.
    This sequential bootstrap scheme has the advantage that overlaps (even repetitions)
    are still possible, but decreasingly likely. The sequential bootstrap sample will
    be much closer to IID than samples drawn from the standard bootstrap method. This
    can be verified by measuring an increase in ![](Image00153.jpg) , relative to
    the standard bootstrap method.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 {δ ^((2)) [*j*]} [*j* = 1, .., *I*] 被缩放为加起来为1，![](Image00135.jpg) *.* 现在我们可以进行第二次抽取，更新
    φ ^((2)) 并重新评估 {δ ^((3)) [*j*]} [*j* = 1, …, *I*]。该过程重复进行，直到进行 *I* 次抽取。这个顺序自助法方案的优点是仍然可能出现重叠（甚至重复），但可能性逐渐降低。顺序自助样本将比从标准自助法中抽取的样本更接近独立同分布（IID）。可以通过测量相对于标准自助法的
    ![](Image00153.jpg) 的增加来验证这一点。
- en: '**4.5.2 Implementation of Sequential Bootstrap**'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: '**4.5.2 顺序自助法的实现**'
- en: 'Snippet 4.3 derives an indicator matrix from two arguments: the index of bars
    ( `barIx` ), and the pandas Series `t1` , which we used multiple times in Chapter
    3\. As a reminder, `t1` is defined by an index containing the time at which the
    features are observed, and a values array containing the time at which the label
    is determined. The output of this function is a binary matrix indicating what
    (price) bars influence the label for each observation.'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 片段4.3根据两个参数推导出一个指示矩阵：柱子的索引（`barIx`）和 pandas Series `t1`，我们在第3章中多次使用。提醒一下，`t1`的定义是一个包含特征观察时间的索引和一个包含标签确定时间的值数组。该函数的输出是一个二进制矩阵，指示哪些（价格）柱子影响每个观察的标签。
- en: '**SNIPPET 4.3 BUILD AN INDICATOR MATRIX**'
  id: totrans-144
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '**片段4.3 构建指示矩阵**'
- en: '![](Image00170.jpg)'
  id: totrans-145
  prefs:
  - PREF_BQ
  type: TYPE_IMG
  zh: '![](Image00170.jpg)'
- en: Snippet 4.4 returns the average uniqueness of each observed feature. The input
    is the indicator matrix built by `getIndMatrix` .
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 片段4.4返回每个观察特征的平均唯一性。输入是通过 `getIndMatrix` 构建的指示矩阵。
- en: '**SNIPPET 4.4 COMPUTE AVERAGE UNIQUENESS**'
  id: totrans-147
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '**片段4.4 计算平均唯一性**'
- en: '![](Image00629.jpg)'
  id: totrans-148
  prefs:
  - PREF_BQ
  type: TYPE_IMG
  zh: '![](Image00629.jpg)'
- en: Snippet 4.5 gives us the index of the features sampled by sequential bootstrap.
    The inputs are the indicator matrix ( `indM` ) and an optional sample length (
    `sLength` ), with a default value of as many draws as rows in `indM` .
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 片段4.5为我们提供了通过顺序自助抽样的特征索引。输入是指示矩阵（`indM`）和一个可选的样本长度（`sLength`），其默认值为与`indM`中的行数相同的抽样次数。
- en: '**SNIPPET 4.5 RETURN SAMPLE FROM SEQUENTIAL BOOTSTRAP**'
  id: totrans-150
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '**片段4.5 从顺序自助法返回样本**'
- en: '![](Image00706.jpg)'
  id: totrans-151
  prefs:
  - PREF_BQ
  type: TYPE_IMG
  zh: '![](Image00706.jpg)'
- en: '**4.5.3 A Numerical Example**'
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: '**4.5.3 数值示例**'
- en: Consider a set of labels { *y [*i*]* } [*i* = 1, 2, 3] , where label *y [1]*
    is a function of return *r [0, 3]* , label *y [2]* is a function of return *r
    [2, 4]* and label *y [3]* is a function of return *r [4, 6]* . The outcomes’ overlaps
    are characterized by this indicator matrix {1 [*t* , *i*] },
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑一组标签{ *y [*i*]* } [*i* = 1, 2, 3]，其中标签*y [1]*是回报*r [0, 3]*的函数，标签*y [2]*是回报*r
    [2, 4]*的函数，标签*y [3]*是回报*r [4, 6]*的函数。结果的重叠由此指示矩阵{1 [*t* , *i*]}表征，
- en: '![](Image00183.jpg)'
  id: totrans-154
  prefs: []
  type: TYPE_IMG
  zh: '![](Image00183.jpg)'
- en: 'The procedure starts with φ ^((0)) = ∅, and a uniform distribution of probability,
    ![](Image00186.jpg) , ∀ *i* = 1, 2, 3 *.* Suppose that we randomly draw a number
    from {1, 2, 3}, and 2 is selected. Before we make a second draw on {1, 2, 3} (remember,
    a bootstrap samples with repetition), we need to adjust the probabilities. The
    set of observations drawn so far is φ ^((1)) = {2}. The average uniqueness for
    the first feature is ![](Image00266.jpg) , and for the second feature is ![](Image00194.jpg)
    *.* The probabilities for the second draw are ![](Image00199.jpg) . Two points
    are worth mentioning: (1) The lowest probability goes to the feature that was
    picked in the first draw, as that would exhibit the highest overlap; and (2) among
    the two possible draws outside φ ^((1)) , the greater probability goes to δ ^((2))
    [  3  ] , as that is the label with no overlap to φ ^((1)) . Suppose that the
    second draw selects number 3\. We leave as an exercise the update of the probabilities
    δ ^((3)) for the third and final draw. Snippet 4.6 runs a sequential bootstrap
    on the {1 [*t*  ,  *i*] } indicator matrix in this example.'
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 该程序以φ ^((0)) = ∅开始，概率均匀分布为![](Image00186.jpg)，∀ *i* = 1, 2, 3 *.* 假设我们从{1, 2,
    3}中随机抽取一个数字，选择了2。在我们对{1, 2, 3}进行第二次抽样之前（请记住，自助抽样是带重复的），我们需要调整概率。迄今为止抽取的观察集为φ ^((1))
    = {2}。第一个特征的平均独特性为![](Image00266.jpg)，第二个特征为![](Image00194.jpg) *.* 第二次抽样的概率为![](Image00199.jpg)。有两点值得注意：(1)
    最低概率赋予首次抽样中被选中的特征，因为这将表现出最高的重叠；(2) 在φ ^((1))外的两个可能抽样中，较大的概率赋予δ ^((2)) [  3  ]，因为这是与φ
    ^((1))没有重叠的标签。假设第二次抽样选择了数字3。我们留作练习更新第三次和最后一次抽样的概率δ ^((3))。片段4.6在此示例中对{1 [*t* ,
    *i*]}指示矩阵进行顺序自助抽样。
- en: '**SNIPPET 4.6 EXAMPLE OF SEQUENTIAL BOOTSTRAP**'
  id: totrans-156
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '**片段4.6 顺序自助法示例**'
- en: '![](Image00319.jpg)'
  id: totrans-157
  prefs:
  - PREF_BQ
  type: TYPE_IMG
  zh: '![](Image00319.jpg)'
- en: '**4.5.4 Monte Carlo Experiments**'
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: '**4.5.4 蒙特卡洛实验**'
- en: We can evaluate the efficiency of the sequential bootstrap algorithm through
    experimental methods. Snippet 4.7 lists the function that generates a random `t1`
    series for a number of observations `numObs` ( *I* ). Each observation is made
    at a random number, drawn from a uniform distribution, with boundaries 0 and `numBars`
    , where `numBars` is the number of bars ( *T* ). The number of bars spanned by
    the observation is determined by drawing a random number from a uniform distribution
    with boundaries 0 and `maxH` .
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以通过实验方法评估顺序自助算法的效率。片段4.7列出了为多个观察值`numObs`（*I*）生成随机`t1`系列的函数。每个观察值是在从均匀分布中抽取的随机数下进行的，其边界为0和`numBars`，其中`numBars`是柱子的数量（*T*）。观察值跨度的柱子数量是通过从均匀分布中抽取边界为0和`maxH`的随机数来确定的。
- en: '**SNIPPET 4.7 GENERATING A RANDOM T1 SERIES**'
  id: totrans-160
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '**片段4.7 生成随机T1系列**'
- en: '![](Image00203.jpg)'
  id: totrans-161
  prefs:
  - PREF_BQ
  type: TYPE_IMG
  zh: '![](Image00203.jpg)'
- en: Snippet 4.8 takes that random `t1` series, and derives the implied indicator
    matrix, `indM` . This matrix is then subjected to two procedures. In the first
    one, we derive the average uniqueness from a standard bootstrap (random sampling
    with replacement). In the second one, we derive the average uniqueness by applying
    our sequential bootstrap algorithm. Results are reported as a dictionary.
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 片段4.8利用该随机`t1`系列推导出隐含的指示矩阵`indM`。然后，该矩阵经过两个程序。在第一个程序中，我们从标准自助法（带替换的随机抽样）中推导出平均独特性。在第二个程序中，我们通过应用顺序自助算法推导出平均独特性。结果以字典的形式报告。
- en: '**SNIPPET 4.8 UNIQUENESS FROM STANDARD AND SEQUENTIAL BOOTSTRAPS**'
  id: totrans-163
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '**片段4.8 从标准和顺序自助法中得出的独特性**'
- en: '![](Image00356.jpg)'
  id: totrans-164
  prefs:
  - PREF_BQ
  type: TYPE_IMG
  zh: '![](Image00356.jpg)'
- en: These operations have to be repeated over a large number of iterations. Snippet 4.9
    implements this Monte Carlo using the multiprocessing techniques discussed in
    Chapter 20\. For example, it will take about 6 hours for a 24-cores server to
    carry out a Monte Carlo of 1E6 iterations, where `numObs=10` , `numBars=100` ,
    and `maxH=5` . Without parallelization, a similar Monte Carlo experiment would
    have taken about 6 days.
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 这些操作必须在大量迭代中重复进行。片段 4.9 使用第20章讨论的多进程技术实现了这个蒙特卡罗实验。例如，使用24核服务器进行1E6次迭代的蒙特卡罗实验大约需要6小时，其中
    `numObs=10`，`numBars=100`，`maxH=5`。如果不并行化，类似的蒙特卡罗实验将需要约6天。
- en: '**SNIPPET 4.9 MULTI-THREADED MONTE CARLO**'
  id: totrans-166
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '**片段 4.9 多线程蒙特卡罗**'
- en: '![](Image00377.jpg)'
  id: totrans-167
  prefs:
  - PREF_BQ
  type: TYPE_IMG
  zh: '![](Image00377.jpg)'
- en: '[Figure 4.2](text00001.html#filepos0000310792) plots the histogram of the uniqueness
    from standard bootstrapped samples (left) and the sequentially bootstrapped samples
    (right). The median of the average uniqueness for the standard method is 0.6,
    and the median of the average uniqueness for the sequential method is 0.7\. An
    ANOVA test on the difference of means returns a vanishingly small probability.
    Statistically speaking, samples from the sequential bootstrap method have an expected
    uniqueness that exceeds that of the standard bootstrap method, at any reasonable
    confidence level.'
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: '[图 4.2](text00001.html#filepos0000310792)绘制了标准重采样样本（左）和顺序重采样样本（右）唯一性的直方图。标准方法的平均唯一性的中位数为0.6，顺序方法的平均唯一性的中位数为0.7。对均值差异的方差分析（ANOVA）测试返回的概率极小。从统计学角度来看，顺序重采样方法的样本在任何合理的置信水平下，其预期唯一性超过标准重采样方法。'
- en: '![](Image00392.jpg)'
  id: totrans-169
  prefs: []
  type: TYPE_IMG
  zh: '![](Image00392.jpg)'
- en: '[**Figure 4.2**](text00001.html#filepos0000310005) Monte Carlo experiment of
    standard vs. sequential bootstraps'
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: '[**图 4.2**](text00001.html#filepos0000310005)标准与顺序重采样的蒙特卡罗实验'
- en: '**4.6 Return Attribution**'
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: '**4.6 回报归因**'
- en: In the previous section we learned a method to bootstrap samples closer to IID.
    In this section we will introduce a method to weight those samples for the purpose
    of training an ML algorithm. Highly overlapping outcomes would have disproportionate
    weights if considered equal to non-overlapping outcomes. At the same time, labels
    associated with large absolute returns should be given more importance than labels
    with negligible absolute returns. In short, we need to weight observations by
    some function of both uniqueness and absolute return.
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 在前一节中，我们学习了一种将样本重采样得更接近IID的方法。在这一节中，我们将介绍一种为训练机器学习算法加权这些样本的方法。如果考虑到重叠结果与非重叠结果相等，重叠结果将具有不成比例的权重。同时，与大绝对回报相关的标签应比与微不足道的绝对回报相关的标签更重要。简而言之，我们需要通过唯一性和绝对回报的某种函数对观察进行加权。
- en: When labels are a function of the return sign ({ − 1, 1} for standard labeling
    or {0, 1} for meta-labeling), the sample weights can be defined in terms of the
    sum of the attributed returns over the event's lifespan, [ *t [*i* , 0]* , *t
    [*i* , 1]* ],
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 当标签是回报符号的函数（{−1, 1}用于标准标记或{0, 1}用于元标记）时，样本权重可以根据事件生命周期内归因回报的总和来定义，[ *t [*i*
    , 0]* , *t [*i* , 1]* ]。
- en: '![](Image00410.jpg)'
  id: totrans-174
  prefs: []
  type: TYPE_IMG
  zh: '![](Image00410.jpg)'
- en: '![](Image00220.jpg)'
  id: totrans-175
  prefs: []
  type: TYPE_IMG
  zh: '![](Image00220.jpg)'
- en: hence ![](Image00656.jpg) . We have scaled these weights to add up to *I* ,
    since libraries (including sklearn) usually define algorithmic parameters assuming
    a default weight of 1.
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 因此 ![](Image00656.jpg)。我们已将这些权重缩放为加起来等于 *I*，因为库（包括sklearn）通常假设算法参数的默认权重为1。
- en: The rationale for this method is that we wish to weight an observation as a
    function of the absolute log returns that can be attributed uniquely to it. However,
    this method will not work if there is a “neutral” (return below threshold) case.
    For that case, lower returns should be assigned higher weights, not the reciprocal.
    The “neutral” case is unnecessary, as it can be implied by a “−1” or “1” prediction
    with low confidence. This is one of several reasons I would generally advise you
    to drop “neutral” cases. Snippet 4.10 implements this method.
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 该方法的 rationale 是我们希望将观察加权为可以唯一归因于它的绝对对数回报的函数。然而，如果存在“中性”（低于阈值的回报）情况，则该方法将无效。在这种情况下，应给较低回报分配较高的权重，而不是其倒数。“中性”情况是多余的，因为它可以通过低置信度的“−1”或“1”预测来隐含。这是我一般建议你放弃“中性”情况的几个原因之一。片段
    4.10 实现了该方法。
- en: '**SNIPPET 4.10 DETERMINATION OF SAMPLE WEIGHT BY ABSOLUTE RETURN ATTRIBUTION**'
  id: totrans-178
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '**片段 4.10 通过绝对回报归因确定样本权重**'
- en: '![](Image00115.jpg)'
  id: totrans-179
  prefs:
  - PREF_BQ
  type: TYPE_IMG
  zh: '![](Image00115.jpg)'
- en: '**4.7 Time Decay**'
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: '**4.7 时间衰减**'
- en: 'Markets are adaptive systems (Lo [2017]). As markets evolve, older examples
    are less relevant than the newer ones. Consequently, we would typically like sample
    weights to decay as new observations arrive. Let ![](Image00119.jpg) be the time-decay
    factors that will multiply the sample weights derived in the previous section.
    The final weight has no decay, ![](Image00123.jpg) , and all other weights will
    be adjusted relative to that. Let *c* ∈ ( − 1., .1] be a user-defined parameter
    that determines the decay function as follows: For *c* ∈ [0, 1], then *d* [1]
    = *c* , with linear decay; for *c* ∈ ( − 1, 0), then ![](Image00125.jpg) , with
    linear decay between ![](Image00128.jpg) and *d* [ *x* ] = 0 ![](Image00130.jpg)
    . For a linear piecewise function *d* = max{0, *a* + *bx* }, such requirements
    are met by the following boundary conditions:'
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 市场是自适应系统（Lo [2017]）。随着市场的演变，旧例子相对于新例子而言相关性下降。因此，我们通常希望样本权重在新观察到来时衰减。设 ![](Image00119.jpg)
    为将乘以上一节中得出的样本权重的时间衰减因子。最终权重没有衰减，![](Image00123.jpg)，所有其他权重将相对调整。让 *c* ∈ (−1.,
    .1] 成为用户定义的参数，以决定衰减函数如下：对于 *c* ∈ [0, 1]，则 *d* [1] = *c*，线性衰减；对于 *c* ∈ (−1, 0)，则
    ![](Image00125.jpg)，在 ![](Image00128.jpg) 和 *d* [*x*] = 0 之间线性衰减！[](Image00130.jpg)。对于线性分段函数
    *d* = max{0, *a* + *bx*}，以下边界条件满足这些要求：
- en: '![](Image00134.jpg) .'
  id: totrans-182
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '![](Image00134.jpg)。'
- en: 'Contingent on c:'
  id: totrans-183
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 依赖于 *c*：
- en: '![](Image00137.jpg) , ∀*c* ∈ [0, 1]'
  id: totrans-184
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '![](Image00137.jpg)，∀*c* ∈ [0, 1]'
- en: '![](Image00140.jpg) , ∀*c* ∈ ( − 1, 0)'
  id: totrans-185
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_IMG
  zh: '![](Image00140.jpg)，∀*c* ∈ (−1, 0)'
- en: Snippet 4.11 implements this form of time-decay factors. Note that time is not
    meant to be chronological. In this implementation, decay takes place according
    to cumulative uniqueness, ![](Image00143.jpg) , because a chronological decay
    would reduce weights too fast in the presence of redundant observations.
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 片段 4.11 实现了这种时间衰减因子。注意，时间并不意味着是按时间顺序的。在这个实现中，衰减是根据累计独特性进行的，![](Image00143.jpg)，因为按时间顺序的衰减会在存在冗余观察时过快地降低权重。
- en: '**SNIPPET 4.11 IMPLEMENTATION OF TIME-DECAY FACTORS**'
  id: totrans-187
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '**片段 4.11 时间衰减因子的实现**'
- en: '![](Image00147.jpg)'
  id: totrans-188
  prefs:
  - PREF_BQ
  type: TYPE_IMG
  zh: '![](Image00147.jpg)'
- en: 'It is worth discussing a few interesting cases:'
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 值得讨论几个有趣的案例：
- en: '*c* = 1 means that there is no time decay.'
  id: totrans-190
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*c* = 1 表示没有时间衰减。'
- en: 0 < *c* < 1 means that weights decay linearly over time, but every observation
    still receives a strictly positive weight, regardless of how old.
  id: totrans-191
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 0 < *c* < 1 表示权重随时间线性衰减，但每个观察仍然会得到严格正的权重，无论它有多老。
- en: '*c* = 0 means that weights converge linearly to zero, as they become older.'
  id: totrans-192
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*c* = 0 意味着权重随着时间的推移线性收敛到零。'
- en: '*c* < 0 means that the oldest portion *cT* of the observations receive zero
    weight (i.e., they are erased from memory).'
  id: totrans-193
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*c* < 0 意味着观察中最老的部分 *cT* 得到零权重（即，它们从记忆中被抹去）。'
- en: '[Figure 4.3](text00001.html#filepos0000318296) shows the decayed weights, `out[''w'']*df`
    , after applying the decay factors for *c* ∈ {1, .75, .5, 0, −.25, −.5}. Although
    not necessarily practical, the procedure allows the possibility of generating
    weights that increase as they get older, by setting *c* > 1.'
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: '[图 4.3](text00001.html#filepos0000318296) 显示了在 *c* ∈ {1, .75, .5, 0, −.25,
    −.5} 的时间衰减因子应用后的衰减权重，`out[''w'']*df`。尽管不一定实用，但该过程允许生成随着时间变老而增加的权重，通过设置 *c* > 1。'
- en: '![](Image00150.jpg)'
  id: totrans-195
  prefs: []
  type: TYPE_IMG
  zh: '![](Image00150.jpg)'
- en: '[**Figure 4.3**](text00001.html#filepos0000317671) Piecewise-linear time-decay
    factors'
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: '[**图 4.3**](text00001.html#filepos0000317671) 分段线性时间衰减因子'
- en: '**4.8 Class Weights**'
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: '**4.8 类别权重**'
- en: In addition to sample weights, it is often useful to apply class weights. Class
    weights are weights that correct for underrepresented labels. This is particularly
    critical in classification problems where the most important classes have rare
    occurrences (King and Zeng [2001]). For example, suppose that you wish to predict
    liquidity crisis, like the flash crash of May 6, 2010\. These events are rare
    relative to the millions of observations that take place in between them. Unless
    we assign higher weights to the samples associated with those rare labels, the
    ML algorithm will maximize the accuracy of the most common labels, and flash crashes
    will be deemed to be outliers rather than rare events.
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 除了样本权重，应用类别权重通常也是有用的。类别权重是用于纠正欠代表标签的权重。这在分类问题中尤为重要，其中最重要的类别出现频率较低（King 和 Zeng
    [2001]）。例如，假设你希望预测流动性危机，如2010年5月6日的闪电崩盘。这些事件相对于其间发生的数百万次观察来说是罕见的。除非我们为与这些罕见标签相关的样本分配更高的权重，否则机器学习算法将最大化最常见标签的准确性，而将闪电崩盘视为异常值而非罕见事件。
- en: ML libraries typically implement functionality to handle class weights. For
    example, sklearn penalizes errors in samples of `class[j]` , *j=1,…,J* , with
    weighting `class_weight[j]` rather than 1\. Accordingly, higher class weights
    on label *j* will force the algorithm to achieve higher accuracy on *j* . When
    class weights do not add up to *J* , the effect is equivalent to changing the
    regularization parameter of the classifier.
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: ML 库通常实现处理类权重的功能。例如，sklearn 会对 `class[j]` 的样本中的错误进行惩罚，*j=1,…,J*，权重为 `class_weight[j]`
    而不是1。因此，标签 *j* 的更高类权重将迫使算法在 *j* 上实现更高的准确性。当类权重不加总为 *J* 时，效果相当于改变分类器的正则化参数。
- en: 'In financial applications, the standard labels of a classification algorithm
    are { − 1, 1}, where the zero (or neutral) case will be implied by a prediction
    with probability only slightly above 0.5 and below some neutral threshold. There
    is no reason for favoring accuracy of one class over the other, and as such a
    good default is to assign `class_weight=''balanced''` . This choice re-weights
    observations so as to simulate that all classes appeared with equal frequency.
    In the context of bagging classifiers, you may want to consider the argument `class_weight=''balanced_subsample''`
    , which means that `class_weight=''balanced''` will be applied to the in-bag bootstrapped
    samples, rather than to the entire dataset. For full details, it is helpful to
    read the source code implementing `class_weight` in sklearn. Please also be aware
    of this reported bug: [https://github.com/scikit-learn/scikit-learn/issues/4324](https://github.com/scikit-learn/scikit-learn/issues/4324)
    .'
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 在金融应用中，分类算法的标准标签为 { − 1, 1}，其中零（或中性）案例将由概率略高于0.5且低于某个中性阈值的预测暗示。没有理由偏向于一个类别的准确性，因此一个好的默认选择是将
    `class_weight='balanced'`。这个选择重新加权观察，以模拟所有类别以相等频率出现。在包外分类器的上下文中，你可能想考虑参数 `class_weight='balanced_subsample'`，这意味着
    `class_weight='balanced'` 将应用于包内自举样本，而不是整个数据集。有关完整细节，查看实现 `class_weight` 的 sklearn
    源代码是有帮助的。请注意这个报告的错误：[https://github.com/scikit-learn/scikit-learn/issues/4324](https://github.com/scikit-learn/scikit-learn/issues/4324)。
- en: '**Exercises**'
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: '**练习**'
- en: In Chapter 3, we denoted as `t1` a pandas series of timestamps where the first
    barrier was touched, and the index was the timestamp of the observation. This
    was the output of the `getEvents` function.
  id: totrans-202
  prefs:
  - PREF_OL
  - PREF_BQ
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 在第3章中，我们将第一次触及障碍的时间戳 pandas 系列表示为 `t1`，索引为观察的时间戳。这是 `getEvents` 函数的输出。
- en: Compute a `t1` series on dollar bars derived from E-mini S&P 500 futures tick
    data.
  id: totrans-203
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在从 E-mini S&P 500 期货的tick数据导出的美元条上计算 `t1` 系列。
- en: Apply the function `mpNumCoEvents` to compute the number of overlapping outcomes
    at each point in time.
  id: totrans-204
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 应用函数 `mpNumCoEvents` 来计算每个时间点的重叠结果数量。
- en: Plot the time series of the number of concurrent labels on the primary axis,
    and the time series of exponentially weighted moving standard deviation of returns
    on the secondary axis.
  id: totrans-205
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在主轴上绘制并发标签数量的时间序列，在次轴上绘制回报的指数加权移动标准差的时间序列。
- en: Produce a scatterplot of the number of concurrent labels (x-axis) and the exponentially
    weighted moving standard deviation of returns (y-axis). Can you appreciate a relationship?
  id: totrans-206
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 生成并发标签数量（x轴）与回报的指数加权移动标准差（y轴）的散点图。你能体会到一种关系吗？
- en: Using the function `mpSampleTW` , compute the average uniqueness of each label.
    What is the first-order serial correlation, AR(1), of this time series? Is it
    statistically significant? Why?
  id: totrans-207
  prefs:
  - PREF_OL
  - PREF_BQ
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 使用函数 `mpSampleTW`，计算每个标签的平均唯一性。这个时间序列的一阶序列相关性 AR(1) 是多少？它在统计上显著吗？为什么？
- en: Fit a random forest to a financial dataset where ![](Image00577.jpg) .
  id: totrans-208
  prefs:
  - PREF_OL
  - PREF_BQ
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 为包含 ![](Image00577.jpg) 的金融数据集拟合随机森林。
- en: What is the mean out-of-bag accuracy?
  id: totrans-209
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 什么是平均包外准确率？
- en: What is the mean accuracy of k-fold cross-validation (without shuffling) on
    the same dataset?
  id: totrans-210
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在同一数据集上，k折交叉验证（不打乱）的平均准确率是多少？
- en: Why is out-of-bag accuracy so much higher than cross-validation accuracy? Which
    one is more correct / less biased? What is the source of this bias?
  id: totrans-211
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 为什么包外准确率比交叉验证准确率高得多？哪个更正确/偏差更小？这种偏差的来源是什么？
- en: Modify the code in Section 4.7 to apply an exponential time-decay factor.
  id: totrans-212
  prefs:
  - PREF_OL
  - PREF_BQ
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 修改第4.7节中的代码以应用指数时间衰减因子。
- en: Consider you have applied meta-labels to events determined by a trend-following
    model. Suppose that two thirds of the labels are 0 and one third of the labels
    are 1.
  id: totrans-213
  prefs:
  - PREF_OL
  - PREF_BQ
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 考虑到你已经对由趋势跟踪模型确定的事件应用了元标签。假设三分之二的标签为0，三分之一的标签为1。
- en: What happens if you fit a classifier without balancing class weights?
  id: totrans-214
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如果在没有平衡类别权重的情况下拟合分类器，会发生什么？
- en: A label 1 means a true positive, and a label 0 means a false positive. By applying
    balanced class weights, we are forcing the classifier to pay more attention to
    the true positives, and less attention to the false positives. Why does that make
    sense?
  id: totrans-215
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 标签1表示真正例，标签0表示假正例。通过应用平衡类别权重，我们迫使分类器更关注真正例，减少对假正例的关注。这有什么意义？
- en: What is the distribution of the predicted labels, before and after applying
    balanced class weights?
  id: totrans-216
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在应用平衡类别权重之前和之后，预测标签的分布是什么？
- en: Update the draw probabilities for the final draw in Section 4.5.3.
  id: totrans-217
  prefs:
  - PREF_OL
  - PREF_BQ
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 更新4.5.3节最后一次抽取的概率。
- en: In Section 4.5.3, suppose that number 2 is picked again in the second draw.
    What would be the updated probabilities for the third draw?
  id: totrans-218
  prefs:
  - PREF_OL
  - PREF_BQ
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 在4.5.3节中，假设在第二次抽取中再次选择了数字2。第三次抽取的更新概率会是什么？
- en: '**References**'
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: '**参考文献**'
- en: 'Rao, C., P. Pathak and V. Koltchinskii (1997): “Bootstrap by sequential resampling.”
    *Journal of Statistical Planning and Inference* , Vol. 64, No. 2, pp. 257–281.'
  id: totrans-220
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Rao, C.，P. Pathak 和 V. Koltchinskii (1997)： “通过序贯重抽样进行自助法。” *统计规划与推断杂志* ，第64卷，第2期，第257–281页。
- en: 'King, G. and L. Zeng (2001): “Logistic Regression in Rare Events Data.” Working
    paper, Harvard University. Available at [https://gking.harvard.edu/files/0s.pdf](https://gking.harvard.edu/files/0s.pdf)
    .'
  id: totrans-221
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: King, G. 和 L. Zeng (2001)： “稀有事件数据中的逻辑回归。” 工作论文，哈佛大学。可在 [https://gking.harvard.edu/files/0s.pdf](https://gking.harvard.edu/files/0s.pdf)
    获得。
- en: 'Lo, A. (2017): *Adaptive Markets* , 1st ed. Princeton University Press.'
  id: totrans-222
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Lo, A. (2017)： *自适应市场* ，第1版。普林斯顿大学出版社。
- en: '**Bibliography**'
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: '**参考文献**'
- en: Sample weighting is a common topic in the ML learning literature. However the
    practical problems discussed in this chapter are characteristic of investment
    applications, for which the academic literature is extremely scarce. Below are
    some publications that tangentially touch some of the issues discussed in this
    chapter.
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: 样本加权是机器学习文献中的一个常见主题。然而，本章讨论的实际问题是投资应用的特征，相关的学术文献极为稀缺。以下是一些与本章讨论的一些问题间接相关的出版物。
- en: 'Efron, B. (1979): “Bootstrap methods: Another look at the jackknife.” *Annals
    of Statistics* , Vol. 7, pp. 1–26.'
  id: totrans-225
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Efron, B. (1979)： “自助法：对切片法的另一种看法。” *统计年鉴* ，第7卷，第1–26页。
- en: 'Efron, B. (1983): “Estimating the error rote of a prediction rule: Improvement
    on cross-validation.” *Journal of the American Statistical Association* , Vol.
    78, pp. 316–331.'
  id: totrans-226
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Efron, B. (1983)： “估计预测规则的错误率：交叉验证的改进。” *美国统计协会杂志* ，第78卷，第316–331页。
- en: 'Bickel, P. and D. Freedman (1981): “Some asymptotic theory for the bootstrap.”
    *Annals of Statistics* , Vol. 9, pp. 1196–1217.'
  id: totrans-227
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Bickel, P. 和 D. Freedman (1981)： “自助法的一些渐近理论。” *统计年鉴* ，第9卷，第1196–1217页。
- en: 'Gine, E. and J. Zinn (1990): “Bootstrapping general empirical measures.” *Annals
    of Probability* , Vol. 18, pp. 851–869.'
  id: totrans-228
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Gine, E. 和 J. Zinn (1990)： “引导一般经验度量。” *概率年鉴* ，第18卷，第851–869页。
- en: 'Hall, P. and E. Mammen (1994): “On general resampling algorithms and their
    performance in distribution estimation.” *Annals of Statistics* , Vol. 24, pp.
    2011–2030.'
  id: totrans-229
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Hall, P. 和 E. Mammen (1994)： “关于一般重抽样算法及其在分布估计中的表现。” *统计年鉴* ，第24卷，第2011–2030页。
- en: 'Mitra, S. and P. Pathak (1984): “The nature of simple random sampling.” *Annals
    of Statistics* , Vol. 12, pp. 1536–1542.'
  id: totrans-230
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Mitra, S. 和 P. Pathak (1984)： “简单随机抽样的性质。” *统计年鉴* ，第12卷，第1536–1542页。
- en: 'Pathak, P. (1964): “Sufficiency in sampling theory.” *Annals of Mathematical
    Statistics* , Vol. 35, pp. 795–808.'
  id: totrans-231
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Pathak, P. (1964)： “抽样理论中的充分性。” *数学统计年鉴* ，第35卷，第795–808页。
- en: 'Pathak, P. (1964): “On inverse sampling with unequal probabilities.” *Biometrika*
    , Vol. 51, pp. 185–193.'
  id: totrans-232
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Pathak, P. (1964)： “在不等概率下的逆抽样。” *生物统计* ，第51卷，第185–193页。
- en: 'Praestgaard, J. and J. Wellner (1993): “Exchangeably weighted bootstraps of
    the general empirical process.” *Annals of Probability* , Vol. 21, pp. 2053–2086.'
  id: totrans-233
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Praestgaard, J. 和 J. Wellner (1993)： “一般经验过程的可交换加权自助法。” *概率年鉴* ，第21卷，第2053–2086页。
- en: 'Rao, C., P. Pathak and V. Koltchinskii (1997): “Bootstrap by sequential resampling.”
    *Journal of Statistical Planning and Inference* , Vol. 64, No. 2, pp. 257–281.'
  id: totrans-234
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Rao, C.，P. Pathak 和 V. Koltchinskii (1997)： “通过序贯重抽样进行自助法。” *统计规划与推断杂志* ，第64卷，第2期，第257–281页。
- en: '**CHAPTER 5**'
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: '**第五章**'
- en: '**Fractionally Differentiated Features**'
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: '**分数差分特征**'
- en: '**5.1 Motivation**'
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: '**5.1 动机**'
- en: It is known that, as a consequence of arbitrage forces, financial series exhibit
    low signal-to-noise ratios (López de Prado [2015]). To make matters worse, standard
    stationarity transformations, like integer differentiation, further reduce that
    signal by removing memory. Price series have memory, because every value is dependent
    upon a long history of previous levels. In contrast, integer differentiated series,
    like returns, have a memory cut-off, in the sense that history is disregarded
    entirely after a finite sample window. Once stationarity transformations have
    wiped out all memory from the data, statisticians resort to complex mathematical
    techniques to extract whatever residual signal remains. Not surprisingly, applying
    these complex techniques on memory-erased series likely leads to false discoveries.
    In this chapter we introduce a data transformation method that ensures the stationarity
    of the data while preserving as much memory as possible.
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: 已知，由于套利力量的影响，金融序列表现出低信噪比（洛佩斯·德·普拉多[2015]）。更糟糕的是，标准平稳性变换，比如整数差分，进一步通过去除记忆降低了信号。价格序列具有记忆，因为每个值依赖于长期的历史水平。相比之下，像收益这样的整数差分序列具有记忆截止，意味着在有限的样本窗口后历史被完全忽视。一旦平稳性变换将数据中的所有记忆抹去，统计学家会
    resort to 复杂的数学技术来提取剩余的信号。不出所料，在去记忆序列上应用这些复杂技术可能会导致错误发现。在本章中，我们介绍了一种数据变换方法，确保数据的平稳性，同时尽可能保留记忆。
- en: '**5.2 The Stationarity vs. Memory Dilemma**'
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: '**5.2 平稳性与记忆的困境**'
- en: 'It is common in finance to find non-stationary time series. What makes these
    series non-stationary is the presence of memory, i.e., a long history of previous
    levels that shift the series'' mean over time. In order to perform inferential
    analyses, researchers need to work with invariant processes, such as returns on
    prices (or changes in log-prices), changes in yield, or changes in volatility.
    These data transformations make the series stationary, at the expense of removing
    all memory from the original series (Alexander [2001], chapter 11). Although stationarity
    is a necessary property for inferential purposes, it is rarely the case in signal
    processing that we wish all memory to be erased, as that memory is the basis for
    the model''s predictive power. For example, equilibrium (stationary) models need
    some memory to assess how far the price process has drifted away from the long-term
    expected value in order to generate a forecast. The dilemma is that returns are
    stationary, however memory-less, and prices have memory, however they are non-stationary.
    The question arises: What is the minimum amount of differentiation that makes
    a price series stationary while preserving as much memory as possible? Accordingly,
    we would like to generalize the notion of returns to consider *stationary series
    where not all memory is erased.* Under this framework, returns are just one kind
    of (and in most cases suboptimal) price transformation among many other possibilities.'
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: 在金融领域，发现非平稳时间序列是很常见的。导致这些序列非平稳的原因是存在记忆，即长期的历史水平影响序列的均值。为了进行推断分析，研究人员需要处理不变过程，比如价格的收益（或对数价格的变化）、收益变化或波动率变化。这些数据变换使序列平稳，但代价是从原始序列中去除了所有记忆（亚历山大[2001]，第11章）。虽然平稳性是推断分析的必要属性，但在信号处理时，我们通常并不希望完全消除所有记忆，因为这些记忆是模型预测能力的基础。例如，平衡（平稳）模型需要一定的记忆来评估价格过程偏离长期期望值的程度，以生成预测。困境在于，收益是平稳的，但没有记忆，而价格有记忆，但又是非平稳的。问题是：什么是使价格序列平稳的最小差分量，同时尽可能保留记忆？因此，我们希望推广收益的概念，以考虑*保留部分记忆的平稳序列*。在这个框架下，收益只是一种（在大多数情况下是次优的）价格变换，还有许多其他可能性。
- en: Part of the importance of cointegration methods is their ability to model series
    with memory. But why would the particular case of zero differentiation deliver
    best outcomes? Zero differentiation is as arbitrary as 1-step differentiation.
    There is a wide region between these two extremes (fully differentiated series
    on one hand, and zero differentiated series on the other) that can be explored
    through fractional differentiation for the purpose of developing a highly predictive
    ML model.
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: 协整方法的重要性部分在于它们能够对具有记忆的序列建模。但为什么零差分的特定情况会带来最佳结果？零差分与1步差分一样任意。这两者之间有一个广泛的区域（完全差分序列在一端，零差分序列在另一端），可以通过分数差分来探索，以便开发出高预测能力的机器学习模型。
- en: Supervised learning algorithms typically require stationary features. The reason
    is that we need to map a previously unseen (unlabeled) observation to a collection
    of labeled examples, and infer from them the label of that new observation. If
    the features are not stationary, we cannot map the new observation to a large
    number of known examples. But stationarity does not ensure predictive power. Stationarity
    is a necessary, non-sufficient condition for the high performance of an ML algorithm.
    The problem is, there is a trade-off between stationarity and memory. We can always
    make a series more stationary through differentiation, but it will be at the cost
    of erasing some memory, which will defeat the forecasting purpose of the ML algorithm.
    In this chapter, we will study one way to resolve this dilemma.
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: 监督学习算法通常要求特征是平稳的。原因在于我们需要将一个先前未见（未标记）的观察值映射到一组标记示例，并从中推断出该新观察值的标签。如果特征不平稳，我们就无法将新观察值映射到大量已知示例。然而，平稳性并不保证预测能力。平稳性是机器学习算法高性能的必要但不充分条件。问题在于，平稳性与记忆之间存在权衡。我们总是可以通过微分使序列更平稳，但这将以抹去部分记忆为代价，这将削弱机器学习算法的预测目的。在本章中，我们将研究解决这一困境的一种方法。
- en: '**5.3 Literature Review**'
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: '**5.3 文献综述**'
- en: 'Virtually all the financial time series literature is based on the premise
    of making non-stationary series stationary through integer transformation (see
    Hamilton [1994] for an example). This raises two questions: (1) Why would integer
    1 differentiation (like the one used for computing returns on log-prices) be optimal?
    (2) Is over-differentiation one reason why the literature has been so biased in
    favor of the efficient markets hypothesis?'
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: 几乎所有的金融时间序列文献都是基于通过整数变换使非平稳序列平稳的前提（参见汉密尔顿 [1994] 的例子）。这引出了两个问题：（1）为什么整数1微分（例如用于计算对数价格回报的微分）是最优的？（2）过度微分是否是文献在高效市场假说方面存在偏见的一个原因？
- en: 'The notion of fractional differentiation applied to the predictive time series
    analysis dates back at least to Hosking [1981]. In that paper, a family of ARIMA
    processes was generalized by permitting the degree of differencing to take fractional
    values. This was useful because fractionally differenced processes exhibit long-term
    persistence and antipersistence, hence enhancing the forecasting power compared
    to the standard ARIMA approach. In the same paper, Hosking states: “Apart from
    a passing reference by Granger (1978), fractional differencing does not appear
    to have been previously mentioned in connection with time series analysis.”'
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: 应用于预测时间序列分析的分数微分概念至少可以追溯到霍斯金 [1981]。在那篇论文中，通过允许微分的阶数取分数值，推广了一类ARIMA过程。这是有用的，因为分数微分过程表现出长期的持续性和反持续性，因此增强了与标准ARIMA方法相比的预测能力。在同一篇论文中，霍斯金指出：“除了格兰杰（1978）的简短提及，分数微分似乎以前在时间序列分析中并未被提及。”
- en: 'After Hosking''s paper, the literature on this subject has been surprisingly
    scarce, adding up to eight journal articles written by only nine authors: Hosking,
    Johansen, Nielsen, MacKinnon, Jensen, Jones, Popiel, Cavaliere, and Taylor. See
    the references for details. Most of those papers relate to technical matters,
    such as fast algorithms for the calculation of fractional differentiation in continuous
    stochastic processes (e.g., Jensen and Nielsen [2014]).'
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: 在霍斯金的论文发表后，这一主题的文献出奇地稀少，总共只有九位作者撰写的八篇期刊文章：霍斯金、约汉森、尼尔森、麦金农、延森、琼斯、波皮尔、卡瓦利埃雷和泰勒。具体参考文献请见文后。大多数论文涉及技术问题，例如计算连续随机过程中的分数微分的快速算法（例如，延森和尼尔森
    [2014]）。
- en: 'Differentiating the stochastic process is a computationally expensive operation.
    In this chapter we will take a practical, alternative, and novel approach to recover
    stationarity: We will generalize the difference operator to non-integer steps.'
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: 对随机过程进行微分是一项计算成本高昂的操作。在本章中，我们将采用一种实用的、替代的和新颖的方法来恢复平稳性：我们将将差分算子推广到非整数步。
- en: '**5.4 The Method**'
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: '**5.4 方法**'
- en: Consider the backshift operator, *B* , applied to a matrix of real-valued features
    { *X [*t*]* }, where *B ^(*k*) X [*t*]* = *X [*t* − *k*]* for any integer *k*
    ≥ 0 *.* For example, (1 − *B* ) ² = 1 − 2 *B* + *B ²* , where *B ² X [*t*]* =
    *X [*t* − 2]* , so that (1 − *B* ) ² *X [*t*]* = *X [*t*]* − 2 *X [*t* − 1]* +
    *X [*t* − 2]* . Note that ![](Image00154.jpg) , for *n* a positive integer. For
    a real number *d* , ![](Image00158.jpg) , the binomial series.
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑反移位算子 *B*，应用于实值特征矩阵 { *X [*t*]* }，其中 *B ^(*k*) X [*t*]* = *X [*t* − *k*]*，对于任何整数
    *k* ≥ 0 *.* 例如，(1 − *B* ) ² = 1 − 2 *B* + *B ²*，其中 *B ² X [*t*]* = *X [*t* − 2]*，因此
    (1 − *B* ) ² *X [*t*]* = *X [*t*]* − 2 *X [*t* − 1]* + *X [*t* − 2]*。注意 ![](Image00154.jpg)，对于
    *n* 为正整数。对于实数 *d*，![](Image00158.jpg)，二项级数。
- en: 'In a fractional model, the exponent *d* is allowed to be a real number, with
    the following formal binomial series expansion:'
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: 在分数模型中，指数 *d* 可以是实数，具有以下形式的二项级数展开：
- en: '![](Image00847.jpg)'
  id: totrans-251
  prefs: []
  type: TYPE_IMG
  zh: '![](Image00847.jpg)'
- en: '**5.4.1 Long Memory**'
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: '**5.4.1 长期记忆**'
- en: Let us see how a real (non-integer) positive *d* preserves memory. This arithmetic
    series consists of a dot product
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看一个实数（非整数）正 *d* 是如何保持记忆的。这一算术序列由一个点积组成。
- en: '![](Image00163.jpg)'
  id: totrans-254
  prefs: []
  type: TYPE_IMG
  zh: '![](Image00163.jpg)'
- en: with weights ω
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: 带权重ω
- en: '![](Image00172.jpg)'
  id: totrans-256
  prefs: []
  type: TYPE_IMG
  zh: '![](Image00172.jpg)'
- en: and values *X*
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: 和数值 *X*
- en: '![](Image00169.jpg)'
  id: totrans-258
  prefs: []
  type: TYPE_IMG
  zh: '![](Image00169.jpg)'
- en: When *d* is a positive integer number, ![](Image00173.jpg) , and memory beyond
    that point is cancelled. For example, *d* = 1 is used to compute returns, where
    ![](Image00175.jpg) , and ω = {1, −1, 0, 0, …}.
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: 当 *d* 是正整数时，![](Image00173.jpg)，超出该点的内存被取消。例如，*d* = 1 用于计算回报，其中 ![](Image00175.jpg)，且
    ω = {1, −1, 0, 0, …}。
- en: '**5.4.2 Iterative Estimation**'
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
  zh: '**5.4.2 迭代估计**'
- en: 'Looking at the sequence of weights, ω, we can appreciate that for *k* = 0,
    …, ∞, with ω [0] = 1, the weights can be generated iteratively as:'
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
  zh: 从权重序列 ω 中，我们可以看出对于 *k* = 0, …, ∞，当 ω [0] = 1 时，权重可以通过迭代生成：
- en: '![](Image00176.jpg)'
  id: totrans-262
  prefs: []
  type: TYPE_IMG
  zh: '![](Image00176.jpg)'
- en: '[Figure 5.1](text00001.html#filepos0000341782) plots the sequence of weights
    used to compute each value of the fractionally differentiated series. The legend
    reports the value of *d* used to generate each sequence, the x-axis indicates
    the value of *k* , and the y-axis shows the value of ω [*k*] . For example, for
    *d* = 0, all weights are 0 except for ω [0] = 1 *.* That is the case where the
    differentiated series coincides with the original one. For *d* = 1, all weights
    are 0 except for ω [0] = 1 and ω [1] = −1 *.* That is the standard first-order
    integer differentiation, which is used to derive log-price returns. Anywhere in
    between these two cases, all weights after ω [0] = 1 are negative and greater
    than −1.'
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
  zh: '[图 5.1](text00001.html#filepos0000341782) 绘制了用于计算每个分数差分序列值的权重序列。图例报告了生成每个序列所使用的
    *d* 值，x 轴表示 *k* 值，y 轴显示 ω [*k*] 的值。例如，对于 *d* = 0，所有权重均为 0，只有 ω [0] = 1。这是差分序列与原始序列重合的情况。对于
    *d* = 1，所有权重均为 0，只有 ω [0] = 1 和 ω [1] = −1。这是标准的一阶整数差分，用于推导对数价格回报。在这两种情况之间，所有
    ω [0] = 1 之后的权重均为负且大于 −1。'
- en: '![](Image00178.jpg)'
  id: totrans-264
  prefs: []
  type: TYPE_IMG
  zh: '![](Image00178.jpg)'
- en: '[**Figure 5.1**](text00001.html#filepos0000340412) *ω [*k*]* (y-axis) as *k*
    increases (x-axis). Each line is associated with a particular value of *d* ∈ [0,1],
    in 0.1 increments.'
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
  zh: '[**图 5.1**](text00001.html#filepos0000340412) *ω [*k*]*（y 轴）随着 *k* 的增加（x 轴）。每条线与特定值的
    *d* ∈ [0,1] 相关，增量为 0.1。'
- en: '[Figure 5.2](text00001.html#filepos0000342705) plots the sequence of weights
    where *d* ∈ [1, 2], at increments of 0.1\. For *d* > 1, we observe ω [1] < −1
    and ω [*k*] > 0, ∀ *k* ≥ 2 *.*'
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
  zh: '[图 5.2](text00001.html#filepos0000342705) 绘制了 *d* ∈ [1, 2] 的权重序列，增量为 0.1。当
    *d* > 1 时，我们观察到 ω [1] < −1 和 ω [*k*] > 0，∀ *k* ≥ 2 *.* '
- en: '![](Image00228.jpg)'
  id: totrans-267
  prefs: []
  type: TYPE_IMG
  zh: '![](Image00228.jpg)'
- en: '[**Figure 5.2**](text00001.html#filepos0000342096) *ω [*k*]* (y-axis) as *k*
    increases (x-axis). Each line is associated with a particular value of *d* ∈ [1,2],
    in 0.1 increments.'
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
  zh: '[**图 5.2**](text00001.html#filepos0000342096) *ω [*k*]*（y 轴）随着 *k* 的增加（x 轴）。每条线与特定值的
    *d* ∈ [1,2] 相关，增量为 0.1。'
- en: Snippet 5.1 lists the code used to generate these plots.
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
  zh: 片段 5.1 列出了用于生成这些图表的代码。
- en: '**SNIPPET 5.1 WEIGHTING FUNCTION**'
  id: totrans-270
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '**片段 5.1 加权函数**'
- en: '![](Image00246.jpg)'
  id: totrans-271
  prefs:
  - PREF_BQ
  type: TYPE_IMG
  zh: '![](Image00246.jpg)'
- en: '**5.4.3 Convergence**'
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
  zh: '**5.4.3 收敛性**'
- en: Let us consider the convergence of the weights. From the above result, we can
    see that for *k* > *d* , if ω [*k* − 1] ≠ 0, then ![](Image00190.jpg) , and ω
    [*k*] = 0 otherwise. Consequently, the weights converge asymptotically to zero,
    as an infinite product of factors within the unit circle. Also, for a positive
    *d* and *k* < *d* + 1, we have ![](Image00281.jpg) , which makes the initial weights
    alternate in sign. For a non-integer *d* , once *k* ≥ *d* + 1, ω [*k*] will be
    negative if int[ *d* ] is even, and positive otherwise. Summarizing, ![](Image00300.jpg)
    (converges to zero from the left) when int[ *d* ] is even, and ![](Image00202.jpg)
    (converges to zero from the right) when Int[ *d* ] is odd. In the special case
    *d* ∈ (0, 1), this means that − 1 < ω [*k*] < 0, ∀ *k* > 0 *.* This alternation
    of weight signs is necessary to make ![](Image00336.jpg) stationary, as memory
    wanes or is offset over the long run.
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们考虑权重的收敛性。从上述结果可以看出，对于*k* > *d*，如果ω [*k* − 1] ≠ 0，则![](Image00190.jpg)，而ω
    [*k*] = 0则为其他情况。因此，权重渐近收敛到零，作为单位圆内因素的无限乘积。此外，对于正的*d*和*k* < *d* + 1，我们有![](Image00281.jpg)，这使得初始权重在符号上交替。对于非整数*d*，一旦*k*
    ≥ *d* + 1，若int[*d*]为偶数，ω [*k*]将为负，反之则为正。总结一下，当int[*d*]为偶数时，![](Image00300.jpg)（从左收敛到零），而当Int[*d*]为奇数时，![](Image00202.jpg)（从右收敛到零）。在特殊情况下*d*
    ∈ (0, 1)，这意味着−1 < ω [*k*] < 0，∀ *k* > 0 *。这种权重符号的交替对于使![](Image00336.jpg)保持平稳是必要的，因为记忆随着时间的推移而减弱或被抵消。
- en: '**5.5 Implementation**'
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
  zh: '**5.5 实施**'
- en: 'In this section we will explore two alternative implementations of fractional
    differentiation: the standard “expanding window” method, and a new method that
    I call “fixed-width window fracdiff” (FFD).'
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将探讨两种替代的分数微分实现方法：标准的“扩展窗口”方法和我称之为“固定宽度窗口分数微分”（FFD）的新方法。
- en: '**5.5.1 Expanding Window**'
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
  zh: '**5.5.1 扩展窗口**'
- en: Let us discuss how to fractionally differentiate a (finite) time series in practice.
    Suppose a time series with *T* real observations, { *X [*t*]* },  *t* = 1, …,
    *T* . Because of data limitations, the fractionally differentiated value ![](Image00206.jpg)
    cannot be computed on an infinite series of weights. For instance, the last point
    ![](Image00209.jpg) will use weights {ω [*k*] },  *k* = 0, …, *T* − 1, and ![](Image00213.jpg)
    will use weights {ω [*k*] },  *k* = 0, …, *T* − *l* − 1\. This means that the
    initial points will have a different amount of memory compared to the final points.
    For each *l* , we can determine the relative weight-loss, ![](Image00216.jpg)
    . Given a tolerance level τ ∈ [0, 1], we can determine the value *l* * such that
    ![](Image00425.jpg) and ![](Image00741.jpg) . This value *l* * corresponds to
    the first results ![](Image00744.jpg) where the weight-loss is beyond the acceptable
    threshold, λ [*t*] > τ (e.g., τ = 0.01) *.*
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们讨论如何在实践中进行分数微分的（有限）时间序列。假设有一个包含*T*个实际观察值的时间序列，{*X [*t*]*}，*t* = 1, …, *T*。由于数据限制，分数微分值![](Image00206.jpg)无法在无限权重序列上计算。例如，最后一个点![](Image00209.jpg)将使用权重{ω
    [*k*]}，*k* = 0, …, *T* − 1，而![](Image00213.jpg)将使用权重{ω [*k*]}，*k* = 0, …, *T*
    − *l* − 1。这意味着初始点与最终点的记忆量不同。对于每个*l*，我们可以确定相对权重损失![](Image00216.jpg)。给定容忍水平τ ∈
    [0, 1]，我们可以确定值*l* *，使得![](Image00425.jpg)和![](Image00741.jpg)。这个值*l* *对应于权重损失超过可接受阈值的第一次结果![](Image00744.jpg)，其中λ
    [*t*] > τ（例如，τ = 0.01）*。
- en: From our earlier discussion, it is clear that ![](Image00749.jpg) depends on
    the convergence speed of {ω [*k*] }, which in turn depends on *d* ∈ [0, 1]. For
    *d* = 1, ω [*k*] = 0, ∀ *k* > 1, and λ [*l*] = 0, ∀ *l* > 1, hence it suffices
    to drop ![](Image00753.jpg) . As *d* → 0 ^+ , *l* * increases, and a larger portion
    of the initial ![](Image00754.jpg) needs to be dropped in order to keep the weight-loss
    ![](Image00755.jpg) .  [ Figure 5.3 ](text00001.html#filepos0000350067) plots
    the E-mini S&P 500 futures trade bars of size 1E4, rolled forward, fractionally
    differentiated, with parameters ( *d* = .4, τ = 1) on the top and parameters (
    *d* = .4, τ = 1 *E* − 2) on the bottom.
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
  zh: 从我们之前的讨论来看，![](Image00749.jpg)依赖于{ω [*k*]}的收敛速度，而这又依赖于*d* ∈ [0, 1]。对于*d* = 1，ω
    [*k*] = 0，∀ *k* > 1，而λ [*l*] = 0，∀ *l* > 1，因此可以忽略![](Image00753.jpg)。随着*d* → 0
    ^+，*l* *增加，为了保持权重损失![](Image00755.jpg)，需要丢弃更多初始的![](Image00754.jpg)。 [图5.3](text00001.html#filepos0000350067)绘制了E-mini
    S&P 500期货的交易条，大小为1E4，前移，分数微分，参数为（*d* = .4，τ = 1）在上方，参数为（*d* = .4，τ = 1 *E* − 2）在下方。
- en: '![](Image00758.jpg)'
  id: totrans-279
  prefs: []
  type: TYPE_IMG
  zh: '![](Image00758.jpg)'
- en: '[**Figure 5.3**](text00001.html#filepos0000349525) Fractional differentiation
    without controlling for weight loss (top plot) and after controlling for weight
    loss with an expanding window (bottom plot)'
  id: totrans-280
  prefs: []
  type: TYPE_NORMAL
  zh: '[**图 5.3**](text00001.html#filepos0000349525) 在不控制体重损失的情况下的分数微分（顶部图）和在使用扩展窗口控制体重损失后的分数微分（底部图）'
- en: The negative drift in both plots is caused by the negative weights that are
    added to the initial observations as the window is expanded. When we do not control
    for weight loss, the negative drift is extreme, to the point that only that trend
    is visible. The negative drift is somewhat more moderate in the right plot, after
    controlling for the weight loss, however, it is still substantial, because values
    ![](Image00762.jpg) are computed on an expanding window. This problem can be corrected
    by a fixed-width window, implemented in Snippet 5.2.
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
  zh: 两个图中的负漂移是由于在扩展窗口时添加到初始观测值的负权重造成的。当我们不控制体重损失时，负漂移非常明显，以至于只有该趋势可见。然而，在控制体重损失后，右侧图中的负漂移相对温和，但仍然相当显著，因为值
    ![](Image00762.jpg) 是在扩展窗口上计算的。这个问题可以通过在代码片段 5.2 中实现的固定宽度窗口来纠正。
- en: '**SNIPPET 5.2 STANDARD FRACDIFF (EXPANDING WINDOW)**'
  id: totrans-282
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '**代码片段 5.2 标准分数微分（扩展窗口）**'
- en: '![](Image00767.jpg)'
  id: totrans-283
  prefs:
  - PREF_BQ
  type: TYPE_IMG
  zh: '![](Image00767.jpg)'
- en: '**5.5.2 Fixed-Width Window Fracdiff**'
  id: totrans-284
  prefs: []
  type: TYPE_NORMAL
  zh: '**5.5.2 固定宽度窗口分数微分**'
- en: Alternatively, fractional differentiation can be computed using a fixed-width
    window, that is, dropping the weights after their modulus (|ω [*k*] |) falls below
    a given threshold value (τ) *.* This is equivalent to finding the first *l* *
    such that ![](Image00771.jpg) and ![](Image00775.jpg) , setting a new variable
    ![](Image00779.jpg)
  id: totrans-285
  prefs: []
  type: TYPE_NORMAL
  zh: 或者，可以使用固定宽度窗口计算分数微分，也就是说，当权重的模（|ω [*k*] |）低于给定阈值（τ）时，丢弃权重 *。这相当于找到第一个 *l* *，使得
    ![](Image00771.jpg) 和 ![](Image00775.jpg) 成立，并设置一个新变量 ![](Image00779.jpg)。
- en: '![](Image00781.jpg)'
  id: totrans-286
  prefs: []
  type: TYPE_IMG
  zh: '![](Image00781.jpg)'
- en: and ![](Image00783.jpg) , for *t* = *T* − *l* * + 1, …, *T* .  [ Figure 5.4
    ](text00001.html#filepos0000353033) plots E-mini S&P 500 futures trade bars of
    size 1E4, rolled forward, fractionally differentiated ( *d* = .4, τ = 1 *E* −
    5).
  id: totrans-287
  prefs: []
  type: TYPE_NORMAL
  zh: 和 ![](Image00783.jpg)，对于 *t* = *T* − *l* * + 1, …, *T*。 [图 5.4](text00001.html#filepos0000353033)
    绘制了 E-mini S&P 500 期货交易条的大小为 1E4，向前滚动，分数微分（*d* = .4, τ = 1 *E* − 5）。
- en: '![](Image00786.jpg)'
  id: totrans-288
  prefs: []
  type: TYPE_IMG
  zh: '![](Image00786.jpg)'
- en: '[**Figure 5.4**](text00001.html#filepos0000352607) Fractional differentiation
    after controlling for weight loss with a fixed-width window'
  id: totrans-289
  prefs: []
  type: TYPE_NORMAL
  zh: '[**图 5.4**](text00001.html#filepos0000352607) 在控制体重损失后，使用固定宽度窗口的分数微分'
- en: This procedure has the advantage that the same vector of weights is used across
    all estimates of ![](Image00791.jpg) , hence avoiding the negative drift caused
    by an expanding window's added weights. The result is a driftless blend of level
    plus noise, as expected. The distribution is no longer Gaussian, as a result of
    the skewness and excess kurtosis that comes with memory, however it is stationary.
    Snippet 5.3 presents an implementation of this idea.
  id: totrans-290
  prefs: []
  type: TYPE_NORMAL
  zh: 该过程的优点在于，在所有 ![](Image00791.jpg) 的估计中使用相同的权重向量，从而避免了扩展窗口添加的权重所造成的负漂移。结果是一个无漂移的水平加噪声的组合，符合预期。由于记忆带来的偏斜度和超额峰度，分布不再是高斯分布，但它是平稳的。代码片段
    5.3 展示了这个想法的实现。
- en: '**SNIPPET 5.3 THE NEW FIXED-WIDTH WINDOW FRACDIFF METHOD**'
  id: totrans-291
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '**代码片段 5.3 新的固定宽度窗口分数微分方法**'
- en: '![](Image00794.jpg)'
  id: totrans-292
  prefs:
  - PREF_BQ
  type: TYPE_IMG
  zh: '![](Image00794.jpg)'
- en: '**5.6 Stationarity with Maximum Memory Preservation**'
  id: totrans-293
  prefs: []
  type: TYPE_NORMAL
  zh: '**5.6 最大记忆保留的平稳性**'
- en: Consider a series { *X [*t*]* } [*t* = 1, …, *T*] . Applying the fixed-width
    window fracdiff (FFD) method on this series, we can compute the minimum coefficient
    *d* * such that the resulting fractionally differentiated series ![](Image00798.jpg)
    is stationary. This coefficient *d* * quantifies the amount of memory that needs
    to be removed to achieve stationarity. If ![](Image00801.jpg) is already stationary,
    then *d* * = 0 *.* If ![](Image00803.jpg) contains a unit root, then *d* * < 1
    *.* If ![](Image00806.jpg) exhibits explosive behavior (like in a bubble), then
    *d* * > 1 *.* A case of particular interest is 0 < *d* * ≪ 1, when the original
    series is “mildly non-stationary.” In this case, although differentiation is needed,
    a full integer differentiation removes excessive memory (and predictive power).
  id: totrans-294
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑一个序列 { *X [*t*]* } [*t* = 1, …, *T*] 。对该序列应用固定宽度窗口的分数差分 (FFD) 方法，我们可以计算出最小系数
    *d* *，使得得到的分数差分序列 ![](Image00798.jpg) 是平稳的。这个系数 *d* * 量化了需要去除的记忆量以实现平稳性。如果 ![](Image00801.jpg)
    已经是平稳的，则 *d* * = 0 *。如果 ![](Image00803.jpg) 包含单位根，则 *d* * < 1 *。如果 ![](Image00806.jpg)
    表现出爆炸性行为（如在泡沫中），则 *d* * > 1 *。一个特别值得关注的情况是 0 < *d* * ≪ 1，当原始序列“轻微非平稳”时。在这种情况下，虽然需要进行差分，但完整的整数差分会移除过多的记忆（和预测能力）。
- en: '[Figure 5.5](text00001.html#filepos0000357571) illustrates this concept. On
    the right y-axis, it plots the ADF statistic computed on E-mini S&P 500 futures
    log-prices, rolled forward using the ETF trick (see Chapter 2), downsampled to
    daily frequency, going back to the contract''s inception. On the x-axis, it displays
    the *d* value used to generate the series on which the ADF statistic was computed.
    The original series has an ADF statistic of –0.3387, while the returns series
    has an ADF statistic of –46.9114\. At a 95% confidence level, the test''s critical
    value is –2.8623\. The ADF statistic crosses that threshold in the vicinity of
    *d* = 0.35 *.* The left y-axis plots the correlation between the original series
    ( *d* = 0) and the differentiated series at various *d* values. At *d* = 0.35
    the correlation is still very high, at 0.995\. This confirms that the procedure
    introduced in this chapter has been successful in achieving stationarity without
    giving up too much memory. In contrast, the correlation between the original series
    and the returns series is only 0.03, hence showing that the standard integer differentiation
    wipes out the series’ memory almost entirely.'
  id: totrans-295
  prefs: []
  type: TYPE_NORMAL
  zh: '[图 5.5](text00001.html#filepos0000357571) 说明了这个概念。在右侧 y 轴上，它绘制了基于 E-mini S&P
    500 期货对数价格计算的 ADF 统计量，使用 ETF 技巧（见第 2 章）向前滚动，降采样到每日频率，追溯到合同的起始时间。在 x 轴上，它显示了用于生成计算
    ADF 统计量的序列的 *d* 值。原始序列的 ADF 统计量为 –0.3387，而收益序列的 ADF 统计量为 –46.9114。在 95% 的置信水平下，测试的临界值为
    –2.8623。ADF 统计量在 *d* = 0.35 附近穿越了该阈值。左侧 y 轴绘制了原始序列（ *d* = 0）与在不同 *d* 值下的差分序列之间的相关性。在
    *d* = 0.35 时，相关性仍然非常高，为 0.995。这证实了本章介绍的程序在实现平稳性而不牺牲过多记忆方面是成功的。相比之下，原始序列与收益序列之间的相关性仅为
    0.03，这表明标准整数差分几乎完全消除了序列的记忆。'
- en: '![](Image00808.jpg)'
  id: totrans-296
  prefs: []
  type: TYPE_IMG
  zh: '![](Image00808.jpg)'
- en: '[**Figure 5.5**](text00001.html#filepos0000355959) ADF statistic as a function
    of *d* , on E-mini S&P 500 futures log-prices'
  id: totrans-297
  prefs: []
  type: TYPE_NORMAL
  zh: '[**图 5.5**](text00001.html#filepos0000355959) ADF 统计量作为 *d* 的函数，基于 E-mini S&P
    500 期货的对数价格。'
- en: Virtually all finance papers attempt to recover stationarity by applying an
    integer differentiation *d* = 1 ≫ 0.35, which means that most studies have over-differentiated
    the series, that is, they have removed much more memory than was necessary to
    satisfy standard econometric assumptions. Snippet 5.4 lists the code used to produce
    these results.
  id: totrans-298
  prefs: []
  type: TYPE_NORMAL
  zh: 几乎所有的金融论文都试图通过应用整数差分 *d* = 1 ≫ 0.35 来恢复平稳性，这意味着大多数研究过度差分了序列，也就是说，它们移除了比满足标准计量经济学假设所需的更多记忆。片段
    5.4 列出了用于产生这些结果的代码。
- en: '**SNIPPET 5.4 FINDING THE MINIMUM** ***D*** **VALUE THAT PASSES THE ADF TEST**'
  id: totrans-299
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '**片段 5.4 寻找通过 ADF 测试的最小** ***D*** **值**'
- en: '![](Image00809.jpg)'
  id: totrans-300
  prefs:
  - PREF_BQ
  type: TYPE_IMG
  zh: '![](Image00809.jpg)'
- en: The example on E-mini futures is by no means an exception. [Table 5.1](text00001.html#filepos0000359372)
    shows the ADF statistics after applying FFD( *d* ) on various values of *d* ,
    for 87 of the most liquid futures worldwide. In all cases, the standard *d* =
    1 used for computing returns implies over-differentiation. In fact, in all cases
    stationarity is achieved with *d* < 0.6 *.* In some cases, like orange juice (JO1
    Comdty) or live cattle (LC1 Comdty) no differentiation at all was needed.
  id: totrans-301
  prefs: []
  type: TYPE_NORMAL
  zh: E-mini 期货的例子绝对不是例外。[表 5.1](text00001.html#filepos0000359372) 显示了在对 87 个全球最活跃期货合约的各种
    *d* 值应用 FFD(*d*) 后的 ADF 统计量。在所有情况下，用于计算收益的标准 *d* = 1 表示过度差分。事实上，在所有情况下，*d* < 0.6
    时都能实现平稳性。在某些情况下，如橙汁 (JO1 Comdty) 或活牛 (LC1 Comdty)，根本不需要进行任何差分。
- en: '[**Table 5.1**](text00001.html#filepos0000358629) **ADF Statistic on FFD(**
    ***d*** **) for Some of the Most Liquid Futures Contracts**'
  id: totrans-302
  prefs: []
  type: TYPE_NORMAL
  zh: '[**表 5.1**](text00001.html#filepos0000358629) **某些最活跃期货合约的 FFD(** ***d*** **)
    的 ADF 统计量**'
- en: '![](Image00812.jpg)'
  id: totrans-303
  prefs: []
  type: TYPE_IMG
  zh: '![](Image00812.jpg)'
- en: '![](Image00815.jpg)'
  id: totrans-304
  prefs: []
  type: TYPE_IMG
  zh: '![](Image00815.jpg)'
- en: '**5.7 Conclusion**'
  id: totrans-305
  prefs: []
  type: TYPE_NORMAL
  zh: '**5.7 结论**'
- en: 'To summarize, most econometric analyses follow one of two paradigms:'
  id: totrans-306
  prefs: []
  type: TYPE_NORMAL
  zh: 总结来说，大多数计量经济学分析遵循两种范式之一：
- en: 'Box-Jenkins: Returns are stationary, however memory-less.'
  id: totrans-307
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Box-Jenkins：收益是平稳的，但没有记忆。
- en: 'Engle-Granger: Log-prices have memory, however they are non-stationary. Cointegration
    is the trick that makes regression work on non-stationary series, so that memory
    is preserved. However the number of cointegrated variables is limited, and the
    cointegrating vectors are notoriously unstable.'
  id: totrans-308
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Engle-Granger：对数价格具有记忆，但它们是非平稳的。协整是使回归在非平稳序列上有效的技巧，从而保留记忆。然而，协整变量的数量是有限的，且协整向量
    notoriously 不稳定。
- en: In contrast, the FFD approach introduced in this chapter shows that there is
    no need to give up all of the memory in order to gain stationarity. And there
    is no need for the cointegration trick as it relates to ML forecasting. Once you
    become familiar with FFD, it will allow you to achieve stationarity without renouncing
    to memory (or predictive power).
  id: totrans-309
  prefs: []
  type: TYPE_NORMAL
  zh: 相比之下，本章介绍的 FFD 方法表明，不需要放弃所有的记忆就可以获得平稳性。并且在与 ML 预测相关时，没有必要使用协整技巧。一旦你熟悉 FFD，它将使你能够在不放弃记忆（或预测能力）的情况下实现平稳性。
- en: 'In practice, I suggest you experiment with the following transformation of
    your features: First, compute a cumulative sum of the time series. This guarantees
    that some order of differentiation is needed. Second, compute the FFD( *d* ) series
    for various *d* ∈ [0, 1]. Third, determine the minimum *d* such that the p-value
    of the ADF statistic on FFD( *d* ) falls below 5%. Fourth, use the FFD( *d* )
    series as your predictive feature.'
  id: totrans-310
  prefs: []
  type: TYPE_NORMAL
  zh: 在实践中，我建议你尝试以下特征转换：首先，计算时间序列的累积和。这确保需要某种顺序的差分。第二，计算不同 *d* ∈ [0, 1] 的 FFD(*d*)
    序列。第三，确定最小的 *d* 使得 FFD(*d*) 的 ADF 统计量的 p 值低于 5%。第四，将 FFD(*d*) 序列用作你的预测特征。
- en: '**Exercises**'
  id: totrans-311
  prefs: []
  type: TYPE_NORMAL
  zh: '**练习**'
- en: 'Generate a time series from an IID Gaussian random process. This is a memory-less,
    stationary series:'
  id: totrans-312
  prefs:
  - PREF_OL
  - PREF_BQ
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 从 IID 高斯随机过程中生成一个时间序列。这是一个没有记忆的平稳序列：
- en: Compute the ADF statistic on this series. What is the p-value?
  id: totrans-313
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 计算该序列的 ADF 统计量。p 值是多少？
- en: Compute the cumulative sum of the observations. This is a non-stationary series
    without memory.
  id: totrans-314
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 计算观测值的累积和。这是一个没有记忆的非平稳序列。
- en: What is the order of integration of this cumulative series?
  id: totrans-315
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 这个累积序列的积分顺序是多少？
- en: Compute the ADF statistic on this series. What is the p-value?
  id: totrans-316
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 计算该序列的 ADF 统计量。p 值是多少？
- en: Differentiate the series twice. What is the p-value of this over-differentiated
    series?
  id: totrans-317
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对序列进行两次差分。这个过度差分序列的 p 值是多少？
- en: Generate a time series that follows a sinusoidal function. This is a stationary
    series with memory.
  id: totrans-318
  prefs:
  - PREF_OL
  - PREF_BQ
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 生成一个遵循正弦函数的时间序列。这是一个具有记忆的平稳序列。
- en: Compute the ADF statistic on this series. What is the p-value?
  id: totrans-319
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 计算该序列的 ADF 统计量。p 值是多少？
- en: Shift every observation by the same positive value. Compute the cumulative sum
    of the observations. This is a non-stationary series with memory.
  id: totrans-320
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将每个观测值都加上相同的正值。计算观测值的累积和。这是一个具有记忆的非平稳序列。
- en: Compute the ADF statistic on this series. What is the p-value?
  id: totrans-321
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 计算该序列的 ADF 统计量。p 值是多少？
- en: Apply an expanding window fracdiff, with τ = 1*E* − 2*.* For what minimum *d*
    value do you get a p-value below 5%?
  id: totrans-322
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 应用扩展窗口 fracdiff，τ = 1*E* − 2*.* 在什么最小 *d* 值下，你会得到低于 5% 的 p 值？
- en: Apply FFD, with τ = 1*E* − 5*.* For what minimum *d* value do you get a p-value
    below 5%?
  id: totrans-323
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 应用 FFD，τ = 1*E* − 5*.* 在什么最小 *d* 值下，你会得到低于 5% 的 p 值？
- en: 'Take the series from exercise 2.b:'
  id: totrans-324
  prefs:
  - PREF_OL
  - PREF_BQ
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 取第2.b题中的系列：
- en: Fit the series to a sine function. What is the R-squared?
  id: totrans-325
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将系列拟合到正弦函数上。R平方值是多少？
- en: Apply FFD(*d = 1* ). Fit the series to a sine function. What is the R-squared?
  id: totrans-326
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 应用 FFD(*d = 1*)。将系列拟合到正弦函数上。R平方值是多少？
- en: What value of *d* maximizes the R-squared of a sinusoidal fit on FFD(*d* ).
    Why?
  id: totrans-327
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 什么 *d* 值最大化 FFD(*d*) 上的 R 平方值？为什么？
- en: Take the dollar bar series on E-mini S&P 500 futures. Using the code in Snippet
    5.3, for some *d* ∈ [0, 2], compute `fracDiff_FFD(fracDiff_FFD(series,d),-d)`
    . What do you get? Why?
  id: totrans-328
  prefs:
  - PREF_OL
  - PREF_BQ
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 取 E-mini S&P 500 期货的美元条形系列。使用代码 Snippet 5.3，对于某些 *d* ∈ [0, 2]，计算 `fracDiff_FFD(fracDiff_FFD(series,d),-d)`。你得到了什么？为什么？
- en: Take the dollar bar series on E-mini S&P 500 futures.
  id: totrans-329
  prefs:
  - PREF_OL
  - PREF_BQ
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 取 E-mini S&P 500 期货的美元条形系列。
- en: Form a new series as a cumulative sum of log-prices.
  id: totrans-330
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 形成一个新的系列作为对数价格的累积和。
- en: Apply FFD, with τ = 1*E* − 5*.* Determine for what minimum *d* ∈ [0, 2] the
    new series is stationary.
  id: totrans-331
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 应用 FFD，τ = 1*E* − 5*.* 确定新系列在什么最小 *d* ∈ [0, 2] 下是平稳的。
- en: Compute the correlation of the fracdiff series to the original (untransformed)
    series.
  id: totrans-332
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 计算 fracdiff 系列与原始（未转换）系列的相关性。
- en: Apply an Engel-Granger cointegration test on the original and fracdiff series.
    Are they cointegrated? Why?
  id: totrans-333
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对原始系列和 fracdiff 系列应用 Engel-Granger 协整检验。它们是协整的吗？为什么？
- en: Apply a Jarque-Bera normality test on the fracdiff series.
  id: totrans-334
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对 fracdiff 系列应用 Jarque-Bera 正态性检验。
- en: Take the fracdiff series from exercise 5.
  id: totrans-335
  prefs:
  - PREF_OL
  - PREF_BQ
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 取第5题中的 fracdiff 系列。
- en: Apply a CUSUM filter (Chapter 2), where *h* is twice the standard deviation
    of the series.
  id: totrans-336
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 应用 CUSUM 滤波器（第2章），其中 *h* 是系列的两倍标准差。
- en: Use the filtered timestamps to sample a features’ matrix. Use as one of the
    features the fracdiff value.
  id: totrans-337
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用过滤后的时间戳来抽样特征矩阵。将 fracdiff 值作为特征之一。
- en: Form labels using the triple-barrier method, with symmetric horizontal barriers
    of twice the daily standard deviation, and a vertical barrier of 5 days.
  id: totrans-338
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用三重障碍法形成标签，设置为每日标准差的两倍的对称水平障碍，以及5天的垂直障碍。
- en: 'Fit a bagging classifier of decision trees where:'
  id: totrans-339
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 拟合一个决策树的包袋分类器，其中：
- en: The observed features are bootstrapped using the sequential method from Chapter
    4.
  id: totrans-340
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 观察到的特征使用第4章中的顺序方法进行自助抽样。
- en: On each bootstrapped sample, sample weights are determined using the techniques
    from Chapter 4.
  id: totrans-341
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在每个自助抽样中，样本权重使用第4章中的技术确定。
- en: '**References**'
  id: totrans-342
  prefs: []
  type: TYPE_NORMAL
  zh: '**参考文献**'
- en: 'Alexander, C. (2001): *Market Models* , 1st edition. John Wiley & Sons.'
  id: totrans-343
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 'Alexander, C. (2001): *市场模型* , 第1版。约翰·威利与儿子公司。'
- en: 'Hamilton, J. (1994): *Time Series Analysis* , 1st ed. Princeton University
    Press.'
  id: totrans-344
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 'Hamilton, J. (1994): *时间序列分析* , 第1版。普林斯顿大学出版社。'
- en: 'Hosking, J. (1981): “Fractional differencing.” *Biometrika* , Vol. 68, No.
    1, pp. 165–176.'
  id: totrans-345
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 'Hosking, J. (1981): “分数差分。” *生物统计学* , 第68卷，第1期，页码165–176。'
- en: 'Jensen, A. and M. Nielsen (2014): “A fast fractional difference algorithm.”
    *Journal of Time Series Analysis* , Vol. 35, No. 5, pp. 428–436.'
  id: totrans-346
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 'Jensen, A. 和 M. Nielsen (2014): “快速分数差分算法。” *时间序列分析杂志* , 第35卷，第5期，页码428–436。'
- en: 'López de Prado, M. (2015): “The Future of Empirical Finance.” *Journal of Portfolio
    Management* , Vol. 41, No. 4, pp. 140–144\. Available at [https://ssrn.com/abstract=2609734](https://ssrn.com/abstract=2609734)
    .'
  id: totrans-347
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 'López de Prado, M. (2015): “经验金融的未来。” *投资组合管理杂志* , 第41卷，第4期，页码140–144\. 可在
    [https://ssrn.com/abstract=2609734](https://ssrn.com/abstract=2609734) 获取。'
- en: '**Bibliography**'
  id: totrans-348
  prefs: []
  type: TYPE_NORMAL
  zh: '**参考书目**'
- en: 'Cavaliere, G., M. Nielsen, and A. Taylor (2017): “Quasi-maximum likelihood
    estimation and bootstrap inference in fractional time series models with heteroskedasticity
    of unknown form.” *Journal of Econometrics* , Vol. 198, No. 1, pp. 165–188.'
  id: totrans-349
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 'Cavaliere, G., M. Nielsen 和 A. Taylor (2017): “具有未知形式异方差的分数时间序列模型的准最大似然估计和自助推断。”
    *计量经济学杂志* , 第198卷，第1期，页码165–188。'
- en: 'Johansen, S. and M. Nielsen (2012): “A necessary moment condition for the fractional
    functional central limit theorem.” *Econometric Theory* , Vol. 28, No. 3, pp.
    671–679.'
  id: totrans-350
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 'Johansen, S. 和 M. Nielsen (2012): “分数函数中心极限定理的必要矩条件。” *计量经济理论* , 第28卷，第3期，页码671–679。'
- en: 'Johansen, S. and M. Nielsen (2012): “Likelihood inference for a fractionally
    cointegrated vector autoregressive model.” *Econometrica* , Vol. 80, No. 6, pp.
    2267–2732.'
  id: totrans-351
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 'Johansen, S. 和 M. Nielsen (2012): “分数协整向量自回归模型的似然推断。” *计量经济学* , 第80卷，第6期，页码2267–2732。'
- en: 'Johansen, S. and M. Nielsen (2016): “The role of initial values in conditional
    sum-of-squares estimation of nonstationary fractional time series models.” *Econometric
    Theory* , Vol. 32, No. 5, pp. 1095–1139.'
  id: totrans-352
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 'Johansen, S. 和 M. Nielsen (2016): “初始值在非平稳分数时间序列模型的条件最小二乘估计中的作用。” *计量经济理论*
    , 第32卷，第5期，页码1095–1139。'
- en: 'Jones, M., M. Nielsen and M. Popiel (2015): “A fractionally cointegrated VAR
    analysis of economic voting and political support.” *Canadian Journal of Economics*
    , Vol. 47, No. 4, pp. 1078–1130.'
  id: totrans-353
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 'Jones, M., M. Nielsen 和 M. Popiel (2015): “经济投票与政治支持的分数协整VAR分析。” *加拿大经济学杂志*，第47卷，第4期，页1078–1130。'
- en: 'Mackinnon, J. and M. Nielsen, M. (2014): “Numerical distribution functions
    of fractional unit root and cointegration tests.” *Journal of Applied Econometrics*
    , Vol. 29, No. 1, pp. 161–171.'
  id: totrans-354
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 'Mackinnon, J. 和 M. Nielsen, M. (2014): “分数单位根和协整检验的数值分布函数。” *应用计量经济学杂志*，第29卷，第1期，页161–171。'
- en: '**PART 2**'
  id: totrans-355
  prefs: []
  type: TYPE_NORMAL
  zh: '**第二部分**'
- en: '**Modelling**'
  id: totrans-356
  prefs: []
  type: TYPE_NORMAL
  zh: '**建模**'
- en: '[Chapter 6 Ensemble Methods](text00001.html#filepos0000371207)'
  id: totrans-357
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[第六章 集成方法](text00001.html#filepos0000371207)'
- en: '[Chapter 7 Cross-Validation in Finance](text00001.html#filepos0000404229)'
  id: totrans-358
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[第七章 金融中的交叉验证](text00001.html#filepos0000404229)'
- en: '[Chapter 8 Feature Importance](text00001.html#filepos0000434781)'
  id: totrans-359
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[第八章 特征重要性](text00001.html#filepos0000434781)'
- en: '[Chapter 9 Hyper-Parameter Tuning with Cross-Validation](text00001.html#filepos0000476302)'
  id: totrans-360
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[第九章 超参数调整与交叉验证](text00001.html#filepos0000476302)'
- en: '**CHAPTER 6**'
  id: totrans-361
  prefs: []
  type: TYPE_NORMAL
  zh: '**第六章**'
- en: '**Ensemble Methods**'
  id: totrans-362
  prefs: []
  type: TYPE_NORMAL
  zh: '**集成方法**'
- en: '**6.1 Motivation**'
  id: totrans-363
  prefs: []
  type: TYPE_NORMAL
  zh: '**6.1 动机**'
- en: In this chapter we will discuss two of the most popular ML ensemble methods.
    ^([1](text00001.html#filepos0000402654)) In the references and footnotes you will
    find books and articles that introduce these techniques. As everywhere else in
    this book, the assumption is that you have already used these approaches. The
    goal of this chapter is to explain what makes them effective, and how to avoid
    common errors that lead to their misuse in finance.
  id: totrans-364
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一章中，我们将讨论两种最流行的机器学习集成方法。^([1](text00001.html#filepos0000402654)) 在参考文献和脚注中，您将找到介绍这些技术的书籍和文章。就像本书的其他部分一样，假设您已经使用过这些方法。本章的目的是解释是什么使它们有效，以及如何避免导致其在金融中误用的常见错误。
- en: '**6.2 The Three Sources of Errors**'
  id: totrans-365
  prefs: []
  type: TYPE_NORMAL
  zh: '**6.2 三种误差来源**'
- en: 'ML models generally suffer from three errors: ^([2](text00001.html#filepos0000403004))'
  id: totrans-366
  prefs: []
  type: TYPE_NORMAL
  zh: 机器学习模型通常会遭受三种错误：^([2](text00001.html#filepos0000403004))
- en: '**Bias:** This error is caused by unrealistic assumptions. When bias is high,
    the ML algorithm has failed to recognize important relations between features
    and outcomes. In this situation, the algorithm is said to be “underfit.”'
  id: totrans-367
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**偏差：** 这种误差是由于不现实的假设造成的。当偏差高时，机器学习算法未能识别特征与结果之间的重要关系。在这种情况下，算法被称为“欠拟合”。'
- en: '**Variance:** This error is caused by sensitivity to small changes in the training
    set. When variance is high, the algorithm has overfit the training set, and that
    is why even minimal changes in the training set can produce wildly different predictions.
    Rather than modelling the general patterns in the training set, the algorithm
    has mistaken noise with signal.'
  id: totrans-368
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**方差：** 这种误差是由于对训练集中的小变化的敏感性造成的。当方差高时，算法已经对训练集进行了过拟合，因此即使是训练集的微小变化也会产生截然不同的预测。算法没有建模训练集中的一般模式，而是将噪声误认为信号。'
- en: '**Noise:** This error is caused by the variance of the observed values, like
    unpredictable changes or measurement errors. This is the irreducible error, which
    cannot be explained by any model.'
  id: totrans-369
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**噪声：** 这种误差是由于观测值的方差造成的，例如不可预测的变化或测量误差。这是不可减少的误差，任何模型都无法解释。'
- en: Consider a training set of observations { *x [*i*]* } [*i* = 1, …, *n*] and
    real-valued outcomes { *y [*i*]* } [*i* = 1, …, *n*] . Suppose a function *f*
    [ *x* ] exists, such that *y* = *f* [ *x* ] + ϵ, where *ϵ* is white noise with
    E[ϵ [*i*] ] = 0 and E[ϵ ² [*i*] ] = σ [ϵ] ² . We would like to estimate the function
    ![](Image00818.jpg) that best fits *f* [ *x* ], in the sense of making the variance
    of the estimation error ![](Image00821.jpg) minimal (the mean squared error cannot
    be zero, because of the noise represented by σ ² [  ϵ  ] ). This mean-squared
    error can be decomposed as
  id: totrans-370
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑一个观察训练集 { *x [*i*]* } [*i* = 1, …, *n*] 和实际值结果 { *y [*i*]* } [*i* = 1, …,
    *n*]。假设存在一个函数 *f* [ *x* ]，使得 *y* = *f* [ *x* ] + ϵ，其中 *ϵ* 是白噪声，且 E[ϵ [*i*] ] =
    0 和 E[ϵ ² [*i*] ] = σ [ϵ] ²。我们希望估计最佳拟合 *f* [ *x* ] 的函数 ![](Image00818.jpg)，以使得估计误差的方差
    ![](Image00821.jpg) 最小（均方误差不能为零，因为噪声由 σ ² [ ϵ ] 表示）。这个均方误差可以分解为
- en: '![](Image00826.jpg)'
  id: totrans-371
  prefs: []
  type: TYPE_IMG
  zh: '![](Image00826.jpg)'
- en: An ensemble method is a method that combines a set of weak learners, all based
    on the same learning algorithm, in order to create a (stronger) learner that performs
    better than any of the individual ones. Ensemble methods help reduce bias and/or
    variance.
  id: totrans-372
  prefs: []
  type: TYPE_NORMAL
  zh: 集成方法是一种结合了一组弱学习者的方法，所有学习者均基于相同的学习算法，以创建一个（更强的）学习者，该学习者的表现优于任何单个学习者。集成方法有助于减少偏差和/或方差。
- en: '**6.3 Bootstrap Aggregation**'
  id: totrans-373
  prefs: []
  type: TYPE_NORMAL
  zh: '**6.3 自助聚合**'
- en: 'Bootstrap aggregation (bagging) is an effective way of reducing the variance
    in forecasts. It works as follows: First, generate *N* training datasets by random
    sampling *with replacement.* Second, fit *N* estimators, one on each training
    set. These estimators are fit independently from each other, hence the models
    can be fit in parallel. Third, the ensemble forecast is the *simple* average of
    the individual forecasts from the *N* models. In the case of categorical variables,
    the probability that an observation belongs to a class is given by the proportion
    of estimators that classify that observation as a member of that class (majority
    voting). When the base estimator can make forecasts with a prediction probability,
    the bagging classifier may derive a mean of the probabilities.'
  id: totrans-374
  prefs: []
  type: TYPE_NORMAL
  zh: 自助聚合（袋装法）是减少预测方差的有效方法。其工作原理如下：首先，通过带替换的随机抽样生成 *N* 个训练数据集。其次，为每个训练集拟合 *N* 个估计器。这些估计器是相互独立拟合的，因此模型可以并行拟合。第三，集成预测是
    *N* 个模型的个别预测的 *简单* 平均值。在分类变量的情况下，观察属于某个类别的概率由将该观察分类为该类别成员的估计器的比例给出（多数投票）。当基础估计器能够用预测概率进行预测时，袋装分类器可能会推导出概率的均值。
- en: 'If you use sklearn''s `BaggingClassifier` class to compute the out-of-bag accuracy,
    you should be aware of this bug: [https://github.com/scikit-learn/scikit-learn/issues/8933](https://github.com/scikit-learn/scikit-learn/issues/8933)
    . One workaround is to rename the labels in integer sequential order.'
  id: totrans-375
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你使用 sklearn 的 `BaggingClassifier` 类来计算袋外准确率，你应该注意到这个错误：[https://github.com/scikit-learn/scikit-learn/issues/8933](https://github.com/scikit-learn/scikit-learn/issues/8933)。一种解决方法是将标签按整数顺序重命名。
- en: '**6.3.1 Variance Reduction**'
  id: totrans-376
  prefs: []
  type: TYPE_NORMAL
  zh: '**6.3.1 方差减少**'
- en: 'Bagging''s main advantage is that it reduces forecasts’ variance, hence helping
    address overfitting. The variance of the bagged prediction (φ [*i*] [ *c* ]) is
    a function of the number of bagged estimators ( *N* ), the average variance of
    a single estimator''s prediction ( ![](Image00830.jpg) ), and the average correlation
    among their forecasts ( ![](Image00833.jpg) ):'
  id: totrans-377
  prefs: []
  type: TYPE_NORMAL
  zh: 袋装法的主要优点是它降低了预测的方差，从而有助于解决过拟合问题。袋装预测的方差 (φ [*i*] [ *c* ]) 是袋装估计器数量 (*N*)、单个估计器预测的平均方差
    ( ![](Image00830.jpg) ) 和它们预测之间的平均相关性 ( ![](Image00833.jpg) ) 的函数：
- en: '![](Image00836.jpg)'
  id: totrans-378
  prefs: []
  type: TYPE_IMG
  zh: '![](Image00836.jpg)'
- en: where σ [*i* , *j*] is the covariance of predictions by estimators *i* , *j*
    ; ![](Image00838.jpg) ![](Image00843.jpg) ; and ![](Image00849.jpg) ![](Image00850.jpg)
    .
  id: totrans-379
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 σ [*i* , *j*] 是估计器 *i* 和 *j* 的预测协方差； ![](Image00838.jpg) ![](Image00843.jpg)；和
    ![](Image00849.jpg) ![](Image00850.jpg)。
- en: The equation above shows that bagging is only effective to the extent that ![](Image00851.jpg)
    ; as ![](Image00223.jpg) . One of the goals of sequential bootstrapping (Chapter
    4) is to produce samples as independent as possible, thereby reducing ![](Image00003.jpg)
    , which should lower the variance of bagging classifiers.  [ Figure 6.1 ](text00001.html#filepos0000380113)
    plots the standard deviation of the bagged prediction as a function of *N* ∈ [5,
    30], ![](Image00007.jpg) and ![](Image00012.jpg) *.*
  id: totrans-380
  prefs: []
  type: TYPE_NORMAL
  zh: 上面的方程表明，袋装法仅在 ![](Image00851.jpg) 的范围内有效；当 ![](Image00223.jpg) 时。序列自助抽样（第4章）的一个目标是生成尽可能独立的样本，从而减少
    ![](Image00003.jpg)，这应该降低袋装分类器的方差。[图6.1](text00001.html#filepos0000380113) 绘制了袋装预测的标准差与
    *N* ∈ [5, 30] 的关系， ![](Image00007.jpg) 和 ![](Image00012.jpg) *.*
- en: '![](Image00083.jpg)'
  id: totrans-381
  prefs: []
  type: TYPE_IMG
  zh: '![](Image00083.jpg)'
- en: '[**Figure 6.1**](text00001.html#filepos0000379588) Standard deviation of the
    bagged prediction'
  id: totrans-382
  prefs: []
  type: TYPE_NORMAL
  zh: '[**图6.1**](text00001.html#filepos0000379588) 袋装预测的标准差'
- en: '**6.3.2 Improved Accuracy**'
  id: totrans-383
  prefs: []
  type: TYPE_NORMAL
  zh: '**6.3.2 提高准确性**'
- en: Consider a bagging classifier that makes a prediction on *k* classes by majority
    voting among *N* independent classifiers. We can label the predictions as {0,
    1}, where 1 means a correct prediction. The accuracy of a classifier is the probability
    *p* of labeling a prediction as 1\. On average we will get *Np* predictions labeled
    as 1, with variance *Np* (1 − *p* ). Majority voting makes the correct prediction
    when the most forecasted class is observed. For example, for *N* = 10 and *k*
    = 3, the bagging classifier made a correct prediction when class *A* was observed
    and the cast votes were [ *A* , *B* , *C* ] = [4, 3, 3]. However, the bagging
    classifier made an incorrect prediction when class *A* was observed and the cast
    votes were [ *A* , *B* , *C* ] = [4, 1, 5]. A sufficient condition is that the
    sum of these labels is ![](Image00016.jpg) . A necessary (non-sufficient) condition
    is that ![](Image00019.jpg) , which occurs with probability
  id: totrans-384
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑一个通过对 *N* 个独立分类器进行多数投票来对 *k* 类进行预测的袋装分类器。我们可以将预测标记为 {0, 1}，其中 1 表示正确预测。分类器的准确性是将预测标记为
    1 的概率 *p*。平均而言，我们将获得 *Np* 个标记为 1 的预测，方差为 *Np* (1 − *p*)。当观察到预测的类别是最多被预测的类别时，多数投票会做出正确的预测。例如，当
    *N* = 10 和 *k* = 3 时，当观察到类别 *A* 且投票结果为 [ *A* , *B* , *C* ] = [4, 3, 3] 时，袋装分类器做出了正确的预测。然而，当观察到类别
    *A* 且投票结果为 [ *A* , *B* , *C* ] = [4, 1, 5] 时，袋装分类器做出了错误的预测。一个充分条件是这些标签的总和是 ![](Image00016.jpg)。一个必要（但不充分）条件是
    ![](Image00019.jpg)，其发生的概率为
- en: '![](Image00138.jpg)'
  id: totrans-385
  prefs: []
  type: TYPE_IMG
  zh: '![](Image00138.jpg)'
- en: The implication is that for a sufficiently large *N* , say ![](Image00026.jpg)
    , then ![](Image00174.jpg) , hence the bagging classifier's accuracy exceeds the
    average accuracy of the individual classifiers. Snippet 6.1 implements this calculation.
  id: totrans-386
  prefs: []
  type: TYPE_NORMAL
  zh: 这意味着对于一个足够大的 *N*，例如 ![](Image00026.jpg)，那么 ![](Image00174.jpg)，因此袋装分类器的准确性超过了个体分类器的平均准确性。片段
    6.1 实现了此计算。
- en: '**SNIPPET 6.1 ACCURACY OF THE BAGGING CLASSIFIER**'
  id: totrans-387
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '**片段 6.1 袋装分类器的准确性**'
- en: '![](Image00031.jpg)'
  id: totrans-388
  prefs:
  - PREF_BQ
  type: TYPE_IMG
  zh: '![](Image00031.jpg)'
- en: 'This is a strong argument in favor of bagging any classifier in general, when
    computational requirements permit it. However, unlike boosting, bagging cannot
    improve the accuracy of poor classifiers: If the individual learners are poor
    classifiers ( ![](Image00035.jpg) ), majority voting will still perform poorly
    (although with lower variance).  [ Figure 6.2 ](text00001.html#filepos0000384361)
    illustrates these facts. Because it is easier to achieve ![](Image00037.jpg) than
    ![](Image00041.jpg) , bagging is more likely to be successful in reducing variance
    than in reducing bias.'
  id: totrans-389
  prefs: []
  type: TYPE_NORMAL
  zh: 这是支持一般情况下袋装任何分类器的有力论据，前提是计算要求允许。然而，与提升方法不同，袋装方法无法改善差分类器的准确性：如果个体学习者是差分类器（ ![](Image00035.jpg)），多数投票仍然表现不佳（尽管方差较低）。
    [图 6.2](text00001.html#filepos0000384361) 阐明了这些事实。因为实现 ![](Image00037.jpg) 比实现
    ![](Image00041.jpg) 更容易，袋装方法在降低方差方面更有可能成功，而不是在降低偏差方面。
- en: For further analysis on this topic, the reader is directed to Condorcet's Jury
    Theorem. Although the theorem is derived for the purposes of majority voting in
    political science, the problem addressed by this theorem shares similarities with
    the above discussion.
  id: totrans-390
  prefs: []
  type: TYPE_NORMAL
  zh: 关于此主题的进一步分析，请参阅孔多塞的陪审团定理。尽管该定理是为了政治科学中的多数投票而推导的，但该定理所解决的问题与上述讨论有相似之处。
- en: '![](Image00269.jpg)'
  id: totrans-391
  prefs: []
  type: TYPE_IMG
  zh: '![](Image00269.jpg)'
- en: '[**Figure 6.2**](text00001.html#filepos0000383462) Accuracy of a bagging classifier
    as a function of the individual estimator''s accuracy (*P* ), the number of estimators
    (*N* ), and *k * = 2'
  id: totrans-392
  prefs: []
  type: TYPE_NORMAL
  zh: '[**图 6.2**](text00001.html#filepos0000383462) 袋装分类器的准确性作为个体估计器的准确性 (*P* )、估计器数量
    (*N* ) 和 *k* = 2 的函数'
- en: '**6.3.3 Observation Redundancy**'
  id: totrans-393
  prefs: []
  type: TYPE_NORMAL
  zh: '**6.3.3 观察冗余**'
- en: In Chapter 4 we studied one reason why financial observations cannot be assumed
    to be IID. Redundant observations have two detrimental effects on bagging. First,
    the samples drawn with replacement are more likely to be virtually identical,
    even if they do not share the same observations. This makes ![](Image00046.jpg)
    , and bagging will not reduce variance, regardless of *N.* For example, if each
    observation at *t* is labeled according to the return between *t* and *t + 100*
    , we should sample 1% of the observations per bagged estimator, but not more.
    Chapter 4, Section 4.5 recommended three alternative solutions, one of which consisted
    of setting `max_samples=out[‘tW’].mean()` in sklearn's implementation of the bagging
    classifier class. Another (better) solution was to apply the sequential bootstrap
    method.
  id: totrans-394
  prefs: []
  type: TYPE_NORMAL
  zh: 在第4章中，我们研究了为什么金融观察不能假设为独立同分布（IID）的原因。冗余观察对自助法有两个有害影响。首先，抽样替换的样本更有可能几乎相同，即使它们不共享相同的观察结果。这使得！[](Image00046.jpg)，自助法不会降低方差，无论*N*。例如，如果每个时间点*t*的观察根据*t*与*t
    + 100*之间的回报进行标记，我们应该每个袋装估计器抽样1%的观察，而不是更多。第4章第4.5节推荐了三种替代解决方案，其中之一是在sklearn自助分类器类的实现中设置`max_samples=out[‘tW’].mean()`。另一种（更好的）解决方案是应用序贯自助法。
- en: The second detrimental effect from observation redundancy is that out-of-bag
    accuracy will be inflated. This happens because random sampling with replacement
    places in the training set samples that are very similar to those out-of-bag.
    In such a case, a proper stratified k-fold cross-validation without shuffling
    before partitioning will show a much lower testing-set accuracy than the one estimated
    out-of-bag. For this reason, it is advisable to set `StratifiedKFold(n_splits=k,`
    `shuffle=False)` when using that sklearn class, cross-validate the bagging classifier,
    and ignore the out-of-bag accuracy results. A low number *k* is preferred to a
    high one, as excessive partitioning would again place in the testing set samples
    too similar to those used in the training set.
  id: totrans-395
  prefs: []
  type: TYPE_NORMAL
  zh: 观察冗余的第二个有害影响是袋外准确性会被夸大。这是因为随机抽样替换会在训练集中放入与袋外样本非常相似的样本。在这种情况下，适当的分层k折交叉验证（在分区前不洗牌）将显示出测试集准确性远低于袋外估计值。因此，建议在使用该sklearn类时设置`StratifiedKFold(n_splits=k,`
    `shuffle=False)`来交叉验证自助分类器，并忽略袋外准确性结果。较低的数值*k*比较高的更为可取，因为过度分区会导致测试集样本与训练集样本过于相似。
- en: '**6.4 Random Forest**'
  id: totrans-396
  prefs: []
  type: TYPE_NORMAL
  zh: '**6.4 随机森林**'
- en: Decision trees are known to be prone to overfitting, which increases the variance
    of the forecasts. ^([3](text00001.html#filepos0000403445)) In order to address
    this concern, the random forest (RF) method was designed to produce ensemble forecasts
    with lower variance.
  id: totrans-397
  prefs: []
  type: TYPE_NORMAL
  zh: 决策树被认为容易过拟合，这会增加预测的方差。^([3](text00001.html#filepos0000403445)) 为了解决这个问题，随机森林（RF）方法被设计用来生成具有更低方差的集成预测。
- en: 'RF shares some similarities with bagging, in the sense of training independently
    individual estimators over bootstrapped subsets of the data. The key difference
    with bagging is that random forests incorporate a second level of randomness:
    When optimizing each node split, only a random subsample (without replacement)
    of the attributes will be evaluated, with the purpose of further decorrelating
    the estimators.'
  id: totrans-398
  prefs: []
  type: TYPE_NORMAL
  zh: 随机森林与自助法（bagging）有一些相似之处，即在对数据的自助子集上独立训练个体估计器。与自助法的主要区别在于，随机森林引入了第二层随机性：在优化每个节点分裂时，仅评估随机属性子样本（不替换），目的是进一步去相关化估计器。
- en: Like bagging, RF reduces forecasts’ variance without overfitting (remember,
    as long as ![](Image00050.jpg) ). A second advantage is that RF evaluates feature
    importance, which we will discuss in depth in Chapter 8\. A third advantage is
    that RF provides out-of-bag accuracy estimates, however in financial applications
    they are likely to be inflated (as discussed in Section 6.3.3). But like bagging,
    RF will not necessarily exhibit lower bias than individual decision trees.
  id: totrans-399
  prefs: []
  type: TYPE_NORMAL
  zh: 像自助法一样，随机森林在不发生过拟合的情况下降低预测方差（记住，只要！[](Image00050.jpg)）。第二个优点是，随机森林评估特征的重要性，我们将在第八章深入讨论。第三个优点是，随机森林提供了袋外准确性估计，然而在金融应用中，它们可能被夸大（如第6.3.3节讨论的那样）。但是，像自助法一样，随机森林未必会表现出比单个决策树更低的偏差。
- en: 'If a large number of samples are redundant (non-IID), overfitting will still
    take place: Sampling randomly with replacement will build a large number of essentially
    identical trees ( ![](Image00322.jpg) ), where each decision tree is overfit (a
    flaw for which decision trees are notorious). Unlike bagging, RF always fixes
    the size of the bootstrapped samples to match the size of the training dataset.
    Let us review ways we can address this RF overfitting problem in sklearn. For
    illustration purposes, I will refer to sklearn''s classes; however, these solutions
    can be applied to any implementation:'
  id: totrans-400
  prefs: []
  type: TYPE_NORMAL
  zh: 如果大量样本是冗余的（非独立同分布），仍然会发生过拟合：带替换的随机抽样将构建大量本质上相同的树（ ![](Image00322.jpg)），其中每棵决策树都是过拟合的（决策树臭名昭著的缺陷）。与袋装法不同，随机森林始终将自助样本的大小固定为与训练数据集大小相匹配。让我们回顾一下在sklearn中解决RF过拟合问题的方法。为了便于说明，我将引用sklearn的类；但是，这些解决方案可以应用于任何实现：
- en: Set a parameter `max_features` to a lower value, as a way of forcing discrepancy
    between trees.
  id: totrans-401
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将参数`max_features`设置为较低值，以强制树之间产生差异。
- en: 'Early stopping: Set the regularization parameter `min_weight_fraction_leaf`
    to a sufficiently large value (e.g., 5%) such that out-of-bag accuracy converges
    to out-of-sample (k-fold) accuracy.'
  id: totrans-402
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 提前停止：将正则化参数`min_weight_fraction_leaf`设置为足够大的值（例如5%），以使袋外准确率收敛于样本外（k折）准确率。
- en: Use `BaggingClassifier` on `DecisionTreeClassifier` where `max_samples` is set
    to the average uniqueness (`avgU` ) between samples.
  id: totrans-403
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在`DecisionTreeClassifier`上使用`BaggingClassifier`，其中`max_samples`设置为样本之间的平均唯一性（`avgU`）。
- en: '`clf=DecisionTreeClassifier(criterion=‘entropy’,max_features=‘auto’,class_weight=‘balanced’)`'
  id: totrans-404
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`clf=DecisionTreeClassifier(criterion=‘entropy’,max_features=‘auto’,class_weight=‘balanced’)`'
- en: '`bc=BaggingClassifier(base_estimator=clf,n_estimators=1000,max_samples=avgU,max_features=1.)`'
  id: totrans-405
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`bc=BaggingClassifier(base_estimator=clf,n_estimators=1000,max_samples=avgU,max_features=1.)`'
- en: Use `BaggingClassifier` on `RandomForestClassifier` where `max_samples` is set
    to the average uniqueness (`avgU` ) between samples.
  id: totrans-406
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在`RandomForestClassifier`上使用`BaggingClassifier`，其中`max_samples`设置为样本之间的平均唯一性（`avgU`）。
- en: '`clf=RandomForestClassifier(n_estimators=1,criterion=‘entropy’,bootstrap=False,class_weight=‘balanced_subsample’)`'
  id: totrans-407
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`clf=RandomForestClassifier(n_estimators=1,criterion=‘entropy’,bootstrap=False,class_weight=‘balanced_subsample’)`'
- en: '`bc=BaggingClassifier(base_estimator=clf,n_estimators=1000,max_samples=avgU,max_features=1.)`'
  id: totrans-408
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`bc=BaggingClassifier(base_estimator=clf,n_estimators=1000,max_samples=avgU,max_features=1.)`'
- en: Modify the RF class to replace standard bootstrapping with sequential bootstrapping.
  id: totrans-409
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 修改RF类以用顺序自助抽样替代标准自助抽样。
- en: In summary, Snippet 6.2 demonstrates three alternative ways of setting up an
    RF, using different classes.
  id: totrans-410
  prefs: []
  type: TYPE_NORMAL
  zh: 总结来说，片段6.2展示了使用不同类设置RF的三种替代方法。
- en: '**SNIPPET 6.2 THREE WAYS OF SETTING UP AN RF**'
  id: totrans-411
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '**片段6.2 三种设置RF的方法**'
- en: '![](Image00055.jpg)'
  id: totrans-412
  prefs:
  - PREF_BQ
  type: TYPE_IMG
  zh: '![](Image00055.jpg)'
- en: When fitting decision trees, a rotation of the features space in a direction
    that aligns with the axes typically reduces the number of levels needed by the
    tree. For this reason, I suggest you fit RF on a PCA of the features, as that
    may speed up calculations and reduce some overfitting (more on this in Chapter
    8). Also, as discussed in Chapter 4, Section 4.8, `class_weight=‘balanced_subsample’`
    will help you prevent the trees from misclassifying minority classes.
  id: totrans-413
  prefs: []
  type: TYPE_NORMAL
  zh: 在拟合决策树时，特征空间朝着与轴对齐的方向旋转通常会减少树所需的层数。因此，我建议你在特征的主成分分析（PCA）上拟合随机森林（RF），这样可能会加速计算并减少一些过拟合（在第8章中会详细讨论）。此外，如第4章第4.8节所述，`class_weight=‘balanced_subsample’`将帮助你防止树错误分类少数类。
- en: '**6.5 Boosting**'
  id: totrans-414
  prefs: []
  type: TYPE_NORMAL
  zh: '**6.5 提升**'
- en: 'Kearns and Valiant [1989] were among the first to ask whether one could combine
    weak estimators in order to achieve one with high accuracy. Shortly after, Schapire
    [1990] demonstrated that the answer to that question was affirmative, using the
    procedure we today call boosting. In general terms, it works as follows: First,
    generate one training set by random sampling with replacement, according to some
    sample weights (initialized with uniform weights). Second, fit one estimator using
    that training set. Third, if the single estimator achieves an accuracy greater
    than the acceptance threshold (e.g., 50% in a binary classifier, so that it performs
    better than chance), the estimator is kept, otherwise it is discarded. Fourth,
    give more weight to misclassified observations, and less weight to correctly classified
    observations. Fifth, repeat the previous steps until *N* estimators are produced.
    Sixth, the ensemble forecast is the *weighted* average of the individual forecasts
    from the *N* models, where the weights are determined by the accuracy of the individual
    estimators. There are many boosting algorithms, of which AdaBoost is one of the
    most popular (Geron [2017]). [Figure 6.3](text00001.html#filepos0000393759) summarizes
    the decision flow of a standard AdaBoost implementation.'
  id: totrans-415
  prefs: []
  type: TYPE_NORMAL
  zh: Kearns和Valiant [1989] 是最早提出是否可以结合弱估计器以获得高准确度的研究者之一。随后，Schapire [1990]证明了这个问题的答案是肯定的，使用了我们今天称之为boosting的过程。一般来说，它的工作原理如下：首先，根据某些样本权重（初始化为均匀权重），通过随机抽样带回生成一个训练集。其次，使用该训练集拟合一个估计器。第三，如果单个估计器的准确度超过接受阈值（例如，二分类器中的50%，使其表现优于随机），则保留该估计器，否则将其丢弃。第四，对误分类的观测值给予更多权重，而对正确分类的观测值给予较少权重。第五，重复前面的步骤，直到产生*N*个估计器。第六，集成预测是来自*N*个模型的个别预测的*加权*平均，其中权重由个别估计器的准确度决定。有许多boosting算法，其中AdaBoost是最受欢迎的算法之一（Geron
    [2017]）。[图6.3](text00001.html#filepos0000393759)总结了标准AdaBoost实现的决策流程。
- en: '![](Image00060.jpg)'
  id: totrans-416
  prefs: []
  type: TYPE_IMG
  zh: '![](Image00060.jpg)'
- en: '[**Figure 6.3**](text00001.html#filepos0000393467) AdaBoost decision flow'
  id: totrans-417
  prefs: []
  type: TYPE_NORMAL
  zh: '[**图6.3**](text00001.html#filepos0000393467) AdaBoost决策流程'
- en: '**6.6 Bagging vs. Boosting in Finance**'
  id: totrans-418
  prefs: []
  type: TYPE_NORMAL
  zh: '**6.6 金融中的Bagging与Boosting**'
- en: 'From the above description, a few aspects make boosting quite different from
    bagging: ^([4](text00001.html#filepos0000403829))'
  id: totrans-419
  prefs: []
  type: TYPE_NORMAL
  zh: 从上述描述来看，boosting与bagging有几个方面的显著不同：^([4](text00001.html#filepos0000403829))
- en: Individual classifiers are fit sequentially.
  id: totrans-420
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 各个分类器是顺序拟合的。
- en: Poor-performing classifiers are dismissed.
  id: totrans-421
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 表现不佳的分类器会被淘汰。
- en: Observations are weighted differently in each iteration.
  id: totrans-422
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在每次迭代中，观测值的权重不同。
- en: The ensemble forecast is a weighted average of the individual learners.
  id: totrans-423
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 集成预测是各个学习者的加权平均。
- en: Boosting's main advantage is that it reduces both variance and bias in forecasts.
    However, correcting bias comes at the cost of greater risk of overfitting. It
    could be argued that in financial applications bagging is generally preferable
    to boosting. Bagging addresses overfitting, while boosting addresses underfitting.
    Overfitting is often a greater concern than underfitting, as it is not difficult
    to overfit an ML algorithm to financial data, because of the low signal-to-noise
    ratio. Furthermore, bagging can be parallelized, while generally boosting requires
    sequential running.
  id: totrans-424
  prefs: []
  type: TYPE_NORMAL
  zh: Boosting的主要优势在于它能降低预测中的方差和偏差。然而，纠正偏差的代价是增加过拟合的风险。在金融应用中，可以认为bagging通常优于boosting。Bagging解决过拟合问题，而boosting则解决欠拟合问题。过拟合往往比欠拟合更令人担忧，因为在金融数据上对机器学习算法进行过拟合并不困难，原因是信噪比低。此外，bagging可以并行处理，而boosting通常需要顺序运行。
- en: '**6.7 Bagging for Scalability**'
  id: totrans-425
  prefs: []
  type: TYPE_NORMAL
  zh: '**6.7 可扩展性的Bagging**'
- en: As you know, several popular ML algorithms do not scale well with the sample
    size. Support vector machines (SVMs) are a prime example. If you attempt to fit
    an SVM on a million observations, it may take a while until the algorithm converges.
    And even once it has converged, there is no guarantee that the solution is a global
    optimum, or that it is not overfit.
  id: totrans-426
  prefs: []
  type: TYPE_NORMAL
  zh: 众所周知，几种流行的机器学习算法在样本量增加时扩展性较差。支持向量机（SVM）就是一个典型的例子。如果你尝试在一百万个观测值上拟合SVM，算法可能需要一段时间才能收敛。即使它已经收敛，也不能保证解是全局最优的，或者不是过拟合的。
- en: One practical approach is to build a bagging algorithm, where the base estimator
    belongs to a class that does not scale well with the sample size, like SVM. When
    defining that base estimator, we will impose a tight early stopping condition.
    For example, in sklearn's SVM implementation, you could set a low value for the
    `max_iter` parameter, say 1E5 iterations. The default value is `max_iter=-1` ,
    which tells the estimator to continue performing iterations until errors fall
    below a tolerance level. Alternatively, you could raise the tolerance level through
    the parameter `tol` , which has a default value `tol=1E-3` . Either of these two
    parameters will force an early stop. You can stop other algorithms early with
    equivalent parameters, like the number of levels in an RF ( `max_depth` ), or
    the minimum weighted fraction of the sum total of weights (of all the input samples)
    required to be at a leaf node ( `min_weight_fraction_leaf` ).
  id: totrans-427
  prefs: []
  type: TYPE_NORMAL
  zh: 一种实用的方法是构建一个袋装算法，其中基础估计器属于一种对样本大小扩展不佳的类别，比如支持向量机（SVM）。在定义该基础估计器时，我们将施加严格的早期停止条件。例如，在
    sklearn 的 SVM 实现中，你可以为 `max_iter` 参数设置一个较低的值，比如 1E5 次迭代。默认值是 `max_iter=-1`，这告诉估计器在错误降到容忍水平之前继续进行迭代。或者，你可以通过参数
    `tol` 提高容忍水平，默认值为 `tol=1E-3`。这两个参数中的任何一个都会强制早期停止。你可以用等效参数提前停止其他算法，如随机森林中的层数（`max_depth`）或到达叶子节点所需的输入样本总权重的最小加权比例（`min_weight_fraction_leaf`）。
- en: Given that bagging algorithms can be parallelized, we are transforming a large
    sequential task into many smaller ones that are run simultaneously. Of course,
    the early stopping will increase the variance of the outputs from the individual
    base estimators; however, that increase can be more than offset by the variance
    reduction associated with the bagging algorithm. You can control that reduction
    by adding more independent base estimators. Used in this way, bagging will allow
    you to achieve fast and robust estimates on extremely large datasets.
  id: totrans-428
  prefs: []
  type: TYPE_NORMAL
  zh: 鉴于袋装算法可以并行化，我们将一个大型顺序任务转化为多个同时运行的小任务。当然，早期停止会增加来自单个基础估计器的输出方差；然而，这种增加可以被与袋装算法相关的方差减少所抵消。通过添加更多独立的基础估计器，你可以控制这种减少。以这种方式使用袋装算法将使你在极大数据集上实现快速而稳健的估计。
- en: '**Exercises**'
  id: totrans-429
  prefs: []
  type: TYPE_NORMAL
  zh: '**练习**'
- en: Why is bagging based on random sampling with replacement? Would bagging still
    reduce a forecast's variance if sampling were without replacement?
  id: totrans-430
  prefs:
  - PREF_OL
  - PREF_BQ
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 为什么袋装基于有放回的随机抽样？如果采样是无放回的，袋装是否仍会减少预测的方差？
- en: Suppose that your training set is based on highly overlap labels (i.e., with
    low uniqueness, as defined in Chapter 4).
  id: totrans-431
  prefs:
  - PREF_OL
  - PREF_BQ
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 假设你的训练集基于高度重叠的标签（即，低唯一性，如第4章所定义）。
- en: Does this make bagging prone to overfitting, or just ineffective? Why?
  id: totrans-432
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 这使得袋装算法容易过拟合，还是仅仅无效？为什么？
- en: Is out-of-bag accuracy generally reliable in financial applications? Why?
  id: totrans-433
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 袋外准确率在金融应用中一般可靠吗？为什么？
- en: Build an ensemble of estimators, where the base estimator is a decision tree.
  id: totrans-434
  prefs:
  - PREF_OL
  - PREF_BQ
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 构建一个估计器集成，其中基础估计器是决策树。
- en: How is this ensemble different from an RF?
  id: totrans-435
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 这个集成与随机森林有什么不同？
- en: Using sklearn, produce a bagging classifier that behaves like an RF. What parameters
    did you have to set up, and how?
  id: totrans-436
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用 sklearn，产生一个表现如随机森林的袋装分类器。你需要设置哪些参数，如何设置？
- en: 'Consider the relation between an RF, the number of trees it is composed of,
    and the number of features utilized:'
  id: totrans-437
  prefs:
  - PREF_OL
  - PREF_BQ
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 考虑随机森林、其组成树的数量以及所使用特征数量之间的关系：
- en: Could you envision a relation between the minimum number of trees needed in
    an RF and the number of features utilized?
  id: totrans-438
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 你能设想在随机森林中所需的最小树木数量与所使用的特征数量之间的关系吗？
- en: Could the number of trees be too small for the number of features used?
  id: totrans-439
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 树的数量是否可能对于所使用的特征数量过少？
- en: Could the number of trees be too high for the number of observations available?
  id: totrans-440
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 树的数量是否可能对于可用的观察数量过高？
- en: How is out-of-bag accuracy different from stratified k-fold (with shuffling)
    cross-validation accuracy?
  id: totrans-441
  prefs:
  - PREF_OL
  - PREF_BQ
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 袋外准确率与分层 k 折（带洗牌）交叉验证准确率有什么不同？
- en: '**References**'
  id: totrans-442
  prefs: []
  type: TYPE_NORMAL
  zh: '**参考文献**'
- en: 'Geron, A. (2017): *Hands-on Machine Learning with Scikit-Learn and TensorFlow:
    Concepts, Tools, and Techniques to Build Intelligent Systems* , 1st edition. O''Reilly
    Media.'
  id: totrans-443
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 'Geron, A. (2017): *动手学机器学习与 Scikit-Learn 和 TensorFlow: 构建智能系统的概念、工具和技术*，第1版。O''Reilly
    Media。'
- en: 'Kearns, M. and L. Valiant (1989): “Cryptographic limitations on learning Boolean
    formulae and finite automata.” In Proceedings of the 21st Annual ACM Symposium
    on Theory of Computing, pp. 433–444, New York. Association for Computing Machinery.'
  id: totrans-444
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Kearns, M. 和 L. Valiant (1989)： “关于学习布尔公式和有限自动机的密码学限制。” 见第21届年度ACM计算理论研讨会论文集，第433–444页，纽约。计算机协会。
- en: 'Schapire, R. (1990): “The strength of weak learnability.” *Machine Learning*
    . Kluwer Academic Publishers. Vol. 5 No. 2, pp. 197–227.'
  id: totrans-445
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Schapire, R. (1990)： “弱学习能力的强度。” *机器学习*。Kluwer学术出版社。第5卷第2期，第197–227页。
- en: '**Bibliography**'
  id: totrans-446
  prefs: []
  type: TYPE_NORMAL
  zh: '**参考文献**'
- en: 'Gareth, J., D. Witten, T. Hastie, and R. Tibshirani (2013): *An Introduction
    to Statistical Learning: With Applications in R* , 1st ed. Springer-Verlag.'
  id: totrans-447
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Gareth, J., D. Witten, T. Hastie 和 R. Tibshirani (2013)： *统计学习导论：R语言应用*，第1版。Springer-Verlag。
- en: 'Hackeling, G. (2014): *Mastering Machine Learning with Scikit-Learn* , 1st
    ed. Packt Publishing.'
  id: totrans-448
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Hackeling, G. (2014)： *掌握机器学习与Scikit-Learn*，第1版。Packt出版。
- en: 'Hastie, T., R. Tibshirani and J. Friedman (2016): *The Elements of Statistical
    Learning* , 2nd ed. Springer-Verlag.'
  id: totrans-449
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Hastie, T., R. Tibshirani 和 J. Friedman (2016)： *统计学习的元素*，第2版。Springer-Verlag。
- en: 'Hauck, T. (2014): *Scikit-Learn Cookbook* , 1st ed. Packt Publishing.'
  id: totrans-450
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Hauck, T. (2014)： *Scikit-Learn食谱*，第1版。Packt出版。
- en: 'Raschka, S. (2015): *Python Machine Learning* , 1st ed. Packt Publishing.'
  id: totrans-451
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Raschka, S. (2015)： *Python机器学习*，第1版。Packt出版。
- en: '**Notes**'
  id: totrans-452
  prefs: []
  type: TYPE_NORMAL
  zh: '**注释**'
- en: '^([1](text00001.html#filepos0000371661))    For an introduction to ensemble
    methods, please visit: [http://scikit-learn.org/stable/modules/ ensemble.html.](http://scikit-learn.org/stable/modules/ensemble.html.)'
  id: totrans-453
  prefs: []
  type: TYPE_NORMAL
  zh: ^([1](text00001.html#filepos0000371661))    要了解集成方法的介绍，请访问：[http://scikit-learn.org/stable/modules/ensemble.html.](http://scikit-learn.org/stable/modules/ensemble.html.)
- en: '^([2](text00001.html#filepos0000372338))    I would not typically cite Wikipedia,
    however, on this subject the user may find some of the illustrations in this article
    useful: [https://en.wikipedia.org/wiki/Bias%E2%80%93variance_tradeoff.](https://en.wikipedia.org/wiki/Bias%E2%80%93variance_tradeoff.)'
  id: totrans-454
  prefs: []
  type: TYPE_NORMAL
  zh: ^([2](text00001.html#filepos0000372338))    我通常不会引用维基百科，然而在这个主题上，用户可能会发现这篇文章中的一些插图很有用：[https://en.wikipedia.org/wiki/Bias%E2%80%93variance_tradeoff.](https://en.wikipedia.org/wiki/Bias%E2%80%93variance_tradeoff.)
- en: '^([3](text00001.html#filepos0000387116))    For an intuitive explanation of
    Random Forest, visit the following link: [https://quantdare.com/random -forest-many-is-better-than-one/.](https://quantdare.com/random-forest-many-is-better-than-one/.)'
  id: totrans-455
  prefs: []
  type: TYPE_NORMAL
  zh: ^([3](text00001.html#filepos0000387116))    要获得随机森林的直观解释，请访问以下链接：[https://quantdare.com/random-forest-many-is-better-than-one/.](https://quantdare.com/random-forest-many-is-better-than-one/.)
- en: '^([4](text00001.html#filepos0000394181))    For a visual explanation of the
    difference between bagging and boosting, visit: [https://quantdare.com/ what-is-the-difference-between-bagging-and-boosting/.](https://quantdare.com/what-is-the-difference-between-bagging-and-boosting/.)'
  id: totrans-456
  prefs: []
  type: TYPE_NORMAL
  zh: ^([4](text00001.html#filepos0000394181))    要获取装袋与提升之间差异的视觉解释，请访问：[https://quantdare.com/what-is-the-difference-between-bagging-and-boosting/.](https://quantdare.com/what-is-the-difference-between-bagging-and-boosting/.)
- en: '**CHAPTER 7**'
  id: totrans-457
  prefs: []
  type: TYPE_NORMAL
  zh: '**第7章**'
- en: '**Cross-Validation in Finance**'
  id: totrans-458
  prefs: []
  type: TYPE_NORMAL
  zh: '**金融中的交叉验证**'
- en: '**7.1 Motivation**'
  id: totrans-459
  prefs: []
  type: TYPE_NORMAL
  zh: '**7.1 动机**'
- en: The purpose of cross-validation (CV) is to determine the generalization error
    of an ML algorithm, so as to prevent overfitting. CV is yet another instance where
    standard ML techniques fail when applied to financial problems. Overfitting will
    take place, and CV will not be able to detect it. In fact, CV will contribute
    to overfitting through hyper-parameter tuning. In this chapter we will learn why
    standard CV fails in finance, and what can be done about it.
  id: totrans-460
  prefs: []
  type: TYPE_NORMAL
  zh: 交叉验证（CV）的目的是确定机器学习算法的泛化误差，以防止过拟合。CV在应用于金融问题时又是一个标准机器学习技术失效的例子。过拟合会发生，而CV无法检测到。实际上，CV通过超参数调优会加剧过拟合。在本章中，我们将学习为什么标准CV在金融中失效，以及可以采取什么措施来解决这个问题。
- en: '**7.2 The Goal of Cross-Validation**'
  id: totrans-461
  prefs: []
  type: TYPE_NORMAL
  zh: '**7.2 交叉验证的目标**'
- en: 'One of the purposes of ML is to learn the general structure of the data, so
    that we can produce predictions on future, unseen features. When we test an ML
    algorithm on the same dataset as was used for training, not surprisingly, we achieve
    spectacular results. When ML algorithms are misused that way, they are no different
    from file lossy-compression algorithms: They can summarize the data with extreme
    fidelity, yet with zero forecasting power.'
  id: totrans-462
  prefs: []
  type: TYPE_NORMAL
  zh: 机器学习的一个目的在于学习数据的一般结构，以便我们能够对未来未知特征进行预测。当我们在与训练时使用的同一数据集上测试机器学习算法时，结果往往令人惊讶地出色。当机器学习算法以这种方式被滥用时，它们与文件有损压缩算法并无二致：它们可以极高保真度地总结数据，但却没有任何预测能力。
- en: 'CV splits observations drawn from an IID process into two sets: the *training*
    set and the *testing* set. Each observation in the complete dataset belongs to
    one, and only one, set. This is done as to prevent leakage from one set into the
    other, since that would defeat the purpose of testing on unseen data. Further
    details can be found in the books and articles listed in the references section.'
  id: totrans-463
  prefs: []
  type: TYPE_NORMAL
  zh: 交叉验证将从 IID 过程中抽取的观察值划分为两个集合：*训练* 集和 *测试* 集。完整数据集中每个观察值仅属于一个集合。这是为了防止一个集合的信息泄漏到另一个集合中，因为那样会违背在未见数据上进行测试的目的。更多细节可以在参考文献部分列出的书籍和文章中找到。
- en: 'There are many alternative CV schemes, of which one of the most popular is
    k-fold CV. [Figure 7.1](text00001.html#filepos0000407324) illustrates the *k*
    train/test splits carried out by a k-fold CV, where *k* = 5\. In this scheme:'
  id: totrans-464
  prefs: []
  type: TYPE_NORMAL
  zh: 有许多替代的交叉验证方案，其中最流行的之一是 k 折交叉验证。[图 7.1](text00001.html#filepos0000407324)展示了
    k 折交叉验证所执行的 *k* 次训练/测试划分，其中 *k* = 5。在该方案中：
- en: The dataset is partitioned into *k* subsets.
  id: totrans-465
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 数据集被划分为 *k* 个子集。
- en: For *i = 1,…,k*
  id: totrans-466
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对于 *i = 1,…,k*
- en: The ML algorithm is trained on all subsets excluding *i.*
  id: totrans-467
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 机器学习算法在不包括 *i* 的所有子集上进行训练。
- en: The fitted ML algorithm is tested on *i.*
  id: totrans-468
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 拟合的机器学习算法在 *i* 上进行测试。
- en: '![](Image00063.jpg)'
  id: totrans-469
  prefs: []
  type: TYPE_IMG
  zh: '![](Image00063.jpg)'
- en: '[**Figure 7.1**](text00001.html#filepos0000406409) Train/test splits in a 5-fold
    CV scheme'
  id: totrans-470
  prefs: []
  type: TYPE_NORMAL
  zh: '[**图 7.1**](text00001.html#filepos0000406409) 在 5 折交叉验证方案中的训练/测试划分'
- en: The outcome from k-fold CV is a *kx1* array of cross-validated performance metrics.
    For example, in a binary classifier, the model is deemed to have learned something
    if the cross-validated accuracy is over 1/2, since that is the accuracy we would
    achieve by tossing a fair coin.
  id: totrans-471
  prefs: []
  type: TYPE_NORMAL
  zh: k 折交叉验证的结果是一个 *kx1* 的交叉验证性能指标数组。例如，在一个二元分类器中，如果交叉验证的准确率超过 1/2，则模型被认为学到了某些东西，因为那是我们抛一枚公平硬币所能达到的准确率。
- en: 'In finance, CV is typically used in two settings: model development (like hyper-parameter
    tuning) and backtesting. Backtesting is a complex subject that we will discuss
    thoroughly in Chapters 10–16\. In this chapter, we will focus on CV for model
    development.'
  id: totrans-472
  prefs: []
  type: TYPE_NORMAL
  zh: 在金融领域，交叉验证通常用于两种场景：模型开发（例如超参数调优）和回测。回测是一个复杂的主题，我们将在第 10 到 16 章中详细讨论。在本章中，我们将重点关注用于模型开发的交叉验证。
- en: '**7.3 Why K-Fold CV Fails in Finance**'
  id: totrans-473
  prefs: []
  type: TYPE_NORMAL
  zh: '**7.3 为什么 K 折交叉验证在金融中失败**'
- en: By now you may have read quite a few papers in finance that present k-fold CV
    evidence that an ML algorithm performs well. Unfortunately, it is almost certain
    that those results are wrong. One reason k-fold CV fails in finance is because
    observations cannot be assumed to be drawn from an IID process. A second reason
    for CV's failure is that the testing set is used multiple times in the process
    of developing a model, leading to multiple testing and selection bias. We will
    revisit this second cause of failure in Chapters 11–13\. For the time being, let
    us concern ourselves exclusively with the first cause of failure.
  id: totrans-474
  prefs: []
  type: TYPE_NORMAL
  zh: 到现在为止，你可能已经阅读了不少金融领域的论文，这些论文展示了 k 折交叉验证（k-fold CV）证据，表明某个机器学习算法表现良好。不幸的是，这些结果几乎可以肯定是错误的。k
    折交叉验证在金融中的失败原因之一是观察值不能被假定为来自独立同分布（IID）过程。交叉验证失败的第二个原因是，在模型开发过程中测试集被多次使用，导致多重测试和选择偏差。我们将在第
    11 到 13 章重新讨论这一第二个失败原因。暂时，我们只关注第一个失败原因。
- en: 'Leakage takes place when the training set contains information that also appears
    in the testing set. Consider a serially correlated feature *X* that is associated
    with labels *Y* that are formed on overlapping data:'
  id: totrans-475
  prefs: []
  type: TYPE_NORMAL
  zh: 当训练集包含也出现在测试集中的信息时，就会发生泄漏。考虑一个与重叠数据形成标签 *Y* 的序列相关特征 *X*：
- en: Because of the serial correlation, *X [*t*]* ≈ *X [*t* + 1]* .
  id: totrans-476
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 由于序列相关性，*X [*t*]* ≈ *X [*t* + 1]* 。
- en: Because labels are derived from overlapping datapoints, *Y [*t*]* ≈ *Y [*t*
    + 1]* .
  id: totrans-477
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 由于标签源自重叠的数据点，*Y [*t*]* ≈ *Y [*t* + 1]* 。
- en: By placing *t* and *t + 1* in different sets, information is leaked. When a
    classifier is first trained on ( *X [*t*]* , *Y [*t*]* ), and then it is asked
    to predict E[ *Y [*t* + 1]* | *X [*t* + 1]* ] based on an observed *X [*t* + 1]*
    , this classifier is more likely to achieve *Y [*t* + 1]* = E[ *Y [*t* + 1]* |
    *X [*t* + 1]* ] even if *X* is an irrelevant feature.
  id: totrans-478
  prefs: []
  type: TYPE_NORMAL
  zh: 通过将 *t* 和 *t + 1* 放置在不同的集合中，会导致信息泄漏。当分类器首次在 (*X [*t*]* , *Y [*t*]*) 上训练，然后根据观察到的
    *X [*t* + 1]* 预测 E[*Y [*t* + 1]* | *X [*t* + 1]*] 时，这个分类器更有可能实现 *Y [*t* + 1]*
    = E[*Y [*t* + 1]* | *X [*t* + 1]*]，即使 *X* 是一个无关特征。
- en: 'If *X* is a predictive feature, leakage will enhance the performance of an
    already valuable strategy. The problem is leakage in the presence of irrelevant
    features, as this leads to false discoveries. There are at least two ways to reduce
    the likelihood of leakage:'
  id: totrans-479
  prefs: []
  type: TYPE_NORMAL
  zh: 如果 *X* 是预测特征，泄漏将增强已经有价值策略的表现。问题在于存在不相关特征的泄漏，这会导致虚假发现。至少有两种方法可以减少泄漏的可能性：
- en: Drop from the training set any observation *i* where *Y [*i*]* is a function
    of information used to determine *Y [*j*]* , and *j* belongs to the testing set.
  id: totrans-480
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从训练集中删除任何观察 *i*，其中 *Y [*i*]* 是用于确定 *Y [*j*]* 的信息的函数，并且 *j* 属于测试集。
- en: For example, *Y [*i*]* and *Y [*j*]* should not span overlapping periods (see
    Chapter 4 for a discussion of sample uniqueness).
  id: totrans-481
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 例如，*Y [*i*]* 和 *Y [*j*]* 不应跨越重叠的时间段（见第4章关于样本独特性的讨论）。
- en: 'Avoid overfitting the classifier. In this way, even if some leakage occurs,
    the classifier will not be able to profit from it. Use:'
  id: totrans-482
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 避免对分类器的过拟合。这样，即使发生了一些泄漏，分类器也无法从中获利。使用：
- en: Early stopping of the base estimators (see Chapter 6).
  id: totrans-483
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 提前停止基础估计器（见第6章）。
- en: Bagging of classifiers, while controlling for oversampling on redundant examples,
    so that the individual classifiers are as diverse as possible.
  id: totrans-484
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对分类器进行集成，同时控制对冗余样本的过采样，使得个体分类器尽可能多样化。
- en: Set `max_samples` to the average uniqueness.
  id: totrans-485
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将 `max_samples` 设置为平均独特性。
- en: Apply sequential bootstrap (Chapter 4).
  id: totrans-486
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 应用序贯自助法（第4章）。
- en: Consider the case where *X [*i*]* and *X [*j*]* are formed on overlapping information,
    where *i* belongs to the training set and *j* belongs to the testing set. Is this
    a case of informational leakage? Not necessarily, as long as *Y [*i*]* and *Y
    [*j*]* are independent. For leakage to take place, it must occur that ( *X [*i*]*
    , *Y [*i*]* ) ≈ ( *X [*j*]* , *Y [*j*]* ), and it does not suffice that *X [*i*]*
    ≈ *X [*j*]* or even *Y [*i*]* ≈ *Y [*j*]* .
  id: totrans-487
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑 *X [*i*]* 和 *X [*j*]* 是基于重叠信息形成的情况，其中 *i* 属于训练集，*j* 属于测试集。这是信息泄漏的情况吗？不一定，只要
    *Y [*i*]* 和 *Y [*j*]* 是独立的。要发生泄漏，必须满足（ *X [*i*]* , *Y [*i*]* ）≈（ *X [*j*]* , *Y
    [*j*]* ），而仅仅 *X [*i*]* ≈ *X [*j*]* 或甚至 *Y [*i*]* ≈ *Y [*j*]* 是不够的。
- en: '**7.4 A Solution: Purged K-Fold CV**'
  id: totrans-488
  prefs: []
  type: TYPE_NORMAL
  zh: '**7.4 解决方案：清除 K-折 CV**'
- en: One way to reduce leakage is to purge from the training set all observations
    whose labels overlapped in time with those labels included in the testing set.
    I call this process “purging.” In addition, since financial features often incorporate
    series that exhibit serial correlation (like ARMA processes), we should eliminate
    from the training set observations that immediately follow an observation in the
    testing set. I call this process “embargo.”
  id: totrans-489
  prefs: []
  type: TYPE_NORMAL
  zh: 减少泄漏的一种方法是从训练集中清除与测试集中标签时间重叠的所有观察。我称这个过程为“清除”。此外，由于金融特征通常包含表现出序列相关性的序列（如 ARMA
    过程），我们应该从训练集中清除紧接着测试集中观察的观察。我称这个过程为“禁运”。
- en: '**7.4.1 Purging the Training Set**'
  id: totrans-490
  prefs: []
  type: TYPE_NORMAL
  zh: '**7.4.1 清除训练集**'
- en: Suppose a testing observation whose label *Y [*j*]* is decided based on the
    information set Φ [*j*] . In order to prevent the type of leakage described in
    the previous section, we would like to purge from the training set any observation
    whose label *Y [*i*]* is decided based on the information set Φ [*i*] , such that
    Φ [*i*] ∩Φ [*j*] = ∅ *.*
  id: totrans-491
  prefs: []
  type: TYPE_NORMAL
  zh: 假设一个测试观察，其标签 *Y [*j*]* 是基于信息集 Φ [*j*] 决定的。为了防止前一节所述的泄漏类型，我们希望从训练集中清除任何标签 *Y
    [*i*]* 是基于信息集 Φ [*i*] 决定的观察，以便 Φ [*i*] ∩Φ [*j*] = ∅ *。
- en: 'In particular, we will determine that there is informational overlap between
    two observations *i* and *j* whenever *Y [*i*]* and *Y [*j*]* are concurrent (see
    Chapter 4, Section 4.3), in the sense that both labels are contingent on at least
    one common random draw. For example, consider a label *Y [*j*]* that is a function
    of observations in the closed range *t* ∈ [ *t [*j* , 0]* , *t [*j* , 1]* ], *Y
    [*j*]* = *f* [[ *t [*j* , 0]* , *t [*j* , 1]* ]] (with some abuse of notation).
    For example, in the context of the triple-barrier labeling method (Chapter 3),
    it means that the label is the sign of the return spanning between price bars
    with indices *t [*j* , 0]* and *t [*j* , 1]* , that is ![](Image00065.jpg) . A
    label *Y [*i*] * = *f* [[ *t [*i*  , 0] * , *t [*i*  , 1] * ]] overlaps with *Y
    [*j*] * if any of the three sufficient conditions is met:'
  id: totrans-492
  prefs: []
  type: TYPE_NORMAL
  zh: 特别地，我们将确定两个观察值 *i* 和 *j* 之间存在信息重叠，当 *Y [*i*]* 和 *Y [*j*]* 同时发生时（参见第 4 章，第 4.3
    节），这意味着两个标签依赖于至少一个共同的随机抽样。例如，考虑一个标签 *Y [*j*]*，它是闭区间 *t* ∈ [ *t [*j* , 0]* , *t
    [*j* , 1]* ] 中观察值的函数，*Y [*j*]* = *f* [[ *t [*j* , 0]* , *t [*j* , 1]* ]]（在符号使用上有些滥用）。例如，在三重障碍标记法的背景下（第
    3 章），这意味着该标签是价格柱之间的回报符号，其索引为 *t [*j* , 0]* 和 *t [*j* , 1]*，即 ![](Image00065.jpg)。如果任一满足以下三条充分条件，标签
    *Y [*i*]* = *f* [[ *t [*i* , 0]* , *t [*i* , 1]* ]] 与 *Y [*j*]* 重叠：
- en: '*t [*j* , 0]* ≤ *t [*i* , 0]* ≤ *t [*j* , 1]*'
  id: totrans-493
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '*t [*j* , 0]* ≤ *t [*i* , 0]* ≤ *t [*j* , 1]*'
- en: '*t [*j* , 0]* ≤ *t [*i* , 1]* ≤ *t [*j* , 1]*'
  id: totrans-494
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '*t [*j* , 0]* ≤ *t [*i* , 1]* ≤ *t [*j* , 1]*'
- en: '*t [*i* , 0]* ≤ *t [*j* , 0]* ≤ *t [*j* , 1]* ≤ *t [*i* , 1]*'
  id: totrans-495
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '*t [*i* , 0]* ≤ *t [*j* , 0]* ≤ *t [*j* , 1]* ≤ *t [*i* , 1]*'
- en: 'Snippet 7.1 implements this purging of observations from the training set.
    If the testing set is contiguous, in the sense that no training observations occur
    between the first and last testing observation, then purging can be accelerated:
    The object `testTimes` can be a pandas series with a single item, spanning the
    entire testing set.'
  id: totrans-496
  prefs: []
  type: TYPE_NORMAL
  zh: 片段 7.1 实现了从训练集中清理观察值。如果测试集是连续的，即在第一个和最后一个测试观察值之间没有训练观察值，则可以加速清理：对象 `testTimes`
    可以是一个仅包含一个项目的 pandas 系列，跨越整个测试集。
- en: '**SNIPPET 7.1 PURGING OBSERVATION IN THE TRAINING SET**'
  id: totrans-497
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '**片段 7.1 清理训练集中的观察值**'
- en: '![](Image00068.jpg)'
  id: totrans-498
  prefs:
  - PREF_BQ
  type: TYPE_IMG
  zh: '![](Image00068.jpg)'
- en: 'When leakage takes place, performance improves merely by increasing *k* → *T*
    , where *T* is the number of bars. The reason is that the larger the number of
    testing splits, the greater the number of overlapping observations in the training
    set. In many cases, purging suffices to prevent leakage: Performance will improve
    as we increase *k* , because we allow the model to recalibrate more often. But
    beyond a certain value *k* *, performance will not improve, indicating that the
    backtest is not profiting from leaks. [Figure 7.2](text00001.html#filepos0000421908)
    plots one partition of the k-fold CV. The test set is surrounded by two train
    sets, generating two overlaps that must be purged to prevent leakage.'
  id: totrans-499
  prefs: []
  type: TYPE_NORMAL
  zh: 当泄露发生时，仅通过增加 *k* → *T* 来提高性能，其中 *T* 是柱的数量。原因是测试划分的数量越大，训练集中的重叠观察值数量就越多。在许多情况下，清理就足以防止泄露：随着
    *k* 的增加，性能将得到改善，因为我们允许模型更频繁地重新校准。但超过某个值 *k*，性能将不再改善，这表明回测未能从泄露中获利。[图 7.2](text00001.html#filepos0000421908)
    描绘了 k 折 CV 的一个划分。测试集被两个训练集包围，生成两个必须清理以防止泄露的重叠。
- en: '![](Image00071.jpg)'
  id: totrans-500
  prefs: []
  type: TYPE_IMG
  zh: '![](Image00071.jpg)'
- en: '[**Figure 7.2**](text00001.html#filepos0000421536) Purging overlap in the training
    set'
  id: totrans-501
  prefs: []
  type: TYPE_NORMAL
  zh: '[**图 7.2**](text00001.html#filepos0000421536) 清理训练集中的重叠'
- en: '**7.4.2 Embargo**'
  id: totrans-502
  prefs: []
  type: TYPE_NORMAL
  zh: '**7.4.2 禁运**'
- en: For those cases where purging is not able to prevent all leakage, we can impose
    an embargo on training observations *after* every test set. The embargo does not
    need to affect training observations prior to a test set, because training labels
    *Y [*i*]* = *f* [[ *t [*i* , 0]* , *t [*i* , 1]* ]], where *t [*i* , 1]* < *t
    [*j* , 0]* (training ends before testing begins), contain information that was
    available at the testing time *t [*j* , 0]* . In other words, we are only concerned
    with training labels *Y [*i*]* = *f* [[ *t [*i* , 0]* , *t [*i* , 1]* ]] that
    take place immediately after the test, *t [*j* , 1]* ≤ *t [*i* , 0]* ≤ *t [*j*
    , 1]* + *h.* We can implement this embargo period *h* by setting *Y [*j*]* = *f*
    [[ *t [*j* , 0]* , *t [*j* , 1]* + *h* ]] before purging. A small value *h* ≈
    .01 *T* often suffices to prevent all leakage, as can be confirmed by testing
    that performance does not improve indefinitely by increasing *k* → *T* . [Figure
    7.3](text00001.html#filepos0000425442) illustrates the embargoing of train observations
    immediately after the testing set. Snippet 7.2 implements the embargo logic.
  id: totrans-503
  prefs: []
  type: TYPE_NORMAL
  zh: 对于无法完全防止所有泄漏的情况，我们可以在每个测试集*之后*对训练观察施加禁令。禁令不需要影响测试集之前的训练观察，因为训练标签*Y [*i*]* =
    *f* [[ *t [*i* , 0]* , *t [*i* , 1]* ]]，其中 *t [*i* , 1]* < *t [*j* , 0]*（训练在测试开始之前结束），包含在测试时间*t
    [*j* , 0]*可用的信息。换句话说，我们只关注在测试后立即发生的训练标签*Y [*i*]* = *f* [[ *t [*i* , 0]* , *t [*i*
    , 1]* ]]，满足 *t [*j* , 1]* ≤ *t [*i* , 0]* ≤ *t [*j* , 1]* + *h*。我们可以通过在清除之前设置*Y
    [*j*]* = *f* [[ *t [*j* , 0]* , *t [*j* , 1]* + *h* ]]来实施这个禁令期*h*。一个小的值*h* ≈ .01
    *T* 通常足以防止所有泄漏，可以通过测试性能是否不会随着*k* → *T*而无限改善来确认。[图 7.3](text00001.html#filepos0000425442)说明了在测试集之后立即禁令训练观察。代码片段
    7.2 实现了禁令逻辑。
- en: '![](Image00074.jpg)'
  id: totrans-504
  prefs: []
  type: TYPE_IMG
  zh: '![](Image00074.jpg)'
- en: '[**Figure 7.3**](text00001.html#filepos0000425092) Embargo of post-test train
    observations'
  id: totrans-505
  prefs: []
  type: TYPE_NORMAL
  zh: '[**图 7.3**](text00001.html#filepos0000425092) 测试后训练观察的禁令'
- en: '**SNIPPET 7.2 EMBARGO ON TRAINING OBSERVATIONS**'
  id: totrans-506
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '**SNIPPET 7.2 训练观察的禁令**'
- en: '![](Image00078.jpg)'
  id: totrans-507
  prefs:
  - PREF_BQ
  type: TYPE_IMG
  zh: '![](Image00078.jpg)'
- en: '**7.4.3 The Purged K-Fold Class**'
  id: totrans-508
  prefs: []
  type: TYPE_NORMAL
  zh: '**7.4.3 清除的 K-Fold 类**'
- en: In the previous sections we have discussed how to produce training/testing splits
    when labels overlap. That introduced the notion of purging and embargoing, in
    the particular context of model development. In general, we need to purge and
    embargo overlapping training observations whenever we produce a train/test split,
    whether it is for hyper-parameter fitting, backtesting, or performance evaluation.
    Snippet 7.3 extends scikit-learn's `KFold` class to account for the possibility
    of leakages of testing information into the training set.
  id: totrans-509
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的章节中，我们讨论了在标签重叠时如何生成训练/测试划分。这引入了清除和禁令的概念，特别是在模型开发的背景下。一般来说，每当我们生成训练/测试划分时，无论是用于超参数调整、回测还是性能评估，我们都需要清除和禁令重叠的训练观察。代码片段
    7.3 扩展了 scikit-learn 的 `KFold` 类，以考虑测试信息泄漏到训练集的可能性。
- en: '**SNIPPET 7.3 CROSS-VALIDATION CLASS WHEN OBSERVATIONS OVERLAP**'
  id: totrans-510
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '**SNIPPET 7.3 当观察重叠时的交叉验证类**'
- en: '![](Image00461.jpg)'
  id: totrans-511
  prefs:
  - PREF_BQ
  type: TYPE_IMG
  zh: '![](Image00461.jpg)'
- en: '**7.5 Bugs in Sklearn''s Cross-Validation**'
  id: totrans-512
  prefs: []
  type: TYPE_NORMAL
  zh: '**7.5 Sklearn 交叉验证中的错误**'
- en: 'You would think that something as critical as cross-validation would be perfectly
    implemented in one of the most popular ML libraries. Unfortunately that is not
    the case, and this is one of the reasons you must always read all the code you
    run, and a strong point in favor of open source. One of the many upsides of open-source
    code is that you can verify everything and adjust it to your needs. Snippet 7.4
    addresses two known sklearn bugs:'
  id: totrans-513
  prefs: []
  type: TYPE_NORMAL
  zh: 你可能会认为，像交叉验证这样关键的东西应该在最流行的机器学习库之一中得到完美实现。不幸的是，情况并非如此，这也是你必须始终阅读所运行的所有代码的原因之一，也是开源的一个强有力的支持点。开源代码的许多优点之一是你可以验证所有内容并根据自己的需要进行调整。代码片段
    7.4 解决了两个已知的 sklearn 错误：
- en: 'Scoring functions do not know `classes_` , as a consequence of sklearn''s reliance
    on numpy arrays rather than pandas series: [https://github.com/scikit-learn/scikit-learn/issues/6231](https://github.com/scikit-learn/scikit-learn/issues/6231)'
  id: totrans-514
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 评分函数不知道 `classes_`，这是由于 sklearn 依赖于 numpy 数组而不是 pandas 系列：[https://github.com/scikit-learn/scikit-learn/issues/6231](https://github.com/scikit-learn/scikit-learn/issues/6231)
- en: '`cross_val_score` will give different results because it passes weights to
    the fit method, but not to the `log_loss` method: [https://github.com/scikit-learn/scikit-learn/issues/9144](https://github.com/scikit-learn/scikit-learn/issues/9144)'
  id: totrans-515
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`cross_val_score` 将给出不同的结果，因为它将权重传递给拟合方法，而不传递给 `log_loss` 方法：[https://github.com/scikit-learn/scikit-learn/issues/9144](https://github.com/scikit-learn/scikit-learn/issues/9144)'
- en: '**SNIPPET 7.4 USING THE** `**PURGEDKFOLD**` **CLASS**'
  id: totrans-516
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '**SNIPPET 7.4 使用** `**PURGEDKFOLD**` **类**'
- en: '![](Image00477.jpg)'
  id: totrans-517
  prefs:
  - PREF_BQ
  type: TYPE_IMG
  zh: '![](Image00477.jpg)'
- en: Please understand that it may take a long time until a fix for these bugs is
    agreed upon, implemented, tested, and released. Until then, you should use `cvScore`
    in Snippet 7.4, and avoid running the function `cross_val_score` .
  id: totrans-518
  prefs: []
  type: TYPE_NORMAL
  zh: 请理解，直到就这些错误达成一致、实施、测试和发布可能需要很长时间。在此之前，你应使用`cvScore`，并避免运行函数`cross_val_score`。
- en: '**Exercises**'
  id: totrans-519
  prefs: []
  type: TYPE_NORMAL
  zh: '**练习**'
- en: Why is shuffling a dataset before conducting k-fold CV generally a bad idea
    in finance? What is the purpose of shuffling? Why does shuffling defeat the purpose
    of k-fold CV in financial datasets?
  id: totrans-520
  prefs:
  - PREF_OL
  - PREF_BQ
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 为什么在进行k折CV之前洗牌数据集在金融领域通常是个坏主意？洗牌的目的是什么？洗牌为什么会削弱k折CV在金融数据集中的目的？
- en: Take a pair of matrices ( *X* , *y* ), representing observed features and labels.
    These could be one of the datasets derived from the exercises in Chapter 3.
  id: totrans-521
  prefs:
  - PREF_OL
  - PREF_BQ
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 取一对矩阵（*X*，*y*），表示观察到的特征和标签。这些可能是来自第3章练习的某个数据集。
- en: Derive the performance from a 10-fold CV of an RF classifier on (*X* , *y* ),
    without shuffling.
  id: totrans-522
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从(*X*，*y*)的RF分类器进行10折CV中推导性能，不进行洗牌。
- en: Derive the performance from a 10-fold CV of an RF on (*X* , *y* ), with shuffling.
  id: totrans-523
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从(*X*，*y*)的RF进行10折CV中推导性能，进行洗牌。
- en: Why are both results so different?
  id: totrans-524
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 为什么这两个结果如此不同？
- en: How does shuffling leak information?
  id: totrans-525
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 洗牌如何泄露信息？
- en: Take the same pair of matrices ( *X* , *y* ) you used in exercise 2.
  id: totrans-526
  prefs:
  - PREF_OL
  - PREF_BQ
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 取你在练习2中使用的相同矩阵对（*X*，*y*）。
- en: Derive the performance from a 10-fold purged CV of an RF on (*X* , *y* ), with
    1% embargo.
  id: totrans-527
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从(*X*，*y*)的RF进行10折排除CV中推导性能，保留1%的禁运。
- en: Why is the performance lower?
  id: totrans-528
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 为什么性能更低？
- en: Why is this result more realistic?
  id: totrans-529
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 为什么这个结果更现实？
- en: In this chapter we have focused on one reason why k-fold CV fails in financial
    applications, namely the fact that some information from the testing set leaks
    into the training set. Can you think of a second reason for CV's failure?
  id: totrans-530
  prefs:
  - PREF_OL
  - PREF_BQ
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 本章我们集中讨论了k折交叉验证在金融应用中失败的一个原因，即测试集的一些信息泄露到训练集中。你能想到CV失败的第二个原因吗？
- en: Suppose you try one thousand configurations of the same investment strategy,
    and perform a CV on each of them. Some results are guaranteed to look good, just
    by sheer luck. If you only publish those positive results, and hide the rest,
    your audience will not be able to deduce that these results are false positives,
    a statistical fluke. This phenomenon is called “selection bias.”
  id: totrans-531
  prefs:
  - PREF_OL
  - PREF_BQ
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 假设你尝试一千种相同投资策略的配置，并对每种配置进行CV。由于运气原因，某些结果肯定会看起来很好。如果你只发布这些积极的结果，并隐藏其余部分，观众将无法推断这些结果是虚假积极结果，统计上的偶然现象。这种现象称为“选择偏差”。
- en: Can you imagine one procedure to prevent this?
  id: totrans-532
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 你能想象一种防止这种情况的程序吗？
- en: 'What if we split the dataset in three sets: training, validation, and testing?
    The validation set is used to evaluate the trained parameters, and the testing
    is run only on the one configuration chosen in the validation phase. In what case
    does this procedure still fail?'
  id: totrans-533
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如果我们将数据集分成三个部分：训练集、验证集和测试集，会怎么样？验证集用于评估训练的参数，而测试集只在验证阶段选择的一种配置上运行。在什么情况下这个过程仍然失败？
- en: What is the key to avoiding selection bias?
  id: totrans-534
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 避免选择偏差的关键是什么？
- en: '**Bibliography**'
  id: totrans-535
  prefs: []
  type: TYPE_NORMAL
  zh: '**参考文献**'
- en: 'Bharat Rao, R., G. Fung, and R. Rosales (2008): “On the dangers of cross-validation:
    An experimental evaluation.” White paper, IKM CKS Siemens Medical Solutions USA.
    Available at [http://people.csail.mit.edu/romer/papers/CrossVal_SDM08.pdf](http://people.csail.mit.edu/romer/papers/CrossVal_SDM08.pdf)
    .'
  id: totrans-536
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Bharat Rao, R., G. Fung, 和 R. Rosales (2008)： “关于交叉验证的危险：实验评估。” IKM CKS西门子医疗解决方案白皮书。可在
    [http://people.csail.mit.edu/romer/papers/CrossVal_SDM08.pdf](http://people.csail.mit.edu/romer/papers/CrossVal_SDM08.pdf)
    获取。
- en: 'Bishop, C. (1995): *Neural Networks for Pattern Recognition* , 1st ed. Oxford
    University Press.'
  id: totrans-537
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Bishop, C. (1995)：*模式识别的神经网络*，第一版。牛津大学出版社。
- en: 'Breiman, L. and P. Spector (1992): “Submodel selection and evaluation in regression:
    The X-random case.” White paper, Department of Statistics, University of California,
    Berkeley. Available at [http://digitalassets.lib.berkeley.edu/sdtr/ucb/text/197.pdf](http://digitalassets.lib.berkeley.edu/sdtr/ucb/text/197.pdf)
    .'
  id: totrans-538
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Breiman, L. 和 P. Spector (1992)： “回归中的子模型选择与评估：X-随机情况。” 加州大学伯克利分校统计系白皮书。可在 [http://digitalassets.lib.berkeley.edu/sdtr/ucb/text/197.pdf](http://digitalassets.lib.berkeley.edu/sdtr/ucb/text/197.pdf)
    获取。
- en: 'Hastie, T., R. Tibshirani, and J. Friedman (2009): *The Elements of Statistical
    Learning* , 1st ed. Springer.'
  id: totrans-539
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Hastie, T., R. Tibshirani, 和 J. Friedman (2009)：*统计学习的要素*，第一版。斯普林格。
- en: 'James, G., D. Witten, T. Hastie and R. Tibshirani (2013): *An Introduction
    to Statistical Learning* , 1st ed. Springer.'
  id: totrans-540
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: James, G., D. Witten, T. Hastie 和 R. Tibshirani (2013)：*统计学习导论*，第1版。施普林格出版社。
- en: 'Kohavi, R. (1995): “A study of cross-validation and bootstrap for accuracy
    estimation and model selection.” International Joint Conference on Artificial
    Intelligence. Available at [http://web.cs.iastate.edu/∼jtian/cs573/Papers/Kohavi-IJCAI-95.pdf](http://web.cs.iastate.edu/~jtian/cs573/Papers/Kohavi-IJCAI-95.pdf)
    .'
  id: totrans-541
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Kohavi, R. (1995)： “交叉验证和自助法在准确性估计和模型选择中的研究。” 国际人工智能联合会议。可在[http://web.cs.iastate.edu/∼jtian/cs573/Papers/Kohavi-IJCAI-95.pdf](http://web.cs.iastate.edu/~jtian/cs573/Papers/Kohavi-IJCAI-95.pdf)获取。
- en: 'Ripley, B. (1996): *Pattern Recognition and Neural Networks* , 1st ed. Cambridge
    University Press.'
  id: totrans-542
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Ripley, B. (1996)：*模式识别与神经网络*，第1版。剑桥大学出版社。
- en: '**CHAPTER 8**'
  id: totrans-543
  prefs: []
  type: TYPE_NORMAL
  zh: '**第8章**'
- en: '**Feature Importance**'
  id: totrans-544
  prefs: []
  type: TYPE_NORMAL
  zh: '**特征重要性**'
- en: '**8.1 Motivation**'
  id: totrans-545
  prefs: []
  type: TYPE_NORMAL
  zh: '**8.1 动机**'
- en: 'One of the most pervasive mistakes in financial research is to take some data,
    run it through an ML algorithm, backtest the predictions, and repeat the sequence
    until a nice-looking backtest shows up. Academic journals are filled with such
    pseudo-discoveries, and even large hedge funds constantly fall into this trap.
    It does not matter if the backtest is a walk-forward out-of-sample. The fact that
    we are repeating a test over and over on the same data will likely lead to a false
    discovery. This methodological error is so notorious among statisticians that
    they consider it scientific fraud, and the American Statistical Association warns
    against it in its ethical guidelines (American Statistical Association [2016],
    Discussion #4). It typically takes about 20 such iterations to discover a (false)
    investment strategy subject to the standard significance level (false positive
    rate) of 5%. In this chapter we will explore why such an approach is a waste of
    time and money, and how feature importance offers an alternative.'
  id: totrans-546
  prefs: []
  type: TYPE_NORMAL
  zh: '财务研究中最普遍的错误之一是拿一些数据，运行机器学习算法，对预测进行回测，然后重复这一过程，直到出现一个看起来不错的回测结果。学术期刊充斥着这种伪发现，甚至大型对冲基金也常常陷入这个陷阱。无论回测是否为前向行走的样本外测试，我们在同一数据上反复测试的事实都可能导致错误发现。这个方法论错误在统计学家中如此臭名昭著，以至于他们将其视为科学欺诈，美国统计协会在其伦理准则中对此发出警告（美国统计协会[2016]，讨论
    #4）。通常需要大约20次这样的迭代才能发现一个（错误的）投资策略，符合5%的标准显著性水平（假阳性率）。在本章中，我们将探讨为何这种方法是浪费时间和金钱，以及特征重要性如何提供替代方案。'
- en: '**8.2 The Importance of Feature Importance**'
  id: totrans-547
  prefs: []
  type: TYPE_NORMAL
  zh: '**8.2 特征重要性的重要性**'
- en: A striking facet of the financial industry is that so many very seasoned portfolio
    managers (including many with a quantitative background) do not realize how easy
    it is to overfit a backtest. How to backtest properly is not the subject of this
    chapter; we will address that extremely important topic in Chapters 11–15\. The
    goal of this chapter is to explain one of the analyses that must be performed
    *before* any backtest is carried out.
  id: totrans-548
  prefs: []
  type: TYPE_NORMAL
  zh: 财务行业的一个显著特征是，许多经验丰富的投资组合经理（包括许多具有定量背景的人）并未意识到过度拟合回测是多么简单。如何正确进行回测并不是本章的主题；我们将在第11至15章中探讨这一极其重要的话题。本章的目标是解释在进行任何回测之前必须执行的分析之一。
- en: Suppose that you are given a pair of matrices ( *X* , *y* ), that respectively
    contain features and labels for a particular financial instrument. We can fit
    a classifier on ( *X* , *y* ) and evaluate the generalization error through a
    purged k-fold cross-validation (CV), as we saw in Chapter 7\. Suppose that we
    achieve good performance. The next natural question is to try to understand what
    features contributed to that performance. Maybe we could add some features that
    strengthen the signal responsible for the classifier's predictive power. Maybe
    we could eliminate some of the features that are only adding noise to the system.
    Notably, understanding feature importance opens up the proverbial black box. We
    can gain insight into the patterns identified by the classifier if we understand
    what source of information is indispensable to it. This is one of the reasons
    why the black box mantra is somewhat overplayed by the ML skeptics. Yes, the algorithm
    has learned without us directing the process (that is the whole point of ML!)
    in a black box, but that does not mean that we cannot (or should not) take a look
    at what the algorithm has found. Hunters do not blindly eat everything their smart
    dogs retrieve for them, do they?
  id: totrans-549
  prefs: []
  type: TYPE_NORMAL
  zh: 假设你得到了一对矩阵（*X*，*y*），分别包含特定金融工具的特征和标签。我们可以在（*X*，*y*）上拟合一个分类器，并通过经过清除的k折交叉验证（CV）来评估泛化误差，如第7章所述。假设我们取得了良好的性能。下一个自然的问题是尝试理解哪些特征对该性能有贡献。也许我们可以添加一些增强分类器预测能力的特征。也许我们可以消除一些仅为系统添加噪声的特征。值得注意的是，理解特征重要性打开了所谓的黑箱。如果我们了解分类器所需的信息来源，我们就能洞察分类器识别的模式。这也是机器学习怀疑论者对黑箱论调过分强调的原因之一。是的，算法在黑箱中学习，没有我们指导这个过程（这就是机器学习的全部意义！），但这并不意味着我们不能（或不应该）查看算法所发现的内容。猎人不会盲目地吃掉他们聪明的狗为他们找回的一切，不是吗？
- en: 'Once we have found what features are important, we can learn more by conducting
    a number of experiments. Are these features important all the time, or only in
    some specific environments? What triggers a change in importance over time? Can
    those regime switches be predicted? Are those important features also relevant
    to other related financial instruments? Are they relevant to other asset classes?
    What are the most relevant features across all financial instruments? What is
    the subset of features with the highest rank correlation across the entire investment
    universe? This is a much better way of researching strategies than the foolish
    backtest cycle. Let me state this maxim as one of the most critical lessons I
    hope you learn from this book:'
  id: totrans-550
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦我们找到了重要特征，我们可以通过进行多次实验来深入了解。 这些特征是否总是重要，还是仅在某些特定环境中重要？是什么导致了重要性的变化？这些转变可以预测吗？这些重要特征与其他相关金融工具相关吗？它们与其他资产类别相关吗？在所有金融工具中，最相关的特征是什么？在整个投资领域中，具有最高排名相关性的特征子集是什么？这是一种比愚蠢的回测周期更好的研究策略的方法。让我把这个格言作为我希望你从本书中学到的最关键的教训之一陈述：
- en: '**Snippet 8.1 Marcos’ First Law of Backtesting—Ignore at your own peril**'
  id: totrans-551
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '**片段8.1 马科斯的回测第一法则—自行承担后果**'
- en: “Backtesting is not a research tool. Feature importance is.”
  id: totrans-552
  prefs:
  - PREF_BQ
  - PREF_BQ
  type: TYPE_NORMAL
  zh: “回测不是研究工具。特征重要性才是。”
- en: Marcos López de Prado *Advances in Financial Machine Learning* (2018)
  id: totrans-553
  prefs:
  - PREF_BQ
  - PREF_BQ
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 马科斯·洛佩斯·德·普拉多 *金融机器学习的进展*（2018）
- en: '**8.3 Feature Importance with Substitution Effects**'
  id: totrans-554
  prefs: []
  type: TYPE_NORMAL
  zh: '**8.3 具有替代效应的特征重要性**'
- en: I find it useful to distinguish between feature importance methods based on
    whether they are impacted by substitution effects. In this context, a substitution
    effect takes place when the estimated importance of one feature is reduced by
    the presence of other related features. Substitution effects are the ML analogue
    of what the statistics and econometrics literature calls “multi-collinearity.”
    One way to address linear substitution effects is to apply PCA on the raw features,
    and then perform the feature importance analysis on the orthogonal features. See
    Belsley et al. [1980], Goldberger [1991, pp. 245–253], and Hill et al. [2001]
    for further details.
  id: totrans-555
  prefs: []
  type: TYPE_NORMAL
  zh: 我认为根据特征重要性方法是否受到替代效应的影响进行区分是有用的。在此背景下，当其他相关特征的存在降低了一个特征的估计重要性时，就会发生替代效应。替代效应是机器学习中类似于统计学和计量经济学文献所称的“多重共线性”的概念。解决线性替代效应的一种方法是对原始特征应用PCA，然后对正交特征进行特征重要性分析。有关更多细节，请参阅Belsley等人[1980]、Goldberger
    [1991，第245-253页]和Hill等人[2001]。
- en: '**8.3.1 Mean Decrease Impurity**'
  id: totrans-556
  prefs: []
  type: TYPE_NORMAL
  zh: '**8.3.1 平均减少杂质**'
- en: 'Mean decrease impurity (MDI) is a fast, explanatory-importance (in-sample,
    IS) method specific to tree-based classifiers, like RF. At each node of each decision
    tree, the selected feature splits the subset it received in such a way that impurity
    is decreased. Therefore, we can derive for each decision tree how much of the
    overall impurity decrease can be assigned to each feature. And given that we have
    a forest of trees, we can average those values across all estimators and rank
    the features accordingly. See Louppe et al. [2013] for a detailed description.
    There are some important considerations you must keep in mind when working with
    MDI:'
  id: totrans-557
  prefs: []
  type: TYPE_NORMAL
  zh: 平均降低杂质（MDI）是一种快速的、解释性的重要性（样本内，IS）方法，专门针对树基分类器，如 RF。在每棵决策树的每个节点，所选特征以减少杂质的方式划分其接收到的子集。因此，我们可以推导出每棵决策树中，整体杂质降低的多少可以归因于每个特征。考虑到我们有一片森林的树木，我们可以在所有估计器中平均这些值，并相应地对特征进行排名。有关详细描述，请参见
    Louppe 等人 [2013]。在使用 MDI 时，有一些重要的考虑因素需要记住：
- en: Masking effects take place when some features are systematically ignored by
    tree-based classifiers in favor of others. In order to avoid them, set `max_features=int(1)`
    when using sklearn's RF class. In this way, only one random feature is considered
    per level.
  id: totrans-558
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 遮蔽效应发生在某些特征被树基分类器系统性忽略以偏向其他特征的情况下。为了避免这种情况，在使用 sklearn 的 RF 类时设置 `max_features=int(1)`。这样，每层只考虑一个随机特征。
- en: Every feature is given a chance (at some random levels of some random trees)
    to reduce impurity.
  id: totrans-559
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 每个特征都有机会（在某些随机树的某些随机层次中）减少杂质。
- en: Make sure that features with zero importance are not averaged, since the only
    reason for a 0 is that the feature was not randomly chosen. Replace those values
    with `np.nan` .
  id: totrans-560
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 确保零重要性的特征不被平均，因为 0 的唯一原因是该特征没有被随机选择。将这些值替换为 `np.nan`。
- en: The procedure is obviously IS. Every feature will have some importance, even
    if they have no predictive power whatsoever.
  id: totrans-561
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 该过程显然是 IS。每个特征都会有一定的重要性，即使它们没有任何预测能力。
- en: MDI cannot be generalized to other non-tree based classifiers.
  id: totrans-562
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: MDI 不能推广到其他非树基分类器。
- en: By construction, MDI has the nice property that feature importances add up to
    1, and every feature importance is bounded between 0 and 1.
  id: totrans-563
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从构造上看，MDI 具有一个良好的特性，即特征重要性之和为 1，且每个特征的重要性介于 0 和 1 之间。
- en: 'The method does not address substitution effects in the presence of correlated
    features. MDI dilutes the importance of substitute features, because of their
    interchangeability: The importance of two identical features will be halved, as
    they are randomly chosen with equal probability.'
  id: totrans-564
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 该方法没有解决相关特征存在时的替代效应。由于特征的可互换性，MDI 稀释了替代特征的重要性：两个相同特征的重要性将减半，因为它们被随机选择的概率相等。
- en: Strobl et al. [2007] show experimentally that MDI is biased towards some predictor
    variables. White and Liu [1994] argue that, in case of single decision trees,
    this bias is due to an unfair advantage given by popular impurity functions toward
    predictors with a large number of categories.
  id: totrans-565
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Strobl 等人 [2007] 实验证明 MDI 对某些预测变量存在偏见。White 和 Liu [1994] 认为，在单棵决策树的情况下，这种偏见是由于流行的杂质函数对类别数量较多的预测变量给予了不公平的优势。
- en: Sklearn's `RandomForest` class implements MDI as the default feature importance
    score. This choice is likely motivated by the ability to compute MDI on the fly,
    with minimum computational cost. ^([1](text00001.html#filepos0000475986)) Snippet
    8.2 illustrates an implementation of MDI, incorporating the considerations listed
    earlier.
  id: totrans-566
  prefs: []
  type: TYPE_NORMAL
  zh: Sklearn 的 `RandomForest` 类将 MDI 实现为默认的特征重要性评分。这一选择可能是因为能够以最低的计算成本实时计算 MDI。^([1](text00001.html#filepos0000475986))
    片段 8.2 说明了 MDI 的实现，包含了前面提到的考虑因素。
- en: '**SNIPPET 8.2 MDI FEATURE IMPORTANCE**'
  id: totrans-567
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '**片段 8.2 MDI 特征重要性**'
- en: '![](Image00087.jpg)'
  id: totrans-568
  prefs:
  - PREF_BQ
  type: TYPE_IMG
  zh: '![](Image00087.jpg)'
- en: '**8.3.2 Mean Decrease Accuracy**'
  id: totrans-569
  prefs: []
  type: TYPE_NORMAL
  zh: '**8.3.2 平均降低准确率**'
- en: 'Mean decrease accuracy (MDA) is a slow, predictive-importance (out-of-sample,
    OOS) method. First, it fits a classifier; second, it derives its performance OOS
    according to some performance score (accuracy, negative log-loss, etc.); third,
    it permutates each column of the features matrix ( *X* ), one column at a time,
    deriving the performance OOS after each column''s permutation. The importance
    of a feature is a function of the loss in performance caused by its column''s
    permutation. Some relevant considerations include:'
  id: totrans-570
  prefs: []
  type: TYPE_NORMAL
  zh: 平均降低准确率（MDA）是一种缓慢的预测重要性（样本外，OOS）方法。首先，它拟合一个分类器；其次，根据某种性能评分（准确率、负对数损失等），推导出其样本外性能；第三，它逐列打乱特征矩阵（*X*），每次处理一列，在每列打乱后推导出样本外性能。特征的重要性是其列打乱造成的性能损失的函数。一些相关考虑包括：
- en: This method can be applied to any classifier, not only tree-based classifiers.
  id: totrans-571
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 此方法可以应用于任何分类器，而不仅限于树基分类器。
- en: MDA is not limited to accuracy as the sole performance score. For example, in
    the context of meta-labeling applications, we may prefer to score a classifier
    with F1 rather than accuracy (see Chapter 14, Section 14.8 for an explanation).
    That is one reason a better descriptive name would have been “permutation importance.”
    When the scoring function does not correspond to a metric space, MDA results should
    be used as a ranking.
  id: totrans-572
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: MDA并不限于准确率作为唯一的性能评分。例如，在元标签应用的背景下，我们可能更愿意用F1而不是准确率来评分分类器（参见第14章，第14.8节的解释）。这也是一个更好的描述性名称应为“打乱重要性”的原因。当评分函数不对应于度量空间时，MDA结果应作为排名使用。
- en: Like MDI, the procedure is also susceptible to substitution effects in the presence
    of correlated features. Given two identical features, MDA always considers one
    to be redundant to the other. Unfortunately, MDA will make both features appear
    to be outright irrelevant, even if they are critical.
  id: totrans-573
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 与MDI一样，考虑到相关特征，该程序也易受到替代效应的影响。考虑到两个相同的特征，MDA总是将一个视为冗余于另一个。不幸的是，MDA会使这两个特征都显得完全无关，即使它们是至关重要的。
- en: Unlike MDI, it is possible that MDA concludes that all features are unimportant.
    That is because MDA is based on OOS performance.
  id: totrans-574
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 与MDI不同，MDA可能会得出所有特征都不重要的结论。这是因为MDA基于OOS性能。
- en: The CV must be purged and embargoed, for the reasons explained in Chapter 7.
  id: totrans-575
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 交叉验证必须被清理和封存，原因在第7章中已解释。
- en: Snippet 8.3 implements MDA feature importance with sample weights, with purged
    k-fold CV, and with scoring by negative log-loss or accuracy. It measures MDA
    importance as a function of the improvement (from permutating to not permutating
    the feature), relative to the maximum possible score (negative log-loss of 0,
    or accuracy of 1). Note that, in some cases, the improvement may be negative,
    meaning that the feature is actually detrimental to the forecasting power of the
    ML algorithm.
  id: totrans-576
  prefs: []
  type: TYPE_NORMAL
  zh: Snippet 8.3实现了带样本权重的MDA特征重要性，采用清理过的k折交叉验证，并通过负对数损失或准确率进行评分。它将MDA重要性测量为改善的函数（从打乱特征到不打乱特征），相对于最大可能得分（负对数损失为0，或准确率为1）。请注意，在某些情况下，改善可能是负的，这意味着该特征实际上对ML算法的预测能力是有害的。
- en: '**SNIPPET 8.3 MDA FEATURE IMPORTANCE**'
  id: totrans-577
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '**SNIPPET 8.3 MDA特征重要性**'
- en: '![](Image00504.jpg)'
  id: totrans-578
  prefs:
  - PREF_BQ
  type: TYPE_IMG
  zh: '![](Image00504.jpg)'
- en: '**8.4 Feature Importance without Substitution Effects**'
  id: totrans-579
  prefs: []
  type: TYPE_NORMAL
  zh: '**8.4 没有替代效应的特征重要性**'
- en: Substitution effects can lead us to discard important features that happen to
    be redundant. This is not generally a problem in the context of prediction, but
    it could lead us to wrong conclusions when we are trying to understand, improve,
    or simplify a model. For this reason, the following single feature importance
    method can be a good complement to MDI and MDA.
  id: totrans-580
  prefs: []
  type: TYPE_NORMAL
  zh: 替代效应可能导致我们丢弃一些偶然冗余的重要特征。这在预测的背景下通常不是问题，但当我们试图理解、改进或简化模型时，可能会导致错误的结论。因此，以下单一特征重要性方法可以很好地补充MDI和MDA。
- en: '**8.4.1 Single Feature Importance**'
  id: totrans-581
  prefs: []
  type: TYPE_NORMAL
  zh: '**8.4.1 单一特征重要性**'
- en: 'Single feature importance (SFI) is a cross-section predictive-importance (out-of-sample)
    method. It computes the OOS performance score of each feature in isolation. A
    few considerations:'
  id: totrans-582
  prefs: []
  type: TYPE_NORMAL
  zh: 单一特征重要性（SFI）是一种横截面预测重要性（样本外）方法。它计算每个特征单独的样本外性能评分。一些考虑事项：
- en: This method can be applied to any classifier, not only tree-based classifiers.
  id: totrans-583
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 此方法可以应用于任何分类器，而不仅限于树基分类器。
- en: SFI is not limited to accuracy as the sole performance score.
  id: totrans-584
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: SFI并不限于准确率作为唯一的性能评分。
- en: Unlike MDI and MDA, no substitution effects take place, since only one feature
    is taken into consideration at a time.
  id: totrans-585
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 与MDI和MDA不同，此时没有替代效应，因为每次只考虑一个特征。
- en: Like MDA, it can conclude that all features are unimportant, because performance
    is evaluated via OOS CV.
  id: totrans-586
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 与 MDA 类似，它可以得出所有特征都不重要的结论，因为性能是通过 OOS CV 进行评估的。
- en: The main limitation of SFI is that a classifier with two features can perform
    better than the bagging of two single-feature classifiers. For example, (1) feature
    B may be useful only in combination with feature A; or (2) feature B may be useful
    in explaining the splits from feature A, even if feature B alone is inaccurate.
    In other words, joint effects and hierarchical importance are lost in SFI. One
    alternative would be to compute the OOS performance score from subsets of features,
    but that calculation will become intractable as more features are considered.
    Snippet 8.4 demonstrates one possible implementation of the SFI method. A discussion
    of the function `cvScore` can be found in Chapter 7.
  id: totrans-587
  prefs: []
  type: TYPE_NORMAL
  zh: SFI 的主要限制在于，具有两个特征的分类器的性能可能优于两个单特征分类器的集成。例如，(1) 特征 B 可能仅在与特征 A 组合时有用；或者 (2)
    特征 B 可能在解释特征 A 的分裂时有用，即使特征 B 单独的表现并不准确。换句话说，联合效应和层次重要性在 SFI 中丧失了。一个替代方案是计算来自特征子集的
    OOS 性能评分，但随着考虑的特征增多，该计算将变得难以处理。片段 8.4 演示了 SFI 方法的一个可能实现。关于函数 `cvScore` 的讨论可以在第
    7 章中找到。
- en: '**SNIPPET 8.4 IMPLEMENTATION OF SFI**'
  id: totrans-588
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '**片段 8.4 SFI 的实现**'
- en: '![](Image00092.jpg)'
  id: totrans-589
  prefs:
  - PREF_BQ
  type: TYPE_IMG
  zh: '![](Image00092.jpg)'
- en: '**8.4.2 Orthogonal Features**'
  id: totrans-590
  prefs: []
  type: TYPE_NORMAL
  zh: '**8.4.2 正交特征**'
- en: As argued in Section 8.3, substitution effects dilute the importance of features
    measured by MDI, and significantly underestimate the importance of features measured
    by MDA. A partial solution is to orthogonalize the features before applying MDI
    and MDA. An orthogonalization procedure such as principal components analysis
    (PCA) does not prevent all substitution effects, but at least it should alleviate
    the impact of linear substitution effects.
  id: totrans-591
  prefs: []
  type: TYPE_NORMAL
  zh: 正如第 8.3 节所述，替代效应稀释了通过 MDI 测量的特征的重要性，并显著低估了通过 MDA 测量的特征的重要性。一个部分解决方案是在应用 MDI
    和 MDA 之前对特征进行正交化。诸如主成分分析（PCA）这样的正交化程序不能完全消除所有替代效应，但至少应该减轻线性替代效应的影响。
- en: Consider a matrix { *X [*t* , *n*]* } of stationary features, with observations
    *t* = 1, …, *T* and variables *n* = 1, …, *N* . First, we compute the standardized
    features matrix *Z* , such that *Z [*t* , *n*]* = σ ^(− 1) [*n*] ( *X [*t* , *n*]*
    − μ [*n*] ), where μ [*n*] is the mean of { *X [*t* , *n*]* } [*t* = 1, …, *T*]
    and σ [*n*] is the standard deviation of { *X [*t* , *n*]* } [*t* = 1, …, *T*]
    . Second, we compute the eigenvalues Λ and eigenvectors *W* such that *Z* ' *ZW*
    = *W* Λ, where Λ is an *NxN* diagonal matrix with main entries sorted in descending
    order, and *W* is an *NxN* orthonormal matrix. Third, we derive the orthogonal
    features as *P* = *ZW* . We can verify the orthogonality of the features by noting
    that *P* ' *P* = *W* ' *Z* ' *ZW* = *W* ' *W* Λ *W* ' *W* = Λ.
  id: totrans-592
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑一个由平稳特征组成的矩阵 { *X [*t* , *n*]* }，其中观测 *t* = 1, …, *T*，变量 *n* = 1, …, *N*。首先，我们计算标准化特征矩阵
    *Z*，使得 *Z [*t* , *n*]* = σ ^(− 1) [*n*] ( *X [*t* , *n*]* − μ [*n*] )，其中 μ [*n*]
    是 { *X [*t* , *n*]* } [*t* = 1, …, *T*] 的均值，σ [*n*] 是 { *X [*t* , *n*]* } [*t*
    = 1, …, *T*] 的标准差。其次，我们计算特征值 Λ 和特征向量 *W*，使得 *Z* ' *ZW* = *W* Λ，其中 Λ 是一个 *NxN*
    的对角矩阵，主条目按降序排列，*W* 是一个 *NxN* 的正交归一化矩阵。第三，我们推导正交特征为 *P* = *ZW*。我们可以通过注意到 *P* '
    *P* = *W* ' *Z* ' *ZW* = *W* ' *W* Λ *W* ' *W* = Λ 来验证特征的正交性。
- en: 'The diagonalization is done on *Z* rather than *X* , for two reasons: (1) centering
    the data ensures that the first principal component is correctly oriented in the
    main direction of the observations. It is equivalent to adding an intercept in
    a linear regression; (2) re-scaling the data makes PCA focus on explaining correlations
    rather than variances. Without re-scaling, the first principal components would
    be dominated by the columns of *X* with highest variance, and we would not learn
    much about the structure or relationship between the variables.'
  id: totrans-593
  prefs: []
  type: TYPE_NORMAL
  zh: 对 *Z* 而不是 *X* 进行对角化，原因有二：（1）数据居中确保第一主成分在观测的主要方向上正确定位。这相当于在线性回归中添加截距；（2）重新缩放数据使
    PCA 专注于解释相关性而非方差。如果不重新缩放，第一主成分将会被 *X* 中方差最大的列主导，我们将无法深入了解变量之间的结构或关系。
- en: Snippet 8.5 computes the smallest number of orthogonal features that explain
    at least 95% of the variance of *Z* .
  id: totrans-594
  prefs: []
  type: TYPE_NORMAL
  zh: 片段 8.5 计算解释至少 95% 方差的最小正交特征数量 *Z*。
- en: '**SNIPPET 8.5 COMPUTATION OF ORTHOGONAL FEATURES**'
  id: totrans-595
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '**片段 8.5 正交特征的计算**'
- en: '![](Image00541.jpg)'
  id: totrans-596
  prefs:
  - PREF_BQ
  type: TYPE_IMG
  zh: '![](Image00541.jpg)'
- en: 'Besides addressing substitution effects, working with orthogonal features provides
    two additional benefits: (1) orthogonalization can also be used to reduce the
    dimensionality of the features matrix *X* , by dropping features associated with
    small eigenvalues. This usually speeds up the convergence of ML algorithms; (2)
    the analysis is conducted on features designed to explain the structure of the
    data.'
  id: totrans-597
  prefs: []
  type: TYPE_NORMAL
  zh: 除了解决替代效应外，使用正交特征还提供了两个额外的好处：（1）正交化还可以通过丢弃与小特征值相关的特征来减少特征矩阵*X*的维度。这通常加快机器学习算法的收敛速度；（2）分析是在设计用来解释数据结构的特征上进行的。
- en: Let me stress this latter point. An ubiquitous concern throughout the book is
    the risk of overfitting. ML algorithms will always find a pattern, even if that
    pattern is a statistical fluke. You should always be skeptical about the purportedly
    important features identified by any method, including MDI, MDA, and SFI. Now,
    suppose that you derive orthogonal features using PCA. Your PCA analysis has determined
    that some features are more “principal” than others, without any knowledge of
    the labels (unsupervised learning). That is, PCA has ranked features without any
    possible overfitting in a classification sense. When your MDI, MDA, or SFI analysis
    selects as most important (using label information) the same features that PCA
    chose as principal (ignoring label information), this constitutes confirmatory
    evidence that the pattern identified by the ML algorithm is not entirely overfit.
    If the features were entirely random, the PCA ranking would have no correspondance
    with the feature importance ranking. [Figure 8.1](text00001.html#filepos0000456463)
    displays the scatter plot of eigenvalues associated with an eigenvector (x-axis)
    paired with MDI of the feature associated with an engenvector (y-axis). The Pearson
    correlation is 0.8491 (p-value below 1E-150), evidencing that PCA identified informative
    features and ranked them correctly without overfitting.
  id: totrans-598
  prefs: []
  type: TYPE_NORMAL
  zh: 让我强调这一点。本书中一个普遍关注的问题是过拟合的风险。机器学习算法总会找到某种模式，即使该模式是统计上的偶然。你应该对任何方法（包括MDI、MDA和SFI）所识别的重要特征保持怀疑态度。现在，假设你使用PCA得出了正交特征。你的PCA分析已经确定某些特征比其他特征更“主要”，而没有任何标签知识（无监督学习）。也就是说，PCA在分类意义上排名特征时没有任何可能的过拟合。当你的MDI、MDA或SFI分析（使用标签信息）选择与PCA作为主要特征所选相同的特征（忽略标签信息）时，这构成了确认性证据，表明机器学习算法识别的模式并非完全过拟合。如果特征完全随机，PCA排名将与特征重要性排名没有任何对应关系。[图8.1](text00001.html#filepos0000456463)显示了与特征相关的特征值（x轴）与特征MDI（y轴）配对的散点图。皮尔逊相关系数为0.8491（p值低于1E-150），证明PCA识别了有信息的特征并在没有过拟合的情况下正确排名。
- en: '![](Image00100.jpg)'
  id: totrans-599
  prefs: []
  type: TYPE_IMG
  zh: '![](Image00100.jpg)'
- en: '[**Figure 8.1**](text00001.html#filepos0000455929) Scatter plot of eigenvalues
    (x-axis) and MDI levels (y-axis) in log-log scale'
  id: totrans-600
  prefs: []
  type: TYPE_NORMAL
  zh: '[**图8.1**](text00001.html#filepos0000455929) 特征值（x轴）和MDI水平（y轴）在对数-对数尺度上的散点图'
- en: I find it useful to compute the weighted Kendall's tau between the feature importances
    and their associated eigenvalues (or equivalently, their inverse PCA rank). The
    closer this value is to 1, the stronger is the consistency between PCA ranking
    and feature importance ranking. One argument for preferring a weighted Kendall's
    tau over the standard Kendall is that we want to prioritize rank concordance among
    the most importance features. We do not care so much about rank concordance among
    irrelevant (likely noisy) features. The hyperbolic-weighted Kendall's tau for
    the sample in [Figure 8.1](text00001.html#filepos0000456463) is 0.8206.
  id: totrans-601
  prefs: []
  type: TYPE_NORMAL
  zh: 我发现计算特征重要性与其相关特征值（或等价地，其逆PCA排名）之间的加权肯德尔τ非常有用。这个值越接近1，PCA排名与特征重要性排名之间的一致性就越强。偏好加权肯德尔τ而不是标准肯德尔的一个理由是，我们希望优先考虑最重要特征之间的排名一致性。我们并不太关心无关（可能噪声）特征之间的排名一致性。[图8.1](text00001.html#filepos0000456463)中的样本的双曲加权肯德尔τ为0.8206。
- en: Snippet 8.6 shows how to compute this correlation using Scipy. In this example,
    sorting the features in descending importance gives us a PCA rank sequence very
    close to an ascending list. Because the `weightedtau` function gives higher weight
    to higher values, we compute the correlation on the inverse PCA ranking, `pcRank**-1`
    . The resulting weighted Kendall's tau is relatively high, at 0.8133.
  id: totrans-602
  prefs: []
  type: TYPE_NORMAL
  zh: 片段8.6展示了如何使用Scipy计算这种相关性。在这个例子中，按重要性降序排列特征给我们一个非常接近升序列表的PCA排名序列。由于`weightedtau`函数对较高值赋予更高权重，我们在逆PCA排名`pcRank**-1`上计算相关性。得到的加权肯德尔τ相对较高，为0.8133。
- en: '**SNIPPET 8.6 COMPUTATION OF WEIGHTED KENDALL''S TAU BETWEEN FEATURE IMPORTANCE
    AND INVERSE PCA RANKING**'
  id: totrans-603
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '**SNIPPET 8.6 加权肯德尔tau计算特征重要性与逆主成分分析排名之间的关系**'
- en: '![](Image00586.jpg)'
  id: totrans-604
  prefs:
  - PREF_BQ
  type: TYPE_IMG
  zh: '![](Image00586.jpg)'
- en: '**8.5 Parallelized vs. Stacked Feature Importance**'
  id: totrans-605
  prefs: []
  type: TYPE_NORMAL
  zh: '**8.5 并行与堆叠特征重要性**'
- en: There are at least two research approaches to feature importance. First, for
    each security *i* in an investment universe *i* = 1, …, *I* , we form a dataset
    ( *X [*i*]* , *y [*i*]* ), and derive the feature importance in parallel. For
    example, let us denote λ [*i* , *j* , *k*] the importance of feature *j* on instrument
    *i* according to criterion *k.* Then we can aggregate all results across the entire
    universe to derive a combined Λ [*j* , *k*] importance of feature *j* according
    to criterion *k.* Features that are important across a wide variety of instruments
    are more likely to be associated with an underlying phenomenon, particularly when
    these feature importances exhibit high rank correlation across the criteria. It
    may be worth studying in-depth the theoretical mechanism that makes these features
    predictive. The main advantage of this approach is that it is computationally
    fast, as it can be parallelized. A disadvantage is that, due to substitution effects,
    important features may swap their ranks across instruments, increasing the variance
    of the estimated λ [*i* , *j* , *k*] . This disadvantage becomes relatively minor
    if we average λ [*i* , *j* , *k*] across instruments for a sufficiently large
    investment universe.
  id: totrans-606
  prefs: []
  type: TYPE_NORMAL
  zh: 至少有两种研究特征重要性的方法。首先，对于投资宇宙中的每个证券 *i*（*i* = 1, …, *I*），我们形成一个数据集（*X [*i*]*，*y
    [*i*]*），并并行推导特征重要性。例如，设λ [*i* , *j* , *k*]为特征 *j* 在标准 *k* 下对工具 *i* 的重要性。然后，我们可以汇总整个宇宙中的所有结果，以推导特征
    *j* 在标准 *k* 下的综合重要性Λ [*j* , *k*]。在各种工具中重要的特征更可能与潜在现象相关，特别是当这些特征重要性在标准之间表现出高排名相关性时。深入研究使这些特征具有预测能力的理论机制可能是值得的。这种方法的主要优点是计算速度快，因为可以并行处理。缺点是，由于替代效应，重要特征可能会在工具之间交换其排名，从而增加估计的λ
    [*i* , *j* , *k*] 的方差。如果我们在足够大的投资宇宙中平均λ [*i* , *j* , *k*]，则这一缺点相对较小。
- en: 'A second alternative is what I call “features stacking.” It consists in stacking
    all datasets ![](Image00609.jpg) into a single combined dataset ( *X* , *y* ),
    where ![](Image00109.jpg) is a transformed instance of *X [*i*] * (e.g., standardized
    on a rolling trailing window). The purpose of this transformation is to ensure
    some distributional homogeneity, ![](Image00335.jpg) . Under this approach, the
    classifier must learn what features are more important across all instruments
    simultaneously, as if the entire investment universe were in fact a single instrument.
    Features stacking presents some advantages: (1) The classifier will be fit on
    a much larger dataset than the one used with the parallelized (first) approach;
    (2) the importance is derived directly, and no weighting scheme is required for
    combining the results; (3) conclusions are more general and less biased by outliers
    or overfitting; and (4) because importance scores are not averaged across instruments,
    substitution effects do not cause the dampening of those scores.'
  id: totrans-607
  prefs: []
  type: TYPE_NORMAL
  zh: 第二种选择是我称之为“特征堆叠”。它包括将所有数据集 ![](Image00609.jpg) 堆叠成一个单一的组合数据集（*X*，*y*），其中 ![](Image00109.jpg)
    是对 *X [*i*] * 的变换实例（例如，在滚动窗口上标准化）。这种变换的目的是确保一些分布同质性，![](Image00335.jpg)。在这种方法下，分类器必须同时学习哪些特征在所有工具中更重要，就好像整个投资宇宙实际上是一个单一工具。特征堆叠有一些优点：（1）分类器将在比并行化（第一种）方法使用的更大数据集上进行训练；（2）重要性是直接推导的，不需要加权方案来组合结果；（3）结论更具普遍性，且不易受异常值或过拟合的影响；（4）由于重要性分数不在工具间平均，替代效应不会抑制这些分数。
- en: I usually prefer features stacking, not only for features importance but whenever
    a classifier can be fit on a set of instruments, including for the purpose of
    model prediction. That reduces the likelihood of overfitting an estimator to a
    particular instrument or small dataset. The main disadvantage of stacking is that
    it may consume a lot of memory and resources, however that is where a sound knowledge
    of HPC techniques will come in handy (Chapters 20–22).
  id: totrans-608
  prefs: []
  type: TYPE_NORMAL
  zh: 我通常更倾向于特征堆叠，不仅用于特征重要性，而且在分类器可以在一组工具上进行拟合时，包括用于模型预测的目的。这减少了对特定工具或小数据集过拟合估计量的可能性。堆叠的主要缺点是可能消耗大量内存和资源，但这正是对HPC技术有良好知识的地方（第20–22章）。
- en: '**8.6 Experiments with Synthetic Data**'
  id: totrans-609
  prefs: []
  type: TYPE_NORMAL
  zh: '**8.6 与合成数据的实验**'
- en: 'In this section, we are going to test how these feature importance methods
    respond to synthetic data. We are going to generate a dataset ( *X* , *y* ) composed
    on three kinds of features:'
  id: totrans-610
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将测试这些特征重要性方法如何对合成数据做出响应。我们将生成一个由三种特征组成的数据集 (*X*, *y*)：
- en: 'Informative: These are features that are used to determine the label.'
  id: totrans-611
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 信息性：这些是用于确定标签的特征。
- en: 'Redundant: These are random linear combinations of the informative features.
    They will cause substitution effects.'
  id: totrans-612
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 冗余：这些是信息性特征的随机线性组合。它们会引起替代效应。
- en: 'Noise: These are features that have no bearing on determining the observation''s
    label.'
  id: totrans-613
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 噪声：这些是对确定观测标签没有影响的特征。
- en: 'Snippet 8.7 shows how we can generate a synthetic dataset of 40 features where
    10 are informative, 10 are redundant, and 20 are noise, on 10,000 observations.
    For details on how sklearn generates synthetic datasets, visit: [http://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_classification.html](http://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_classification.html)
    .'
  id: totrans-614
  prefs: []
  type: TYPE_NORMAL
  zh: 片段 8.7 显示了如何生成一个包含 40 个特征的合成数据集，其中 10 个是信息性特征，10 个是冗余特征，20 个是噪声特征，基于 10,000
    个观测值。有关 sklearn 如何生成合成数据集的详细信息，请访问：[http://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_classification.html](http://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_classification.html)。
- en: '**SNIPPET 8.7 CREATING A SYNTHETIC DATASET**'
  id: totrans-615
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '**片段 8.7 创建合成数据集**'
- en: '![](Image00338.jpg)'
  id: totrans-616
  prefs:
  - PREF_BQ
  type: TYPE_IMG
  zh: '![](Image00338.jpg)'
- en: Given that we know for certain what feature belongs to each class, we can evaluate
    whether these three feature importance methods perform as designed. Now we need
    a function that can carry out each analysis on the same dataset. Snippet 8.8 accomplishes
    that, using bagged decision trees as default classifier (Chapter 6).
  id: totrans-617
  prefs: []
  type: TYPE_NORMAL
  zh: 鉴于我们确切知道每个特征属于哪个类别，我们可以评估这三种特征重要性方法是否按设计执行。现在我们需要一个可以对同一数据集进行各项分析的函数。片段 8.8
    实现了这一点，使用袋装决策树作为默认分类器（第 6 章）。
- en: '**SNIPPET 8.8 CALLING FEATURE IMPORTANCE FOR ANY METHOD**'
  id: totrans-618
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '**片段 8.8 调用任何方法的特征重要性**'
- en: '![](Image00342.jpg)'
  id: totrans-619
  prefs:
  - PREF_BQ
  type: TYPE_IMG
  zh: '![](Image00342.jpg)'
- en: Finally, we need a main function to call all components, from data generation
    to feature importance analysis to collection and processing of output. These tasks
    are performed by Snippet 8.9.
  id: totrans-620
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们需要一个主函数来调用所有组件，从数据生成到特征重要性分析，再到输出的收集和处理。这些任务由片段 8.9 执行。
- en: '**SNIPPET 8.9 CALLING ALL COMPONENTS**'
  id: totrans-621
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '**片段 8.9 调用所有组件**'
- en: '![](Image00344.jpg)'
  id: totrans-622
  prefs:
  - PREF_BQ
  type: TYPE_IMG
  zh: '![](Image00344.jpg)'
- en: For the aesthetically inclined, Snippet 8.10 provides a nice layout for plotting
    feature importances.
  id: totrans-623
  prefs: []
  type: TYPE_NORMAL
  zh: 对于审美有要求的用户，片段 8.10 提供了一个不错的布局来绘制特征重要性图。
- en: '**SNIPPET 8.10 FEATURE IMPORTANCE PLOTTING FUNCTION**'
  id: totrans-624
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '**片段 8.10 特征重要性绘图函数**'
- en: '![](Image00349.jpg)'
  id: totrans-625
  prefs:
  - PREF_BQ
  type: TYPE_IMG
  zh: '![](Image00349.jpg)'
- en: '[Figure 8.2](text00001.html#filepos0000467068) shows results for MDI. For each
    feature, the horizontal bar indicates the mean MDI value across all the decision
    trees, and the horizontal line is the standard deviation of that mean. Since MDI
    importances add up to 1, if all features were equally important, each importance
    would have a value of 1/40\. The vertical dotted line marks that 1/40 threshold,
    separating features whose importance exceeds what would be expected from undistinguishable
    features. As you can see, MDI does a very good job in terms of placing all informative
    and redundant features above the red dotted line, with the exception of R_5, which
    did not make the cut by a small margin. Substitution effects cause some informative
    or redundant features to rank better than others, which was expected.'
  id: totrans-626
  prefs: []
  type: TYPE_NORMAL
  zh: '[图 8.2](text00001.html#filepos0000467068) 显示了 MDI 的结果。对于每个特征，水平条表示所有决策树的平均
    MDI 值，水平线是该平均值的标准差。由于 MDI 重要性加起来为 1，如果所有特征同样重要，则每个重要性值将为 1/40。垂直虚线标记了 1/40 的阈值，区分了特征的重要性是否超过了不可区分特征所期望的水平。如你所见，MDI
    在将所有信息性和冗余特征置于红色虚线之上方面表现非常好，唯一例外的是 R_5，略微未能入选。替代效应使得一些信息性或冗余特征的排名优于其他特征，这一点是预期之中的。'
- en: '![](Image00352.jpg)'
  id: totrans-627
  prefs: []
  type: TYPE_IMG
  zh: '![](Image00352.jpg)'
- en: '[**Figure 8.2**](text00001.html#filepos0000466053) MDI feature importance computed
    on a synthetic dataset'
  id: totrans-628
  prefs: []
  type: TYPE_NORMAL
  zh: '[**图 8.2**](text00001.html#filepos0000466053) 在合成数据集上计算的 MDI 特征重要性'
- en: '[Figure 8.3](text00001.html#filepos0000468012) shows that MDA also did a good
    job. Results are consistent with those from MDI''s in the sense that all the informed
    and redundant features rank better than the noise feature, with the exception
    of R_6, likely due to a substitution effect. One not so positive aspect of MDA
    is that the standard deviation of the means are somewhat higher, although that
    could be addressed by increasing the number of partitions in the purged k-fold
    CV, from, say, 10 to 100 (at the cost of 10 × the computation time without parallelization).'
  id: totrans-629
  prefs: []
  type: TYPE_NORMAL
  zh: '[图 8.3](text00001.html#filepos0000468012) 显示 MDA 也表现良好。结果与 MDI 的结果一致，即所有信息量大的和冗余的特征排名都优于噪声特征，R_6
    除外，可能是由于替代效应。MDA 的一个不太积极的方面是均值的标准差稍高，虽然通过将清理后的 k 折交叉验证中的分区数量从 10 增加到 100（在没有并行化的情况下计算时间增加
    10 倍）可以解决这一问题。'
- en: '![](Image00355.jpg)'
  id: totrans-630
  prefs: []
  type: TYPE_IMG
  zh: '![](Image00355.jpg)'
- en: '[**Figure 8.3**](text00001.html#filepos0000467240) MDA feature importance computed
    on a synthetic dataset'
  id: totrans-631
  prefs: []
  type: TYPE_NORMAL
  zh: '[**图 8.3**](text00001.html#filepos0000467240) 在合成数据集上计算的 MDA 特征重要性'
- en: '[Figure 8.4](text00001.html#filepos0000468942) shows that SFI also does a decent
    job; however, a few important features rank worse than noise (I_6, I_2, I_9, I_1,
    I_3, R_5), likely due to joint effects.'
  id: totrans-632
  prefs: []
  type: TYPE_NORMAL
  zh: '[图 8.4](text00001.html#filepos0000468942) 显示 SFI 的表现也不错；然而，几个重要特征的排名低于噪声 (I_6,
    I_2, I_9, I_1, I_3, R_5)，可能是由于联合效应。'
- en: The labels are a function of a combination of features, and trying to forecast
    them independently misses the joint effects. Still, SFI is useful as a complement
    to MDI and MDA, precisely because both types of analyses are affected by different
    kinds of problems.
  id: totrans-633
  prefs: []
  type: TYPE_NORMAL
  zh: 标签是特征组合的函数，独立预测它们会错过联合效应。不过，SFI 作为 MDI 和 MDA 的补充是有用的，正是因为这两种分析受不同问题的影响。
- en: '![](Image00360.jpg)'
  id: totrans-634
  prefs: []
  type: TYPE_IMG
  zh: '![](Image00360.jpg)'
- en: '[**Figure 8.4**](text00001.html#filepos0000468184) SFI feature importance computed
    on a synthetic dataset'
  id: totrans-635
  prefs: []
  type: TYPE_NORMAL
  zh: '[**图 8.4**](text00001.html#filepos0000468184) 在合成数据集上计算的 SFI 特征重要性'
- en: '**Exercises**'
  id: totrans-636
  prefs: []
  type: TYPE_NORMAL
  zh: '**练习**'
- en: 'Using the code presented in Section 8.6:'
  id: totrans-637
  prefs:
  - PREF_OL
  - PREF_BQ
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 使用第 8.6 节中提供的代码：
- en: Generate a dataset (*X* , *y* ).
  id: totrans-638
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 生成数据集 (*X* , *y* )。
- en: Apply a PCA transformation on *X* , which we denote ![](Image00363.jpg) .
  id: totrans-639
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对 *X* 应用 PCA 转换，记作 ![](Image00363.jpg)。
- en: Compute MDI, MDA, and SFI feature importance on ![](Image00366.jpg) , where
    the base estimator is RF.
  id: totrans-640
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在 ![](Image00366.jpg) 上计算 MDI、MDA 和 SFI 特征重要性，其中基准估计器为 RF。
- en: Do the three methods agree on what features are important? Why?
  id: totrans-641
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 这三种方法在重要特征上是否一致？为什么？
- en: From exercise 1, generate a new dataset ![](Image00370.jpg) , where ![](Image00373.jpg)
    is a feature union of *X* and ![](Image00376.jpg) .
  id: totrans-642
  prefs:
  - PREF_OL
  - PREF_BQ
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 从练习 1 中生成新数据集 ![](Image00370.jpg)，其中 ![](Image00373.jpg) 是 *X* 和 ![](Image00376.jpg)
    的特征联合。
- en: Compute MDI, MDA, and SFI feature importance on ![](Image00379.jpg) , where
    the base estimator is RF.
  id: totrans-643
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在 ![](Image00379.jpg) 上计算 MDI、MDA 和 SFI 特征重要性，其中基准估计器为 RF。
- en: Do the three methods agree on the important features? Why?
  id: totrans-644
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 这三种方法在重要特征上是否一致？为什么？
- en: 'Take the results from exercise 2:'
  id: totrans-645
  prefs:
  - PREF_OL
  - PREF_BQ
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 从练习 2 中获取结果：
- en: Drop the most important features according to each method, resulting in a features
    matrix ![](Image00381.jpg) .
  id: totrans-646
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 根据每种方法删除最重要的特征，得到特征矩阵 ![](Image00381.jpg)。
- en: Compute MDI, MDA, and SFI feature importance on ![](Image00385.jpg) , where
    the base estimator is RF.
  id: totrans-647
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在 ![](Image00385.jpg) 上计算 MDI、MDA 和 SFI 特征重要性，其中基准估计器为 RF。
- en: Do you appreciate significant changes in the rankings of important features,
    relative to the results from exercise 2?
  id: totrans-648
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 相比于练习 2 的结果，你是否注意到重要特征排名的显著变化？
- en: 'Using the code presented in Section 8.6:'
  id: totrans-649
  prefs:
  - PREF_OL
  - PREF_BQ
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 使用第 8.6 节中提供的代码：
- en: Generate a dataset (*X* , *y* ) of 1E6 observations, where 5 features are informative,
    5 are redundant and 10 are noise.
  id: totrans-650
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 生成一个包含 1E6 个观察值的数据集 (*X* , *y* )，其中 5 个特征是信息量大的，5 个是冗余的，10 个是噪声。
- en: Split (*X* , *y* ) into 10 datasets {(*X [*i*]* , *y [*i*]* )} [*i* = 1, …,
    10] , each of 1E5 observations.
  id: totrans-651
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将 (*X* , *y* ) 拆分为 10 个数据集 {(*X [*i*]* , *y [*i*]* )} [*i* = 1, …, 10]，每个包含
    1E5 个观察值。
- en: Compute the parallelized feature importance (Section 8.5), on each of the 10
    datasets, {(*X [*i*]* , *y [*i*]* )} [*i* = 1, …, 10] .
  id: totrans-652
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在每个 10 个数据集 {(*X [*i*]* , *y [*i*]* )} [*i* = 1, …, 10] 上计算并行化特征重要性（第 8.5 节）。
- en: Compute the stacked feature importance on the combined dataset (*X* , *y* ).
  id: totrans-653
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在组合数据集 (*X* , *y* ) 上计算堆叠特征重要性。
- en: What causes the discrepancy between the two? Which one is more reliable?
  id: totrans-654
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 两者之间的差异是什么？哪一个更可靠？
- en: Repeat all MDI calculations from exercises 1–4, but this time allow for masking
    effects. That means, do not set `max_features=int(1)` in Snippet 8.2\. How do
    results differ as a consequence of this change? Why?
  id: totrans-655
  prefs:
  - PREF_OL
  - PREF_BQ
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 重复第1–4题中的所有MDI计算，但这次允许掩蔽效应。这意味着，不要在代码片段8.2中设置`max_features=int(1)`。结果因这一变化而有何不同？为什么？
- en: '**References**'
  id: totrans-656
  prefs: []
  type: TYPE_NORMAL
  zh: '**参考文献**'
- en: 'American Statistical Association (2016): “Ethical guidelines for statistical
    practice.” Committee on Professional Ethics of the American Statistical Association
    (April). Available at [http://www.amstat.org/asa/files/pdfs/EthicalGuidelines.pdf](http://www.amstat.org/asa/files/pdfs/EthicalGuidelines.pdf)
    .'
  id: totrans-657
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '美国统计协会 (2016): “统计实践的伦理指南。”美国统计协会职业伦理委员会（四月）。可在[http://www.amstat.org/asa/files/pdfs/EthicalGuidelines.pdf](http://www.amstat.org/asa/files/pdfs/EthicalGuidelines.pdf)获取。'
- en: 'Belsley, D., E. Kuh, and R. Welsch (1980): *Regression Diagnostics: Identifying
    Influential Data and Sources of Collinearity* , 1st ed. John Wiley & Sons.'
  id: totrans-658
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 'Belsley, D., E. Kuh, 和 R. Welsch (1980): *回归诊断：识别影响数据和共线性来源*，第1版。约翰·威利父子公司。'
- en: 'Goldberger, A. (1991): *A Course in Econometrics* . Harvard University Press,
    1st edition.'
  id: totrans-659
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 'Goldberger, A. (1991): *计量经济学课程*。哈佛大学出版社，第1版。'
- en: 'Hill, R. and L. Adkins (2001): “Collinearity.” In Baltagi, Badi H. *A Companion
    to Theoretical Econometrics* , 1st ed. Blackwell, pp. 256–278.'
  id: totrans-660
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 'Hill, R. 和 L. Adkins (2001): “共线性。”见 Baltagi, Badi H. *理论计量经济学的伴侣*，第1版。Blackwell，页码256–278。'
- en: 'Louppe, G., L. Wehenkel, A. Sutera, and P. Geurts (2013): “Understanding variable
    importances in forests of randomized trees.” Proceedings of the 26th International
    Conference on Neural Information Processing Systems, pp. 431–439.'
  id: totrans-661
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 'Louppe, G., L. Wehenkel, A. Sutera, 和 P. Geurts (2013): “理解随机树森林中的变量重要性。”第26届国际神经信息处理系统会议论文集，页码431–439。'
- en: 'Strobl, C., A. Boulesteix, A. Zeileis, and T. Hothorn (2007): “Bias in random
    forest variable importance measures: Illustrations, sources and a solution.” *BMC
    Bioinformatics* , Vol. 8, No. 25, pp. 1–11.'
  id: totrans-662
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 'Strobl, C., A. Boulesteix, A. Zeileis, 和 T. Hothorn (2007): “随机森林变量重要性度量中的偏差：插图、来源和解决方案。”
    *BMC生物信息学*，第8卷，第25期，页码1–11。'
- en: 'White, A. and W. Liu (1994): “Technical note: Bias in information-based measures
    in decision tree induction.” *Machine Learning* , Vol. 15, No. 3, pp. 321–329.'
  id: totrans-663
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 'White, A. 和 W. Liu (1994): “技术说明：决策树归纳中基于信息的度量的偏差。” *机器学习*，第15卷，第3期，页码321–329。'
- en: '**Note**'
  id: totrans-664
  prefs: []
  type: TYPE_NORMAL
  zh: '**注意**'
- en: ^([1](text00001.html#filepos0000443800))     [http://blog.datadive.net/selecting-good-features-part-iii-random-forests/](http://blog.datadive.net/selecting-good-features-part-iii-random-forests/)
    .
  id: totrans-665
  prefs: []
  type: TYPE_NORMAL
  zh: ^([1](text00001.html#filepos0000443800))     [http://blog.datadive.net/selecting-good-features-part-iii-random-forests/](http://blog.datadive.net/selecting-good-features-part-iii-random-forests/)
    .
- en: '**CHAPTER 9**'
  id: totrans-666
  prefs: []
  type: TYPE_NORMAL
  zh: '**第9章**'
- en: '**Hyper-Parameter Tuning with Cross-Validation**'
  id: totrans-667
  prefs: []
  type: TYPE_NORMAL
  zh: '**交叉验证下的超参数调优**'
- en: '**9.1 Motivation**'
  id: totrans-668
  prefs: []
  type: TYPE_NORMAL
  zh: '**9.1 动机**'
- en: Hyper-parameter tuning is an essential step in fitting an ML algorithm. When
    this is not done properly, the algorithm is likely to overfit, and live performance
    will disappoint. The ML literature places special attention on cross-validating
    any tuned hyper-parameter. As we have seen in Chapter 7, cross-validation (CV)
    in finance is an especially difficult problem, where solutions from other fields
    are likely to fail. In this chapter we will discuss how to tune hyper-parameters
    using the purged k-fold CV method. The references section lists studies that propose
    alternative methods that may be useful in specific problems.
  id: totrans-669
  prefs: []
  type: TYPE_NORMAL
  zh: 超参数调优是拟合机器学习算法的重要步骤。如果这一步没有做好，算法可能会过拟合，实际表现会令人失望。机器学习文献特别关注对任何调优超参数进行交叉验证。正如我们在第7章中看到的，金融领域的交叉验证（CV）是一个特别困难的问题，其他领域的解决方案可能会失败。在本章中，我们将讨论如何使用清除的k折CV方法进行超参数调优。参考文献部分列出了提出可能在特定问题中有用的替代方法的研究。
- en: '**9.2 Grid Search Cross-Validation**'
  id: totrans-670
  prefs: []
  type: TYPE_NORMAL
  zh: '**9.2 网格搜索交叉验证**'
- en: Grid search cross-validation conducts an exhaustive search for the combination
    of parameters that maximizes the CV performance, according to some user-defined
    score function. When we do not know much about the underlying structure of the
    data, this is a reasonable first approach. Scikit-learn has implemented this logic
    in the function `GridSearchCV` , which accepts a CV generator as an argument.
    For the reasons explained in Chapter 7, we need to pass our `PurgedKFold` class
    (Snippet 7.3) in order to prevent that `GridSearchCV` overfits the ML estimator
    to leaked information.
  id: totrans-671
  prefs: []
  type: TYPE_NORMAL
  zh: 网格搜索交叉验证根据用户定义的得分函数对最大化CV性能的参数组合进行彻底搜索。当我们对数据的底层结构了解不多时，这是一种合理的初步方法。Scikit-learn在`GridSearchCV`函数中实现了这一逻辑，该函数接受CV生成器作为参数。由于第七章中解释的原因，我们需要传递`PurgedKFold`类（代码片段7.3），以防止`GridSearchCV`对泄漏信息过拟合ML估计器。
- en: '**SNIPPET 9.1 GRID SEARCH WITH PURGED K-FOLD CROSS-VALIDATION**'
  id: totrans-672
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '**代码片段9.1 使用清除的K折交叉验证的网格搜索**'
- en: '![](Image00387.jpg)'
  id: totrans-673
  prefs:
  - PREF_BQ
  type: TYPE_IMG
  zh: '![](Image00387.jpg)'
- en: Snippet 9.1 lists function `clfHyperFit` , which implements a purged `GridSearchCV`
    . The argument `fit_params` can be used to pass `sample_weight` , and `param_grid`
    contains the values that will be combined into a grid. In addition, this function
    allows for the bagging of the tuned estimator. Bagging an estimator is generally
    a good idea for the reasons explained in Chapter 6, and the above function incorporates
    logic to that purpose.
  id: totrans-674
  prefs: []
  type: TYPE_NORMAL
  zh: 代码片段9.1列出了函数`clfHyperFit`，该函数实现了清除的`GridSearchCV`。参数`fit_params`可用于传递`sample_weight`，而`param_grid`包含将组合成网格的值。此外，该函数允许对调优的估计器进行集成。对估计器进行集成通常是个好主意，原因在第六章中已解释，上述函数包含了为此目的的逻辑。
- en: I advise you to use `scoring=‘f1’` in the context of meta-labeling applications,
    for the following reason. Suppose a sample with a very large number of negative
    (i.e., label ‘0’) cases. A classifier that predicts all cases to be negative will
    achieve high `‘accuracy’` or `‘neg_log_loss’` , even though it has not learned
    from the features how to discriminate between cases. In fact, such a model achieves
    zero recall and undefined precision (see Chapter 3, Section 3.7). The `‘f1’` score
    corrects for that performance inflation by scoring the classifier in terms of
    precision and recall (see Chapter 14, Section 14.8).
  id: totrans-675
  prefs: []
  type: TYPE_NORMAL
  zh: 我建议你在元标签应用的上下文中使用`scoring=‘f1’`，原因如下。假设样本中有大量负面（即，标签为‘0’）案例。一个将所有案例预测为负面的分类器将获得高`‘accuracy’`或`‘neg_log_loss’`，尽管它并未从特征中学习如何区分案例。实际上，这种模型的召回率为零，精确度未定义（见第3章第3.7节）。`‘f1’`分数通过从精确度和召回率的角度评分分类器来纠正这种性能膨胀（见第14章第14.8节）。
- en: For other (non-meta-labeling) applications, it is fine to use `‘accuracy’` or
    `‘neg_log_loss’` , because we are equally interested in predicting all cases.
    Note that a relabeling of cases has no impact on `‘accuracy’` or `‘neg_log_loss’`
    , however it will have an impact on `‘f1’` .
  id: totrans-676
  prefs: []
  type: TYPE_NORMAL
  zh: 对于其他（非元标签）应用，使用`‘accuracy’`或`‘neg_log_loss’`是可以的，因为我们对所有案例的预测都同样感兴趣。请注意，对案例的重新标记对`‘accuracy’`或`‘neg_log_loss’`没有影响，但会对`‘f1’`产生影响。
- en: 'This example introduces nicely one limitation of sklearn''s `Pipelines` : Their
    fit method does not expect a `sample_weight` argument. Instead, it expects a `fit_params`
    keyworded argument. That is a bug that has been reported in GitHub; however, it
    may take some time to fix it, as it involves rewriting and testing much functionality.
    Until then, feel free to use the workaround in Snippet 9.2\. It creates a new
    class, called `MyPipeline` , which inherits all methods from sklearn''s `Pipeline`
    . It overwrites the inherited `fit` method with a new one that handles the argument
    `sample_weight` , after which it redirects to the parent class.'
  id: totrans-677
  prefs: []
  type: TYPE_NORMAL
  zh: 此示例很好地引入了sklearn的`Pipelines`的一个限制：它们的拟合方法不期望`sample_weight`参数，而是期望一个带关键字的`fit_params`参数。这是一个已在GitHub上报告的错误；不过，修复可能需要一些时间，因为这涉及到重写和测试大量功能。在那之前，可以自由使用代码片段9.2中的解决方法。它创建了一个新类，称为`MyPipeline`，该类继承了sklearn的`Pipeline`中的所有方法。它用一个新的方法覆盖继承的`fit`方法，该方法处理参数`sample_weight`，然后重定向到父类。
- en: '**SNIPPET 9.2 AN ENHANCED PIPELINE CLASS**'
  id: totrans-678
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '**代码片段9.2 增强型管道类**'
- en: '![](Image00389.jpg)'
  id: totrans-679
  prefs:
  - PREF_BQ
  type: TYPE_IMG
  zh: '![](Image00389.jpg)'
- en: 'If you are not familiar with this technique for expanding classes, you may
    want to read this introductory Stackoverflow post: [http://stackoverflow.com/questions/
    576169/understanding-python-super-with-init-methods](http://stackoverflow.com/questions/576169/understanding-python-super-with-init-methods)
    .'
  id: totrans-680
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你不熟悉扩展类的这种技术，建议阅读这个入门的 Stackoverflow 文章：[http://stackoverflow.com/questions/
    576169/understanding-python-super-with-init-methods](http://stackoverflow.com/questions/576169/understanding-python-super-with-init-methods)。
- en: '**9.3 Randomized Search Cross-Validation**'
  id: totrans-681
  prefs: []
  type: TYPE_NORMAL
  zh: '**9.3 随机搜索交叉验证**'
- en: 'For ML algorithms with a large number of parameters, a grid search cross-validation
    (CV) becomes computationally intractable. In this case, an alternative with good
    statistical properties is to sample each parameter from a distribution (Begstra
    et al. [2011, 2012]). This has two benefits: First, we can control for the number
    of combinations we will search for, regardless of the dimensionality of the problem
    (the equivalent to a computational budget). Second, having parameters that are
    relatively irrelevant performance-wise will not substantially increase our search
    time, as would be the case with grid search CV.'
  id: totrans-682
  prefs: []
  type: TYPE_NORMAL
  zh: 对于参数数量较多的机器学习算法，网格搜索交叉验证 (CV) 变得计算上不可行。在这种情况下，一个具有良好统计特性的替代方案是从一个分布中抽样每个参数（Begstra
    等人 [2011, 2012]）。这有两个好处：首先，我们可以控制搜索的组合数量，而不考虑问题的维度（相当于计算预算）。其次，性能上相对无关的参数不会显著增加我们的搜索时间，这与网格搜索
    CV 的情况相反。
- en: Rather than writing a new function to work with `RandomizedSearchCV` , let us
    expand Snippet 9.1 to incorporate an option to this purpose. A possible implementation
    is Snippet 9.3.
  id: totrans-683
  prefs: []
  type: TYPE_NORMAL
  zh: 与其编写一个新的函数来处理 `RandomizedSearchCV`，不如扩展代码片段 9.1，加入一个选项。一个可能的实现是代码片段 9.3。
- en: '**SNIPPET 9.3 RANDOMIZED SEARCH WITH PURGED K-FOLD CV**'
  id: totrans-684
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '**代码片段 9.3 使用清除 K 折交叉验证的随机搜索**'
- en: '![](Image00391.jpg)'
  id: totrans-685
  prefs:
  - PREF_BQ
  type: TYPE_IMG
  zh: '![](Image00391.jpg)'
- en: '**9.3.1 Log-Uniform Distribution**'
  id: totrans-686
  prefs: []
  type: TYPE_NORMAL
  zh: '**9.3.1 对数均匀分布**'
- en: It is common for some ML algorithms to accept non-negative hyper-parameters
    only. That is the case of some very popular parameters, such as `C` in the SVC
    classifier and `gamma` in the RBF kernel. ^([1](text00002.html#filepos0000501318))
    We could draw random numbers from a uniform distribution bounded between 0 and
    some large value, say 100\. That would mean that 99% of the values would be expected
    to be greater than 1\. That is not necessarily the most effective way of exploring
    the feasibility region of parameters whose functions do not respond linearly.
    For example, an SVC can be as responsive to an increase in `C` from 0.01 to 1
    as to an increase in `C` from 1 to 100. ^([2](text00002.html#filepos0000501610))
    So sampling `C` from a *U* [0, 100] (uniform) distribution will be inefficient.
    In those instances, it seems more effective to draw values from a distribution
    where the logarithm of those draws will be distributed uniformly. I call that
    a “log-uniform distribution,” and since I could not find it in the literature,
    I must define it properly.
  id: totrans-687
  prefs: []
  type: TYPE_NORMAL
  zh: 一些机器学习算法通常仅接受非负超参数。这就是一些非常流行的参数的情况，例如 SVC 分类器中的 `C` 和 RBF 核中的 `gamma`。^([1](text00002.html#filepos0000501318))
    我们可以从一个界定在 0 和某个大值（例如 100）的均匀分布中抽取随机数。这意味着 99% 的值预计会大于 1。这并不一定是探索非线性响应参数的可行区域的最有效方法。例如，SVC
    对于将 `C` 从 0.01 增加到 1 的反应与将 `C` 从 1 增加到 100 的反应是一样的。^([2](text00002.html#filepos0000501610))
    因此，从 *U* [0, 100]（均匀）分布中抽样 `C` 将是低效的。在这种情况下，似乎从对数分布均匀抽取值更为有效。我称之为“对数均匀分布”，由于在文献中找不到这个定义，我必须对其进行恰当的定义。
- en: 'A random variable *x* follows a log-uniform distribution between *a* > 0 and
    *b* > *a* if and only if log [ *x* ] ∼ *U* [log [ *a* ], log [ *b* ]]. This distribution
    has a CDF:'
  id: totrans-688
  prefs: []
  type: TYPE_NORMAL
  zh: 随机变量 *x* 在 *a* > 0 和 *b* > *a* 之间遵循对数均匀分布，当且仅当 log [ *x* ] ∼ *U* [log [ *a*
    ], log [ *b* ]]。该分布的累积分布函数 (CDF) 为：
- en: '![](Image00394.jpg)'
  id: totrans-689
  prefs: []
  type: TYPE_IMG
  zh: '![](Image00394.jpg)'
- en: 'From this, we derive a PDF:'
  id: totrans-690
  prefs: []
  type: TYPE_NORMAL
  zh: 从中，我们得出一个 PDF：
- en: '![](Image00569.jpg)'
  id: totrans-691
  prefs: []
  type: TYPE_IMG
  zh: '![](Image00569.jpg)'
- en: '![](Image00401.jpg)'
  id: totrans-692
  prefs: []
  type: TYPE_IMG
  zh: '![](Image00401.jpg)'
- en: '[**Figure 9.1**](text00001.html#filepos0000487213) Result from testing the
    `logUniform_gen` class'
  id: totrans-693
  prefs: []
  type: TYPE_NORMAL
  zh: '[**图 9.1**](text00001.html#filepos0000487213) 测试 `logUniform_gen` 类的结果'
- en: Note that the CDF is invariant to the base of the logarithm, since ![](Image00403.jpg)
    for any base *c* , thus the random variable is not a function of *c.* Snippet 9.4
    implements (and tests) in `scipy.stats` a random variable where [ *a* , *b* ]
    = [1 *E* − 3, 1 *E* 3], hence log [ *x* ] ∼ *U* [log [1 *E* − 3], log [1 *E* 3]].  [
    Figure 9.1 ](text00001.html#filepos0000486173) illustrates the uniformity of the
    samples in log-scale.
  id: totrans-694
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，CDF 对对数的底数是不变的，因为！[](Image00403.jpg)，对任何底数 *c* 都是如此，因此随机变量不是 *c* 的函数。Snippet 9.4
    在 `scipy.stats` 中实现（并测试）了一个随机变量，其中 [ *a* , *b* ] = [1 *E* − 3, 1 *E* 3]，因此 log [
    *x* ] ∼ *U* [log [1 *E* − 3], log [1 *E* 3]]。 [ 图 9.1 ](text00001.html#filepos0000486173)
    显示了样本在对数尺度上的均匀性。
- en: '**SNIPPET 9.4 THE** `**LOGUNIFORM_GEN**` **CLASS**'
  id: totrans-695
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '**SNIPPET 9.4 THE** `**LOGUNIFORM_GEN**` **CLASS**'
- en: '![](Image00406.jpg)'
  id: totrans-696
  prefs:
  - PREF_BQ
  type: TYPE_IMG
  zh: '![](Image00406.jpg)'
